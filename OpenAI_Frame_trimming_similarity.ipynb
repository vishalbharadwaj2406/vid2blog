{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./GPTFramesUnique.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Convert the dictionary to a DataFrame\n",
    "df = pd.DataFrame(list(data.items()), columns=['File Name', 'Content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2036"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['Content'] != \"Code NA\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1555"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['```python\\n# Let\\'s now split up the data into train and validation sets\\nn = int(0.9*len(data))  # first 90% will be train, rest val\\ntrain_data = data[:n]\\nval_data = data[n:]\\n\\nblock_size = 8\\ntrain_data[:block_size+1]\\n\\nx = train_data[:block_size]\\ny = train_data[1:block_size+1]\\nfor t in range(block_size):\\n    context = x[t:t+1]\\n    target = y[t]\\n    print(f\"when input is {context} the target: {target}\")\\n```',\n",
       " '```python\\nn = int(0.9*len(data))  # first 90% will be train, rest val\\ntrain_data = data[:n]\\nval_data = data[n:]\\n\\nblock_size = 8\\ntrain_data[:block_size+1]\\n\\nx = train_data[:block_size]\\ny = train_data[1:block_size+1]\\nfor t in range(block_size):\\n    context = x[t:t+1]\\n    target = y[t]\\n    print(f\"when input is {context} the target: {target}\")\\n```',\n",
       " '```python\\nn = int(0.9 * len(data))\\ntrain_data = data[:n]\\nval_data = data[n:]\\n\\nblock_size = 8\\ntrain_data[:block_size+1]\\n\\nx = train_data[:block_size]\\ny = train_data[1:block_size+1]\\nfor t in range(block_size):\\n    context = x[t:t+1]\\n    target = y[t]\\n    print(f\"when input is {context} the target: {target}\")\\n```',\n",
       " '```python\\n# Let\\'s now split up the data into train and validation sets\\nn = int(0.9*len(data))  # first 90% will be train, rest val\\ntrain_data = data[:n]\\nval_data = data[n:]\\n\\nblock_size = 8\\ntrain_data[:block_size+1]\\n\\nx = train_data[:block_size]\\ny = train_data[1:block_size+1]\\nfor t in range(block_size):\\n    context = x[t:t+1]\\n    target = y[t]\\n    print(f\"When input is {context} the target: {target}\")\\n```',\n",
       " '```python\\n# Let\\'s now split up the data into train and validation sets\\nn = int(0.9*len(data))  # first 90% will be train, rest val\\ntrain_data = data[:n]\\nval_data = data[n:]\\n\\nblock_size = 8\\ntrain_data[:block_size+1]\\n\\nx = train_data[:block_size]\\ny = train_data[1:block_size+1]\\nfor t in range(block_size):\\n    context = x[t:t+1]\\n    target = y[t]\\n    print(f\"when input is {context} the target: {target}\")\\n```']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(df[100:200][\"Content\"])[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_snippets_prompt = \"\"\"\n",
    "I will provide you a list of code snippets that may contain overlapping or redundant elements. \n",
    "Your task is to sequentially process these snippets and merge them into a single, coherent code block. \n",
    "While doing so, ensure that redundancies are eliminated, and overlapping sections are appropriately merged. \n",
    "Do not add any additional code beyond what is provided; simply integrate the snippets to create a unified and functional result.\n",
    "Do not include any comments that say stuff like here is the extracted code, etc. \n",
    "It is important that you do not omit any code as well.\n",
    "Just return the final merged code block. \n",
    "\n",
    "Here is the list of code snippets in order: {}\n",
    "\"\"\"\n",
    "\n",
    "def process_one_batch(client, code_snippets):\n",
    "    prompt = merge_snippets_prompt.format(code_snippets)\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "\n",
    "    response_params = {\n",
    "        \"model\": \"gpt-4o-mini\",\n",
    "        \"messages\": messages,\n",
    "        \"temperature\": 0,\n",
    "    }\n",
    "\n",
    "    response = client.chat.completions.create(**response_params)\n",
    "    consolidated_code = response.choices[0].message.content\n",
    "    return consolidated_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lis100 = list(df[:100][\"Content\"])\n",
    "# out100 = process_one_batch(client, lis100)\n",
    "# print(out100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lis100 = list(df[:100][\"Content\"])\n",
    "# out100 = process_one_batch(client, lis100)\n",
    "# print(out100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chunk_size = 100\n",
    "# code_snippet_list = []\n",
    "# for start in range(0, len(df), chunk_size):\n",
    "#     chunk = list(df[start:start + chunk_size][\"Content\"])\n",
    "#     merged_code_block = process_one_batch(client, chunk)\n",
    "#     code_snippet_list.append(merged_code_block)\n",
    "\n",
    "# # final_code = process_one_batch(client, code_snippet_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len(code_snippet_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks: 100%|██████████| 8/8 [01:02<00:00,  7.78s/it]\n"
     ]
    }
   ],
   "source": [
    "# parallel code to process batches\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Function to process one batch, wrapped for parallel execution\n",
    "def process_chunk(index, start, chunk_size, df, client):\n",
    "    chunk = list(df[start:start + chunk_size][\"Content\"])\n",
    "    merged_code_block = process_one_batch(client, chunk)\n",
    "    return index, merged_code_block\n",
    "\n",
    "# Parallel execution using ThreadPoolExecutor with tqdm\n",
    "chunk_size = 200\n",
    "num_workers = 3\n",
    "code_snippet_list = [None] * ((len(df) + chunk_size - 1) // chunk_size)  # Preallocate list\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
    "    # Creating a list of futures with indices\n",
    "    futures = [\n",
    "        executor.submit(process_chunk, idx, start, chunk_size, df, client)\n",
    "        for idx, start in enumerate(range(0, len(df), chunk_size))\n",
    "    ]\n",
    "    \n",
    "    # Use tqdm to track the progress of completed futures\n",
    "    for future in tqdm(as_completed(futures), total=len(futures), desc=\"Processing chunks\"):\n",
    "        index, result = future.result()\n",
    "        code_snippet_list[index] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abcde\n"
     ]
    }
   ],
   "source": [
    "# Solution 1 for merging\n",
    "\n",
    "# final_code_prompt = \"\"\"\n",
    "# I will provide you a list of code snippets that may contain overlapping or redundant elements. \n",
    "# Your task is to sequentially process these snippets and concatenate them. \n",
    "# While doing so, ensure that redundancies are eliminated, and overlapping sections are appropriately merged. \n",
    "# You should not re-order these snippets.\n",
    "# Do not modify or add any additional code beyond what is provided.\n",
    "# It is important that you do not omit any code as well.\n",
    "# Just return the final code without redundant snippets.\n",
    "\n",
    "# Here is the list of code snippets in order: {}\n",
    "# \"\"\"\n",
    "\n",
    "# def get_final_code(client, code_snippets):\n",
    "#     prompt = final_code_prompt.format(code_snippets)\n",
    "#     messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "\n",
    "#     response_params = {\n",
    "#         \"model\": \"gpt-4o\",\n",
    "#         \"messages\": messages,\n",
    "#         \"temperature\": 0,\n",
    "#     }\n",
    "\n",
    "#     response = client.chat.completions.create(**response_params)\n",
    "#     consolidated_code = response.choices[0].message.content\n",
    "#     return consolidated_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1: Combining 8 strings\n",
      "Iteration 2: Combining 4 strings\n",
      "Iteration 3: Combining 2 strings\n"
     ]
    }
   ],
   "source": [
    "# solution 2 for merging\n",
    "# more structured divide and conquer approach that iteratively merges adjacent strings\n",
    "# like (1, 2) (3, 4) etc and then again\n",
    "\n",
    "merge_prompt = \"\"\"\n",
    "Concatenate these two code snippets. If there is any redundancy or overlap, merge that part accordingly.\n",
    "It is very important that you do not re-order the given snippets, you need to merge them in the order given. \n",
    "You only need to merge them when you find a strong overlap in some part. Otherwise, you need to mostly concatenate them. \n",
    "It is also extremely important that you do not omit any code.\n",
    "Return the final full code. \n",
    "\n",
    "Code snippet 1: \n",
    "{}\n",
    "\n",
    "Code snippet 2: \n",
    "{}\n",
    "\"\"\"\n",
    "\n",
    "def get_merged_code(client, code_snippet1, code_snippet2):\n",
    "    prompt = merge_prompt.format(code_snippet1, code_snippet2)\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "\n",
    "    response_params = {\n",
    "        \"model\": \"gpt-4o\",\n",
    "        \"messages\": messages,\n",
    "        \"temperature\": 0,\n",
    "    }\n",
    "\n",
    "    response = client.chat.completions.create(**response_params)\n",
    "    merged = response.choices[0].message.content\n",
    "    return merged\n",
    "\n",
    "def merge_strings(strings):\n",
    "    merged_list = []\n",
    "    for i in range(0, len(strings) - 1, 2):\n",
    "        merged_list.append(get_merged_code(client, strings[i], strings[i + 1]))\n",
    "    if len(strings) % 2 != 0:  # Add the last string if the list is odd\n",
    "        merged_list.append(strings[-1])\n",
    "    return merged_list\n",
    "\n",
    "def merge_until_one_string(strings):\n",
    "    count = 1\n",
    "    while len(strings) > 1:\n",
    "        print(f\"Iteration {count}: Combining {len(strings)} strings\")\n",
    "        count += 1\n",
    "        strings = merge_strings(strings)\n",
    "    return strings[0] if strings else \"\"\n",
    "\n",
    "final_string = merge_until_one_string(code_snippet_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"new.txt\", \"w\") as f:\n",
    "  f.write(final_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cvpr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
