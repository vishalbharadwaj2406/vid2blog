{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_code_blocks(markdown_text):\n",
    "    \"\"\"\n",
    "    Removes code blocks from the given Markdown text.\n",
    "    Code blocks are delimited by lines that start with ``` and end with ``` lines.\n",
    "    \"\"\"\n",
    "    lines = markdown_text.split('\\n')\n",
    "    result_lines = []\n",
    "    in_code_block = False\n",
    "\n",
    "    for line in lines:\n",
    "        # Check if line starts a code block\n",
    "        if not in_code_block and line.strip().startswith('```'):\n",
    "            # Enter code block mode\n",
    "            in_code_block = True\n",
    "            continue\n",
    "        \n",
    "        # Check if line ends a code block\n",
    "        if in_code_block and line.strip().startswith('```'):\n",
    "            # Exit code block mode\n",
    "            in_code_block = False\n",
    "            continue\n",
    "        \n",
    "        if not in_code_block:\n",
    "            result_lines.append(line)\n",
    "\n",
    "    return \"\\n\".join(result_lines)\n",
    "\n",
    "def extract_headings(markdown_text):\n",
    "    \"\"\"\n",
    "    Extracts headings from Markdown text.\n",
    "    Headings are lines starting with one or more # followed by a space.\n",
    "    Returns a list of tuples (level, heading_text).\n",
    "    \"\"\"\n",
    "    headings = []\n",
    "    for line in markdown_text.split('\\n'):\n",
    "        # Strip leading/trailing whitespace\n",
    "        stripped = line.strip()\n",
    "\n",
    "        # Match a heading line: one or more # at start, a space, then the heading text\n",
    "        match = re.match(r'^(#{1,6})\\s+(.*)', stripped)\n",
    "        if match:\n",
    "            hashes = match.group(1)\n",
    "            heading_text = match.group(2)\n",
    "            level = len(hashes)\n",
    "            headings.append(heading_text)\n",
    "    return headings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./FINAL.md', 'r', encoding='utf-8') as file:\n",
    "    markdown_content = file.read()\n",
    "    clean_markdown = remove_code_blocks(markdown_content)\n",
    "    heading_list = extract_headings(clean_markdown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Exploring the Foundations of ChatGPT: A Journey Through Language Models',\n",
       " \"Understanding ChatGPT's Variability\",\n",
       " 'The Power of the Transformer Architecture',\n",
       " 'Simplifying the Complex: A DIY Approach to Transformers',\n",
       " 'Setting Up the Dataset',\n",
       " 'Uncovering the Vocabulary',\n",
       " 'Tokenizing the Text',\n",
       " 'Building a Character-Level Language Model with the Tiny Shakespeare Dataset',\n",
       " 'Data Acquisition and Preliminary Setup',\n",
       " 'Understanding the Vocabulary',\n",
       " 'Tokenization Strategy',\n",
       " 'Data Preparation for Model Training',\n",
       " 'Feeding Data into the Transformer Model',\n",
       " 'Building a Character-Level Transformer Model for Language Processing',\n",
       " 'Generating Training Examples',\n",
       " 'Introducing the Batch Dimension',\n",
       " 'Implementing a Bigram Language Model',\n",
       " 'Evaluating the Model',\n",
       " 'Character Prediction with Deep Learning using PyTorch',\n",
       " 'Setting Up the Model for Character Prediction',\n",
       " 'Understanding Logits and Predictions',\n",
       " 'Evaluating Predictions with a Loss Function',\n",
       " 'Generating Sequences',\n",
       " 'Example of Sequence Generation',\n",
       " 'Building a Text Generation Model: From Basics to Advanced',\n",
       " 'Initializing the Model',\n",
       " 'Training the Model',\n",
       " 'Enhancing the Model with Transformers',\n",
       " 'Transitioning to Script Execution',\n",
       " 'Leveraging GPU Support',\n",
       " 'Optimizing Self-Attention in Transformer Models: A Practical Guide',\n",
       " 'Understanding the Importance of a Stable Loss Function',\n",
       " 'Leveraging Matrix Multiplication for Efficient Self-Attention',\n",
       " 'The Mathematical Trick: Lower Triangular Matrices',\n",
       " 'Building a Simple Language Model',\n",
       " 'Mastering Matrix Operations in PyTorch: A Deep Dive into Dot Products, Matrix Multiplications, and Weighted Aggregation',\n",
       " 'Dot Products and Matrix Multiplication Basics',\n",
       " 'Example: Dot Product with PyTorch',\n",
       " 'Introducing Complexity with Lower Triangular Matrices',\n",
       " 'Implementing Weighted Aggregation',\n",
       " 'Advanced Techniques: Applying Softmax for Self-Attention',\n",
       " 'Self-Attention with PyTorch',\n",
       " 'Understanding Self-Attention in Transformers',\n",
       " 'The Role of Softmax in Self-Attention',\n",
       " 'Creating Dynamic Affinities',\n",
       " 'Implementing Masking to Control Token Interaction',\n",
       " 'Embedding Dimensions and Token Representation',\n",
       " 'Mastering Self-Attention: A Deep Dive Into Modern Sequence Processing',\n",
       " 'The Mechanics of Self-Attention',\n",
       " 'Implementing a Self-Attention Head',\n",
       " 'Data-Dependent Affinity and Aggregation',\n",
       " 'Incorporating Values for Aggregation',\n",
       " 'Understanding Attention Mechanisms in Directed Graphs for Language Modeling',\n",
       " 'The Basics of Attention Mechanisms',\n",
       " 'Directed Graph Structure for Language Modeling',\n",
       " 'Positional Encoding: Providing Spatial Awareness',\n",
       " 'Batch Processing and Independence',\n",
       " 'Autoregressive Constraints in Language Modeling',\n",
       " 'The Role of Scaling in Attention Scores',\n",
       " 'Understanding and Implementing Attention Mechanisms in Neural Networks',\n",
       " 'Computing Keys and Queries for Attention Scores',\n",
       " 'Implementing Multi-Head Attention',\n",
       " 'Adding Feedforward Networks',\n",
       " 'Structuring Transformer Blocks',\n",
       " 'Optimizing Deep Neural Networks with Transformers: A Guide',\n",
       " 'Enhancing Optimization with Skip Connections',\n",
       " 'Implementing Residual Connections in Transformers',\n",
       " 'Layer Normalization for Stabilized Training',\n",
       " 'Incorporating LayerNorm in Transformer Models',\n",
       " 'Implementing Layer Normalization in a Transformer Model',\n",
       " 'Understanding Layer Normalization',\n",
       " 'Implementing Layer Normalization',\n",
       " 'Performance Improvements with Layer Normalization',\n",
       " 'Building a Complete Decoder-Only Transformer',\n",
       " 'Scaling Up the Model',\n",
       " 'Hyperparameter Adjustments',\n",
       " 'Generating Text with the Model',\n",
       " 'Unveiling the Mechanics of Neural Network-Based Language Models',\n",
       " 'The Framework: Encoder-Decoder Architecture',\n",
       " 'Encoder',\n",
       " 'Decoder',\n",
       " 'A Simpler Approach: Decoder-Only Transformer',\n",
       " 'Exploring \"nanoGPT\"',\n",
       " '`train.py` File',\n",
       " '`model.py` File',\n",
       " 'Multi-Layer Perceptron (MLP)',\n",
       " 'Transforming AI Models into Effective Assistants: A Step-by-Step Guide',\n",
       " 'Understanding the Initial Training Phase',\n",
       " 'Fine-Tuning: Making a Model an Assistant',\n",
       " 'Step 1: Collecting Specialized Training Data',\n",
       " 'Step 2: Human Feedback and Reward Model',\n",
       " 'Step 3: Reinforcement Learning with PPO',\n",
       " 'The Challenges of Replicating Alignment',\n",
       " 'Scaling Up: From Basic Models to GPT-3',\n",
       " 'Conclusion']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heading_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"headings.json\", \"w\", encoding=\"utf-8\") as f: \n",
    "    json.dump(heading_list, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cvpr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
