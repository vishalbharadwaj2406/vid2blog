{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./filtered_data.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Convert the dictionary to a DataFrame\n",
    "df = pd.DataFrame(list(data.items()), columns=['File Name', 'Content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "780"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['Content'] != \"Code NA\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "728"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(df[100:200][\"Content\"])[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_snippets_prompt = \"\"\"\n",
    "I will provide you a list of code snippets that may contain overlapping or redundant elements. \n",
    "Your task is to sequentially process these snippets and merge them into a coherent code file. \n",
    "While doing so, ensure that redundancies are eliminated, and overlapping sections are appropriately merged. \n",
    "Do not add any additional code beyond what is provided; simply integrate the snippets to create a unified and functional result.\n",
    "Do not include any comments that say stuff like here is the extracted code, etc. \n",
    "It is important that you do not omit any code as well.\n",
    "Please return only the final merged code block with no other explanation.\n",
    "\n",
    "Here is the list of code snippets in order: {}\n",
    "\"\"\"\n",
    "\n",
    "def process_one_batch(code_snippets):\n",
    "    prompt = merge_snippets_prompt.format(code_snippets)\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "\n",
    "    response_params = {\n",
    "        \"model\": \"gpt-4o\",\n",
    "        \"messages\": messages,\n",
    "        \"temperature\": 0,\n",
    "    }\n",
    "\n",
    "    response = client.chat.completions.create(**response_params)\n",
    "    consolidated_code = response.choices[0].message.content\n",
    "    return consolidated_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [06:07<00:00, 24.52s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "df['Frame_ID'] = df['File Name'].str.extract(r'frame_(\\d+).jpg').astype(int)\n",
    "\n",
    "# Set the chunk size and overlap\n",
    "chunk_size = 600\n",
    "overlap = 120\n",
    "\n",
    "# Determine the maximum frame number\n",
    "max_frame_id = df['Frame_ID'].max()\n",
    "\n",
    "consolidated_code_list = []\n",
    "\n",
    "count = 0\n",
    "# Iterate over the data with an overlap\n",
    "for start_id in tqdm(range(1, max_frame_id + 1, chunk_size - overlap)):\n",
    "    end_id = start_id + chunk_size - 1\n",
    "    subset = df[(df['Frame_ID'] >= start_id) & (df['Frame_ID'] <= end_id)]\n",
    "    \n",
    "    # Do whatever processing you need with 'subset'\n",
    "    # print(f\"Processing frames from {start_id} to {end_id}:\")\n",
    "    code_snippets = list(subset[\"Content\"])\n",
    "    consolidated_code = process_one_batch(code_snippets)\n",
    "    consolidated_code_list.append(consolidated_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"10min.json\"\n",
    "\n",
    "# Save the list to a file as JSON\n",
    "with open(file_name, \"w\") as file:\n",
    "    json.dump(consolidated_code_list, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cvpr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
