{
    "frame_0001.jpg": "Code NA",
    "frame_0290.jpg": "The extracted developer code from the image is:\n\n```bash\npython sample_streaming.py --out_dir=out--shakespeare-char\n```\n\nIf you need further assistance, feel free to ask!",
    "frame_0292.jpg": "Code NA",
    "frame_0495.jpg": "```python\n# read it in to inspect it\nwith open('input.txt', 'r', encoding='utf-8') as f:\n    text = f.read()\n\nprint(\"length of dataset in characters: \", len(text))\n\n# let's look at the first 1000 characters\nprint(text[:1000])\n```",
    "frame_0499.jpg": "```python\n# read it in to inspect it\nwith open('input.txt', 'r', encoding='utf-8') as f:\n    text = f.read()\n\nprint(\"length of dataset in characters: \", len(text))\n\n# let's look at the first 1000 characters\nprint(text[:1000])\n```",
    "frame_0505.jpg": "```python\n# Let's look at the first 1000 characters\nprint(text[:1000])\n```",
    "frame_0507.jpg": "Code NA",
    "frame_0516.jpg": "```python\n# here are all the unique characters that occur in this text\nchars = sorted(list(set(text)))\nvocab_size = len(chars)\nprint(''.join(chars))\nprint(vocab_size)\n```",
    "frame_0574.jpg": "```python\n# here are all the unique characters that occur in this text\nchars = sorted(list(set(text)))\nvocab_size = len(chars)\nprint(''.join(chars))\nprint(vocab_size)\n```",
    "frame_0576.jpg": "```python\n# here are all the unique characters that occur in this text\nchars = sorted(list(set(text)))\nvocab_size = len(chars)\nprint('.'.join(chars))\nprint(vocab_size)\n```",
    "frame_0596.jpg": "```python\n# here are all the unique characters that occur in this text\nchars = sorted(list(set(text)))\nvocab_size = len(chars)\nprint(''.join(chars))\nprint(vocab_size)\n\n# create a mapping from characters to integers\nstoi = {ch: i for i, ch in enumerate(chars)}\nitos = {i: ch for i, ch in enumerate(chars)}\nencode = lambda s: [stoi[ch] for ch in s]  # encoder: take a string, output a list of integers\ndecode = lambda l: ''.join([itos[i] for i in l])  # decoder: take a list of integers, output a string\n\nprint(encode(\"hii there\"))\nprint(decode(encode(\"hii there\")))\n```",
    "frame_0602.jpg": "```python\n# here are all the unique characters that occur in this text\nchars = sorted(list(set(text)))\nvocab_size = len(chars)\nprint(''.join(chars))\nprint(vocab_size)\n\n# create a mapping from characters to integers\nstoi = { ch: i for i, ch in enumerate(chars) }\nitos = { i: ch for i, ch in enumerate(chars) }\nencode = lambda s: [stoi[c] for c in s]  # encoder: take a string, output a list of integers\ndecode = lambda l: ''.join([itos[i] for i in l])  # decoder: take a list of integers, output a string\n\nprint(encode(\"hii there\"))\nprint(decode(encode(\"hii there\")))\n```",
    "frame_0604.jpg": "```python\n# create a mapping from characters to integers\nstoi = {ch: i for i, ch in enumerate(chars)}\nitos = {i: ch for i, ch in enumerate(chars)}\nencode = lambda s: [stoi[ch] for ch in s]  # encoder: take a string, output a list of integers\ndecode = lambda l: ''.join([itos[i] for i in l])  # decoder: take a list of integers, output a string\n\nprint(encode(\"hii there\"))\nprint(decode(encode(\"hii there\")))\n```",
    "frame_0616.jpg": "```python\n# here are all the unique characters that occur in this text\nchars = sorted(list(set(text)))\nvocab_size = len(chars)\nprint('.'.join(chars))\nprint(vocab_size)\n\n# create a mapping from characters to integers\nstoi = {ch: i for i, ch in enumerate(chars)}\nitos = {i: ch for i, ch in enumerate(chars)}\nencode = lambda s: [stoi[c] for c in s]  # encoder: take a string, output a list of integers\ndecode = lambda l: ''.join([itos[i] for i in l])  # decoder: take a list of integers, output a string\n\nprint(encode(\"hii there\"))\nprint(decode(encode(\"hii there\")))\n```",
    "frame_0626.jpg": "```python\n# here are all the unique characters that occur in this text\nchars = sorted(list(set(text)))\nvocab_size = len(chars)\nprint(''.join(chars))\nprint(vocab_size)\n\n!$&'*,-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n65\n\n# create a mapping from characters to integers\nstoi = { ch:i for i,ch in enumerate(chars) }\nitos = { i:ch for i,ch in enumerate(chars) }\nencode = lambda s: [stoi[c] for c in s]  # encoder: take a string, output a list of integers\ndecode = lambda l: ''.join([itos[i] for i in l])  # decoder: take a list of integers, output a string\n\nprint(encode(\"hii there\"))\nprint(decode(encode(\"hii there\")))\n```\n",
    "frame_0637.jpg": "```python\n# here are all the unique characters that occur in this text\nchars = sorted(list(set(text)))\nvocab_size = len(chars)\nprint(''.join(chars))\nprint(vocab_size)\n\n# create a mapping from characters to integers\nstoi = {ch: i for i, ch in enumerate(chars)}\nitos = {i: ch for i, ch in enumerate(chars)}\nencode = lambda s: [stoi[c] for c in s]  # encoder: take a string, output a list of integers\ndecode = lambda l: ''.join([itos[i] for i in l])  # decoder: take a list of integers, output a string\n\nprint(encode(\"hii there\"))\nprint(decode(encode(\"hii there\")))\n```",
    "frame_0659.jpg": "Code NA", 
    "frame_0687.jpg": "```python\nimport tiktoken\nenc = tiktoken.get_encoding('gpt2')\nassert enc.decode(enc.encode('hello world')) == \"hello world\"\n```\n\n```bash\npip install tiktoken\n```",
    "frame_0691.jpg": "```python\nimport tiktoken\nenc = tiktoken.get_encoding(\"gpt2\")\nassert enc.decode(enc.encode(\"hello world\")) == \"hello world\"\n```\n\n```bash\npip install tiktoken\n```",
    "frame_0700.jpg": "```python\nprint('woot')\n```",
    "frame_0739.jpg": "Here's the extracted code from the image:\n\n```python\nprint('woot')\n```\n\nAnd the additional terminal commands:\n```python\nimport tiktoken\nenc = tiktoken.get_encoding('gpt2')\nenc.n_vocab\nenc.encode(\"hii there!\")\nenc.decode([71, 4178, 612])\n```\n\nIf you need anything else, let me know!",
    "frame_0744.jpg": "```python\nprint('woot')\n```\n\n```python\nimport tiktoken\nenc = tiktoken.get_encoding('gpt2')\nenc.n_vocab\nenc.encode(\"hii there\")\nenc.decode([71, 4178, 612])\n```",
    "frame_0746.jpg": "```\nimport tiktoken\nenc = tiktoken.get_encoding(\"gpt2\")\nassert enc.decode(enc.encode(\"hello world\")) == \"hello world\"\n```\n\n```\npip install tiktoken\n``` \n\nCode extracted successfully.",
    "frame_0750.jpg": "```python\n# here are all the unique characters that occur in this text\nchars = sorted(list(set(text)))\nvocab_size = len(chars)\nprint('\\n'.join(chars))\nprint(vocab_size)\n\n!$&'*,.-3:;;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n65\n\n# create a mapping from characters to integers\nstoi = {ch:i for i,ch in enumerate(chars) }\nitos = {i:ch for i,ch in enumerate(chars) }\nencode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\ndecode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n\nprint(encode(\"hii there\"))\nprint(decode(encode(\"hii there\")))\n```\n",
    "frame_0756.jpg": "```python\n# here are all the unique characters that occur in this text\nchars = sorted(list(set(text)))\nvocab_size = len(chars)\nprint('.'.join(chars))\nprint(vocab_size)\n\n!$&'*,.-3:;?ABCDEF GHIJKLM NOPQRSTUVWXYZ abcdefghijklmnopqrstuvwxyz\n65\n\n# create a mapping from characters to integers\nstoi = {ch: i for i, ch in enumerate(chars)}\nitos = {i: ch for i, ch in enumerate(chars)}\nencode = lambda s: [stoi[c] for c in s]  # encoder: take a string, output a list of integers\ndecode = lambda l: ''.join([itos[i] for i in l])  # decoder: take a list of integers, output a string\n\nprint(encode(\"hii there\"))\nprint(decode(encode(\"hii there\")))\n```\n",
    "frame_0764.jpg": "```python\n# create a mapping from characters to integers\nstoi = {ch:i for i,ch in enumerate(chars)}\nitos = {i:ch for i,ch in enumerate(chars)}\nencode = lambda s: [stoi[c] for c in s]  # encoder: take a string, output a list of integers\ndecode = lambda l: ''.join([itos[i] for i in l])  # decoder: take a list of integers, output a string\n\nprint(encode(\"hii there\"))\nprint(decode(encode(\"hii there\")))\n```",
    "frame_0766.jpg": "```python\n# here are all the unique characters that occur in this text\nchars = sorted(list(set(text)))\nvocab_size = len(chars)\nprint('.'.join(chars))\nprint(vocab_size)\n\n# create a mapping from characters to integers\nstoi = {ch: i for i, ch in enumerate(chars)}\nitos = {i: ch for i, ch in enumerate(chars)}\nencode = lambda s: [stoi[c] for c in s]  # encoder: take a string, output a list of integers\ndecode = lambda l: ''.join([itos[i] for i in l])  # decoder: take a list of integers, output a string\n\nprint(encode(\"hii there\"))\nprint(decode(encode(\"hii there\")))\n```\n",
    "frame_0767.jpg": "```python\n# create a mapping from characters to integers\nstoi = {ch: i for i, ch in enumerate(chars)}\nitos = {i: ch for i, ch in enumerate(chars)}\nencode = lambda s: [stoi[c] for c in s]  # encoder: take a string, output a list of integers\ndecode = lambda l: ''.join([itos[i] for i in l])  # decoder: take a list of integers, output a string\n\nprint(encode(\"hii there\"))\nprint(decode(encode(\"hii there\")))\n```",
    "frame_0776.jpg": "```python\n# create a mapping from characters to integers\nstoi = {ch: i for i, ch in enumerate(chars)}\nitos = {i: ch for i, ch in enumerate(chars)}\nencode = lambda s: [stoi[c] for c in s]  # encoder: take a string, output a list of integers\ndecode = lambda l: ''.join([itos[i] for i in l])  # decoder: take a list of integers, output a string\n\nprint(encode(\"hii there\"))\nprint(decode(encode(\"hii there\")))\n\n# let's now encode the entire text dataset and store it into a torch.Tensor\nimport torch  # we use PyTorch: https://pytorch.org\ndata = torch.tensor(encode(text), dtype=torch.long)\nprint(data.shape, data.dtype)\nprint(data[:1000])  # the 1000 characters we looked at earlier will to the GPT look like this\n```",
    "frame_0791.jpg": "```python\n# create a mapping from characters to integers\nstoi = {c: i for i, c in enumerate(chars)}\nitos = {i: c for i, c in enumerate(chars)}\nencode = lambda s: [stoi[c] for c in s]  # encoder: take a string, output a list of integers\ndecode = lambda l: ''.join([itos[i] for i in l])  # decoder: take a list of integers, output a string\n\nprint(encode('hii there'))\nprint(decode(encode('hii there')))\n\n# let's now encode the entire text dataset and store it into a torch.Tensor\nimport torch  # we use PyTorch: https://pytorch.org\ndata = torch.tensor(encode(text), dtype=torch.long)\nprint(data.shape, data.dtype)\nprint(data[:1000])  # the 1000 characters we looked at earlier will to the GPT look like this\n```",
    "frame_0796.jpg": "```python\n# let's now encode the entire text dataset and store it into a torch.Tensor\nimport torch # we use PyTorch: https://pytorch.org\ndata = torch.tensor(encode(text), dtype=torch.long)\nprint(data.shape, data.dtype)\nprint(data[:1000]) # the 1000 characters we looked at earlier will to the GPT look like this\n```",
    "frame_0798.jpg": "Code NA",
    "frame_0801.jpg": "```python\n# here are all the unique characters that occur in this text\nchars = sorted(list(set(text)))\nvocab_size = len(chars)\nprint(''.join(chars))\nprint(vocab_size)\n\n# create a mapping from characters to integers\nstoi = {ch: i for i, ch in enumerate(chars)}\nitos = {i: ch for i, ch in enumerate(chars)}\nencode = lambda s: [stoi[c] for c in s]  # encoder: take a string, output a list of integers\ndecode = lambda l: ''.join([itos[i] for i in l])  # decoder: take a list of integers, output a string\n\nprint(encode('hi there'))\nprint(decode([1, 0, 4, 2, 3, 1, 5]))\n```",
    "frame_0802.jpg": "```python\nprint(text[:1000])\n```",
    "frame_0803.jpg": "Code NA",
    "frame_0804.jpg": "```python\n# here are all the unique characters that occur in this text\nchars = sorted(list(set(text)))\nvocab_size = len(chars)\nprint(''.join(chars))\nprint(vocab_size)\n```",
    "frame_0805.jpg": "```python\nimport torch  # we use PyTorch: https://pytorch.org\ndata = torch.tensor(encode(text), dtype=torch.long)\nprint(data.shape, data.dtype)\nprint(data[:1000])  # the 1000 characters we looked at earlier will to the GPT look like this\n\ntorch.Size([11153949]) torch.int64\ntensor([[18, 47, 56, 57, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n         ...\n         39, 58, 1, 39, 58, 1, 39, 58]]\n```",
    "frame_0806.jpg": "Code NA",
    "frame_0812.jpg": "```python\nimport torch  # we use PyTorch: https://pytorch.org\ndata = torch.tensor(encode(text), dtype=torch.long)\nprint(data.shape, data.dtype)\nprint(data[:1000])  # the 1000 characters we looked at earlier will to the GPT look like this\n```",
    "frame_0817.jpg": "```python\nimport torch  # we use PyTorch: https://pytorch.org\ndata = torch.tensor(encode(text), dtype=torch.long)\nprint(data.shape, data.dtype)\nprint(data[1000])  # the 1000 characters we looked at earlier will to the GPT look like this\n```",
    "frame_0821.jpg": "Code NA",
    "frame_0824.jpg": "```python\n# Let's now split up the data into train and validation sets\nn = int(0.9*len(data))  # first 90% will be train, rest val\ntrain_data = data[:n]\nval_data = data[n:]\n```",
    "frame_0838.jpg": "val_data = data[n:]",
    "frame_0842.jpg": "```python\n# Let's now split up the data into train and validation sets\nn = int(0.9 * len(data))  # first 90% will be train, rest val\ntrain_data = data[:n]\nval_data = data[n:]\n```",
    "frame_0850.jpg": "Code NA",
    "frame_0851.jpg": "```python\n# Let's now split up the data into train and validation sets\nn = int(0.9*len(data))  # first 90% will be train, rest val\ntrain_data = data[:n]\nval_data = data[n:]\n```",
    "frame_0854.jpg": "Here is the extracted code from the image:\n\n```python\n# Let's now split up the data into train and validation sets\nn = int(0.9*len(data))  # first 90% will be train, rest val\ntrain_data = data[:n]\nval_data = data[n:]\n```",
    "frame_0861.jpg": "val_data = data[n:]",
    "frame_0863.jpg": "```python\nval_data = data[n:]\n```",
    "frame_0867.jpg": "```python\n# Let's now split up the data into train and validation sets\nn = int(0.9*len(data))  # first 90% will be train, rest val\ntrain_data = data[:n]\nval_data = data[n:]\n```",
    "frame_0868.jpg": "Code NA",
    "frame_0904.jpg": "```python\n# here are all the unique characters that occur in this text\nchars = sorted(list(set(text)))\n```",
    "frame_0906.jpg": "```python\nimport torch  # we use PyTorch: https://pytorch.org\ndata = torch.tensor(encode(text), dtype=torch.long)\nprint(data.shape, data.dtype)\nprint(data[:1000])  # the 1000 characters we looked at earlier will to the GPT look like this\n```",
    "frame_0924.jpg": "```python\n# Let's now split up the data into train and validation sets\nn = int(0.9 * len(data))  # first 90% will be train, rest val\ntrain_data = data[:n]\nval_data = data[n:]\n\nblock_size = 8\ntrain_data[:block_size + 1]\ntensor([18, 47, 56, 57, 58, 1, 15, 47, 58])\n```",
    "frame_0926.jpg": "```python\n# Let's now split up the data into train and validation sets\nn = int(0.9*len(data))  # first 90% will be train, rest val\ntrain_data = data[:n]\nval_data = data[n:]\n\nblock_size = 8\ntrain_data[:block_size+1]\ntensor([18, 47, 56, 57, 58,  1, 15, 47, 58])\n```",
    "frame_0932.jpg": "```python\n# Let's now split up the data into train and validation sets\nn = int(0.9*len(data))  # first 90% will be train, rest val\ntrain_data = data[:n]\nval_data = data[n:]\n\nblock_size = 8\ntrain_data[:block_size+1]\ntensor([18, 47, 56, 57, 58, 1, 15, 47, 58])\n```",
    "frame_0937.jpg": "```python\n# Let's now split up the data into train and validation sets\nn = int(0.9 * len(data))  # first 90% will be train, rest val\ntrain_data = data[:n]\nval_data = data[n:]\n\nblock_size = 8\ntrain_data[:block_size + 1]\ntensor([18, 47, 56, 57, 58, 1, 15, 47, 58])\n```",
    "frame_0946.jpg": "The extracted code from the image is:\n\n```python\nn = int(0.9*len(data))  # first 90% will be train, rest val\ntrain_data = data[:n]\nval_data = data[n:]\n\nblock_size = 8\ntrain_data[:block_size+1]\ntensor([18, 47, 56, 57, 58, 1, 15, 47, 58])\n```\n\nIf you need further assistance, feel free to ask!",
    "frame_0948.jpg": "```python\n# Let's now split up the data into train and validation sets\nn = int(0.9*len(data))  # first 90% will be train, rest val\ntrain_data = data[:n]\nval_data = data[n:]\n\nblock_size = 8\ntrain_data[:block_size+1]\ntensor([18, 47, 56, 57, 58,  1, 15, 47, 58])\n```",
    "frame_0982.jpg": "```python\n# Let's now split up the data into train and validation sets\nn = int(0.9*len(data))  # first 90% will be train, rest val\ntrain_data = data[:n]\nval_data = data[n:]\n\nblock_size = 8\ntrain_data[:block_size+1]\n\nx = train_data[:block_size]\ny = train_data[1:block_size+1]\nfor t in range(block_size):\n    context = x[t:t+1]\n    target = y[t]\n    print(f\"when input is {context} the target: {target}\")\n```",
    "frame_0995.jpg": "```python\n# Let's now split up the data into train and validation sets\nn = int(0.9*len(data))  # first 90% will be train, rest val\ntrain_data = data[:n]\nval_data = data[n:]\n\nblock_size = 8\ntrain_data[:block_size+1]\n\nx = train_data[:block_size]\ny = train_data[1:block_size+1]\nfor t in range(block_size):\n    context = x[t:t+1]\n    target = y[t]\n    print(f\"When input is {context} the target: {target}\")\n```",
    "frame_1018.jpg": "```python\n# Let's now split up the data into train and validation sets\nn = int(0.9*len(data)) # first 90% will be train, rest val\ntrain_data = data[:n]\nval_data = data[n:]\n\nblock_size = 8\ntrain_data[:block_size+1]\n\nx = train_data[:block_size]\ny = train_data[1:block_size+1]\nfor t in range(block_size):\n    context = x[t:t+1]\n    target = y[t]\n    print(f\"when input is {context} the target: {target}\")\n```",
    "frame_1019.jpg": "Here\u2019s the extracted developer code from the image:\n\n```python\n# Let's now split up the data into train and validation sets\nn = int(0.9*len(data))  # first 90% will be train, rest val\ntrain_data = data[:n]\nval_data = data[n:]\n\nblock_size = 8\ntrain_data[:block_size+1]\n\ntensor([18, 47, 56, 57, 58, 1, 15, 47, 58])\n\nx = train_data[:block_size]\ny = train_data[1:block_size+1]\nfor t in range(block_size):\n    context = x[t:t+1]\n    target = y[t]\n    print(f\"When input is {context} the target: {target}\")\n```\n\nThis includes all the noted lines of code from your image.",
    "frame_1022.jpg": "Here's the extracted developer code from the image:\n\n```python\n# Let's now split up the data into train and validation sets\nn = int(0.9*len(data))  # first 90% will be train, rest val\ntrain_data = data[:n]\nval_data = data[n:]\n\nblock_size = 8\ntrain_data[:block_size+1]\n\nx = train_data[:block_size]\ny = train_data[1:block_size+1]\nfor t in range(block_size):\n    context = x[t:t+1]\n    target = y[t]\n    print(f\"when input is {context} the target: {target}\")\n```",
    "frame_1027.jpg": "Here is the extracted code from the image:\n\n```python\nblock_size = 8\ntrain_data = data[:block_size+1]\ntensor([18, 47, 56, 57, 58, 1, 15, 47, 58])\n\nx = train_data[:block_size]\ny = train_data[1:block_size+1]\nfor t in range(block_size):\n    context = x[t:t+1]\n    target = y[t]\n    print(f\"when input is tensor({context}) the target: {target}\")\n```\n\n",
    "frame_1029.jpg": "```python\n# Let's now split up the data into train and validation sets\nn = int(0.9*len(data))  # first 90% will be train, rest val\ntrain_data = data[:n]\nval_data = data[n:]\n\nblock_size = 8\ntrain_data[:block_size+1]\n\ntensor([18, 47, 56, 57, 58, 1, 15, 47, 58])\n\nx = train_data[:block_size]\ny = train_data[1:block_size+1]\nfor t in range(block_size):\n    context = x[:t+1]\n    target = y[t]\n    print(f\"when input is tensor({context.tolist()}) the target: {target}\")\n```",
    "frame_1031.jpg": "Here's the extracted developer code from the image:\n\n```python\n# Let's now split up the data into train and validation sets\nn = int(0.9*len(data))  # first 90% will be train, rest val\ntrain_data = data[:n]\nval_data = data[n:]\n\nblock_size = 8\ntrain_data[:block_size+1]\n\nx = train_data[:block_size]\ny = train_data[1:block_size+1]\nfor t in range(block_size):\n    context = x[t:t+1]\n    target = y[t]\n    print(f\"when input is {context} the target: {target}\")\n```\n\nThis code performs data splitting and training data processing.",
    "frame_1043.jpg": "```python\n# Let's now split up the data into train and validation sets\nn = int(0.9*len(data))  # first 90% will be train, rest val\ntrain_data = data[:n]\nval_data = data[n:]\n\nblock_size = 8\ntrain_data[:block_size+1]\n\nx = train_data[:block_size]\ny = train_data[1:block_size+1]\nfor t in range(block_size):\n    context = x[t:t+1]\n    target = y[t]\n    print(f\"when input is {context} the target: {target}\")\n```",
    "frame_1052.jpg": "```python\n# Let's now split up the data into train and validation sets\nn = int(0.9*len(data))  # first 90% will be train, rest val\ntrain_data = data[:n]\nval_data = data[n:]\n\nblock_size = 8\ntrain_data[:block_size+1]\n\nx = train_data[:block_size]\ny = train_data[1:block_size+1]\nfor t in range(block_size):\n    context = x[t:t+1]\n    target = y[t]\n    print(f\"when input is {context} the target: {target}\")\n```\n",
    "frame_1057.jpg": "Here is the extracted developer code from the image:\n\n```python\n# Let's now split up the data into train and validation sets\nn = int(0.9*len(data))  # first 90% will be train, rest val\ntrain_data = data[:n]\nval_data = data[n:]\n\nblock_size = 8\ntrain_data = train_data[:block_size+1]\ntensor([18, 47, 56, 57, 58, 1, 15, 47, 58])\n\nx = train_data[:block_size]\ny = train_data[1:block_size+1]\nfor t in range(block_size):\n    context = x[t:t+1]\n    target = y[t]\n    print(f\"when input is {context} the target: {target}\")\n```\n\nIs there anything else you need?",
    "frame_1059.jpg": "```python\n# Let's now split up the data into train and validation sets\nn = int(0.9*len(data))  # first 90% will be train, rest val\ntrain_data = data[:n]\nval_data = data[n:]\n\nblock_size = 8\ntrain_data[:] = tensor([18, 47, 56, 57, 58, 1, 15, 47, 58])\n\nx = train_data[:block_size]\ny = train_data[1:block_size + 1]\nfor t in range(block_size):\n    context = x[t:t + 1]\n    target = y[t]\n    print(f\"when input is tensor({context}) the target: {target}\")\n```",
    "frame_1061.jpg": "```python\n# Let's now split up the data into train and validation sets\nn = int(0.9 * len(data))  # first 90% will be train, rest val\ntrain_data = data[:n]\nval_data = data[n:]\n\nblock_size = 8\ntrain_data[:block_size + 1]\n\nx = train_data[:block_size]\ny = train_data[1:block_size + 1]\nfor t in range(block_size):\n    context = x[t:t + 1]\n    target = y[t]\n    print(f\"when input is {context} the target: {target}\")\n```",
    "frame_1076.jpg": "```python\n# Let's now split up the data into train and validation sets\nn = int(0.9 * len(data))  # First 90% will be train, rest val\ntrain_data = data[:n]\nval_data = data[n:]\n\nblock_size = 8\ntrain_data[:block_size + 1]\n\nx = train_data[:block_size]\ny = train_data[1:block_size + 1]\nfor t in range(block_size):\n    context = x[t:t + 1]\n    target = y[t]\n    print(f\"when input is tensor({context}) the target: {target}\")\n```",
    "frame_1086.jpg": "Here is the extracted code from the image:\n\n```python\nx = train_data[:block_size]\ny = train_data[1:block_size+1]\nfor t in range(block_size):\n    context = x[t+1]\n    target = y[t]\n    print(f\"when input is {context} the target: {target}\")\n```\n\nIf you need anything else, let me know!",
    "frame_1095.jpg": "```\nblock_size = 8\ntrain_data[:block_size+1]\ntensor([18, 47, 56, 57, 58, 1, 15, 47, 58])\nx = train_data[:block_size]\ny = train_data[1:block_size+1]\nfor t in range(block_size):\n    context = x[t+1]\n    target = y[t]\n    print(f\"when input is tensor({context}) the target: {target}\")\n```",
    "frame_1100.jpg": "```python\nblock_size = 8\ntrain_data[:block_size+1]\n\nx = train_data[:block_size]\ny = train_data[1:block_size+1]\nfor t in range(block_size):\n    context = x[t+1]\n    target = y[t]\n    print(f\"when input is tensor({context}) the target: {target}\")\n```",
    "frame_1122.jpg": "```python\nval_data = data[n]\nblock_size = 8\ntrain_data = ...\ntensor([18, 47, 56, 57, 58, 1, 15, 47, 58])\nx = train_data[:block_size]\ny = train_data[1:block_size+1]\nfor t in range(block_size):\n    context = x[t:t+1]\n    target = y[t]\n    print(f\"when input is {context} the target: {target}\")\n``` \n\nThis code is partial and does not contain the complete context. If you would like the whole context, please provide more information.",
    "frame_1124.jpg": "```python\nblock_size = 8\ntrain_data[t:block_size+1]\n\nx = train_data[:block_size]\ny = train_data[1:block_size+1]\nfor t in range(block_size):\n    context = x[t+1]\n    target = y[t]\n    print(f\"when input is tensor({context}) the target: {target}\")\n```",
    "frame_1126.jpg": "```python\ntorch.manual_seed(1337)\nbatch_size = 4  # how many independent sequences will we process in parallel?\nblock_size = 8  # what is the maximum context length for predictions?\n\ndef get_batch(split):\n    # generate a small batch of data of inputs x and targets y\n    data = train_data if split == 'train' else val_data\n    ix = torch.randint(len(data) - block_size, (batch_size,))\n    x = torch.stack([data[i:i + block_size] for i in ix])\n    y = torch.stack([data[i + 1:i + block_size + 1] for i in ix])\n    return x, y\n\nxb, yb = get_batch('train')\nprint('inputs:')\nprint(xb.shape)\nprint(xb)\nprint('targets:')\nprint(yb.shape)\nprint(yb)\n\nprint('-----')\nfor b in range(batch_size):  # batch dimension\n    for t in range(block_size):  # time dimension\n        context = xb[b, t + 1]\n        target = yb[b, t]\n        print(f\"when input is {context.tolist()} the target: {target}\")\n```",
    "frame_1128.jpg": "```python\ntorch.manual_seed(1337)\nbatch_size = 4  # how many independent sequences will we process in parallel?\nblock_size = 8  # what is the maximum context length for predictions?\n\ndef get_batch(split):\n    # generate a small batch of data of inputs x and targets y\n    data = train_data if split == 'train' else val_data\n    ix = torch.randint(len(data) - block_size, (batch_size,))\n    x = torch.stack([data[i:i + block_size] for i in ix])\n    y = torch.stack([data[i + 1:i + block_size + 1] for i in ix])\n    return x, y\n\nxb, yb = get_batch('train')\nprint('inputs:')\nprint(xb.shape)\nprint(xb)\nprint('targets:')\nprint(yb.shape)\nprint(yb)\n\nprint('-----')\n\nfor b in range(batch_size):  # batch dimension\n    for t in range(block_size):  # time dimension\n        context = xb[b, :t + 1]\n        target = yb[b, t]\n        print(f\"When input is {context.tolist()} the target: {target}\")\n```",
    "frame_1132.jpg": "```python\ntorch.manual_seed(1337)\nbatch_size = 4  # how many independent sequences will we process in parallel?\nblock_size = 8  # what is the maximum context length for predictions?\n\ndef get_batch(split):\n    # generate a small batch of data of inputs x and targets y\n    data = train_data if split == 'train' else val_data\n    x = torch.randint(len(data) - block_size, (batch_size,))\n    x = torch.stack([data[i:i+block_size] for i in ix])\n    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n    return x, y\n\nxb, yb = get_batch('train')\nprint('inputs:')\nprint(xb)\nprint(xb.shape)\nprint('targets:')\nprint(yb.shape)\nprint(yb)\n\nfor b in range(batch_size):  # batch dimension\n    for t in range(block_size):  # time dimension\n        context = xb[b, :t+1]\n        target = yb[b, t]\n        print(f\"When input is {context.tolist()} the target: {target}\")\n```\n",
    "frame_1136.jpg": "```python\ntorch.manual_seed(1337)\nbatch_size = 4  # how many independent sequences will we process in parallel?\nblock_size = 8  # what is the maximum context length for predictions?\n\ndef get_batch(split):\n    # generate a small batch of data of inputs x and targets y\n    data = train_data if split == 'train' else val_data\n    ix = torch.randint(len(data) - block_size, (batch_size,))\n    x = torch.stack([data[i:i + block_size] for i in ix])\n    y = torch.stack([data[i + 1:i + block_size + 1] for i in ix])\n    return x, y\n\nxb, yb = get_batch('train')\nprint('inputs:')\nprint(xb.shape)\nprint(xb)\nprint('targets:')\nprint(yb.shape)\nprint(yb)\nprint('-----')\n\nfor b in range(batch_size):  # batch dimension\n    for t in range(block_size):  # time dimension\n        context = xb[b, :t + 1]\n        target = yb[b, t]\n        print(f\"When input is {context.tolist()} the target: {target}\")\n```",
    "frame_1147.jpg": "```python\ntorch.manual_seed(1337)\nbatch_size = 4  # how many independent sequences will we process in parallel?\nblock_size = 8  # what is the maximum context length for predictions?\n\ndef get_batch(split):\n    # generate a small batch of data of inputs x and targets y\n    data = train_data if split == 'train' else val_data\n    ix = torch.randint(len(data) - block_size, (batch_size,))\n    x = torch.stack([data[i:i + block_size] for i in ix])\n    y = torch.stack([data[i + 1:i + block_size + 1] for i in ix])\n    return x, y\n\nxb, yb = get_batch('train')\nprint('inputs:')\nprint(xb.shape)\nprint(xb)\nprint('targets:')\nprint(yb.shape)\nprint(yb)\n\nprint('-----')\n\nfor b in range(batch_size):  # batch dimension\n    for t in range(block_size):  # time dimension\n        context = xb[b, :t + 1]\n        target = yb[b, t]\n        print(f\"when input is {context.tolist()} the target: {target}\")\n```",
    "frame_1178.jpg": "```python\ndef get_batch(split):\n    # generate a small batch of data of inputs x and targets y\n    data = train_data if split == 'train' else val_data\n    ix = torch.randint(len(data), block_size, (batch_size,))\n    x = torch.stack([data[i:i+block_size] for i in ix])\n    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n    return x, y\n\nxb, yb = get_batch('train')\nprint(xb.shape)\nprint(xb)\nprint(yb.shape)\nprint(yb)\nprint('-----')\n\nfor b in range(batch_size):  # batch dimension\n    for t in range(block_size):  # time dimension\n        context = xb[b, t]\n        target = yb[b, t]\n        print(f\"when input is {context.tolist()} the target: {target}\")\n```\n",
    "frame_1196.jpg": "```python\ntorch.manual_seed(1337)\nbatch_size = 4  # how many independent sequences will we process in parallel?\nblock_size = 8  # what is the maximum context length for predictions?\n\ndef get_batch(split):\n    # generate a small batch of data of inputs x and targets y\n    data = train_data if split == 'train' else val_data\n    ix = torch.randint(len(data) - block_size, (batch_size,))\n    x = torch.stack([data[i:i + block_size] for i in ix])\n    y = torch.stack([data[i + 1:i + block_size + 1] for i in ix])\n    return x, y\n\nxb, yb = get_batch('train')\nprint(xb.shape)\nprint(yb.shape)\n\nfor b in range(batch_size):  # batch dimension\n    for t in range(block_size):  # time dimension\n        context = xb[b, :t + 1]\n        target = yb[b, t]\n        print(f\"when input is {context.tolist()} the target: {target}\")\n```\n",
    "frame_1217.jpg": "```python\ndef get_batch(split):\n    x = torch.stack([data[i:i+block_size] for i in ix])\n    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n    return x, y\n\nxb, yb = get_batch('train')\nprint(xb.shape)\nprint(xb)\nprint(yb.shape)\nprint(yb)\n    \nfor b in range(batch_size):  # batch dimension\n    for t in range(block_size):  # time dimension\n        context = xb[b, t+1]\n        target = yb[b, t]\n        print(f\"when input is {context.tolist()} the target: {target}\")\n    \ninputs = torch.tensor([[4, 8], [5, 6], [7, 9]])\n```",
    "frame_1219.jpg": "```python\ntorch.manual_seed(1337)\nbatch_size = 4  # how many independent sequences will we process in parallel?\nblock_size = 8  # what is the maximum context length for predictions?\n\ndef get_batch(split):\n    # generate a small batch of data of inputs x and targets y\n    data = train_data if split == 'train' else val_data\n    ix = torch.randint(len(data), (batch_size,))\n    x = torch.stack([data[i:i+block_size] for i in ix])\n    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n    return x, y\n\nxb, yb = get_batch('train')\nprint('inputs:')\nprint(xb.shape)\nprint('targets:')\nprint(yb.shape)\n```",
    "frame_1221.jpg": "```python\ntorch.manual_seed(1337)\nbatch_size = 4  # how many independent sequences will we process in parallel?\nblock_size = 8  # what is the maximum context length for predictions?\n\ndef get_batch(split):\n    # generate a small batch of data of inputs x and targets y\n    data = train_data if split == 'train' else val_data\n    ix = torch.randint(len(data) - block_size, (batch_size,))\n    x = torch.stack([data[i:i + block_size] for i in ix])\n    y = torch.stack([data[i + 1:i + block_size + 1] for i in ix])\n    return x, y\n\nxb, yb = get_batch('train')\nprint('inputs:')\nprint(xb.shape)\nprint('targets:')\nprint(yb.shape)\n```",
    "frame_1230.jpg": "```python\ntorch.manual_seed(1337)\nbatch_size = 4  # how many independent sequences will we process in parallel?\nblock_size = 8  # what is the maximum context length for predictions?\n\ndef get_batch(split):\n    # generate a small batch of data of inputs x and targets y\n    data = train_data if split == 'train' else val_data\n    ix = torch.randint(len(data) - block_size, (batch_size,))\n    x = torch.stack([data[i:i + block_size] for i in ix])\n    y = torch.stack([data[i + 1:i + block_size + 1] for i in ix])\n    return x, y\n\nxb, yb = get_batch('train')\nprint('inputs:')\nprint(xb.shape)\nprint(xb)\nprint('targets:')\nprint(yb.shape)\nprint(yb)\n\nprint('-----')\n```",
    "frame_1232.jpg": "```python\nx = torch.stack([data[i:i+block_size] for i in ix])\ny = torch.stack([data[i+1:i+block_size+1] for i in ix])\nreturn x, y\n\nxb, yb = get_batch('train')\nprint('inputs:')\nprint(xb.shape)\nprint(xb)\nprint('targets:')\nprint(yb.shape)\nprint(yb)\n\nfor b in range(batch_size):  # batch dimension\n    for t in range(block_size):  # time dimension\n        context = xb[b, t+t1]\n        target = yb[b, t]\n        print(f'when input is {context.tolist()} the target: {target}')\n```\n",
    "frame_1236.jpg": "```python\nxb, yb = get_batch('train')\nprint('inputs:')\nprint(xb.shape)\nprint(xb)\nprint('targets:')\nprint(yb.shape)\nprint(yb)\n\nprint('-----')\n\nfor b in range(batch_size):  # batch dimension\n    for t in range(block_size):  # time dimension\n        context = xb[b, t+1]\n        target = yb[b, t]\n        print(f\"when input is {context.tolist()} the target: {target}\")\n```",
    "frame_1237.jpg": "```python\ny = torch.stack([data[1+i:block_size+i+1] for i in ix])\nreturn x, y\n\nxb, yb = get_batch('train')\nprint('inputs:')\nprint(xb)\nprint('targets:')\nprint(yb.shape)\nprint(yb)\n\nprint('----')\n\nfor b in range(batch_size):  # batch dimension\n    for t in range(block_size):  # time dimension\n        context = xb[b, t+1]\n        target = yb[b, t]\n        print(f\"when input is {context.tolist()} the target: {target}\")\n```",
    "frame_1255.jpg": "```python\ny = torch.stack([data[i + 1:i + block_size + 1] for i in range(1)])\nxb, yb = get_batch('train')\nprint('inputs:')\nprint(xb.shape)\nprint('targets:')\nprint(yb.shape)\n\nprint('-----')\n\nfor b in range(batch_size):  # batch dimension\n    for t in range(block_size):  # time dimension\n        context = xb[b, t + 1]\n        target = yb[b, t]\n        print(f\"when input is {context.tolist()} the target: {target}\")\n```",
    "frame_1259.jpg": "```python\ny = torch.stack([data[i:i+block_size+1] for i in range(x1)])\n\nxb, yb = get_batch('train')\nprint('inputs:')\nprint(xb)\nprint('targets:')\nprint(yb)\n\nprint('----')\n\nfor b in range(batch_size):  # batch dimension\n    for t in range(block_size):  # time dimension\n        context = xb[b, t+1]\n        target = yb[b, t]\n        print(f\"when input is {context.tolist()} the target: {target}\")\n```",
    "frame_1262.jpg": "```python\ny = torch.stack([data[i:i+block_size+1] for i in range(len(data)-block_size)])\nreturn x, y\n\nxb, yb = get_batch('train')\nprint('inputs:')\nprint(xb.shape)\nprint('targets:')\nprint(yb.shape)\n\nfor b in range(batch_size): # batch dimension\n    for t in range(block_size): # time dimension\n        context = xb[b, t+1]\n        target = yb[b, t]\n        print(f\"when input is {context.tolist()} the target: {target}\")\n        \ninputs:\ntorch.Size([4, 8])\ntensor([[24, 43, 58, 5, 57, 1, 46, 43, 39],\n        [53, 56, 1, 58, 46, 39, 58, 1],\n        [58, 1, 58, 46, 39, 58, 1, 46],\n        [27, 10, 21, 1, 54, 39]])\n        \ntargets:\ntorch.Size([4, 8])\ntensor([[43, 56, 5, 57, 1, 46, 43, 39],\n        [53, 56, 1, 58, 46, 39, 58, 1],\n        [58, 1, 58, 39, 58, 1, 46, 1],\n        [17, 27, 10, 21, 1, 54, 39]])\n```",
    "frame_1265.jpg": "```python\nfor b in range(batch_size):  # batch dimension\n    for t in range(block_size):  # time dimension\n        context = xb[b, t+1]\n        target = yb[b, t]\n        print(f\"when input is {context.tolist()} the target: {target}\")\n```",
    "frame_1270.jpg": "```python\ninputs = torch.Size([4, 8])\ntensor([[24, 43, 58, 57, 1, 46, 43, 39],\n        [44, 53, 56, 1, 58, 39, 58, 1],\n        [52, 58, 1, 1, 54, 39, 1, 1]])\n\ntargets = torch.Size([4, 8])\ntensor([[43, 58, 57, 5, 1, 46, 43, 39],\n        [53, 56, 1, 1, 46, 39, 58, 1],\n        [58, 1, 58, 46, 39, 58, 1, 1],\n        [27, 10, 0, 21, 1, 54, 39, 1]])\n\n# Outputs similar information\nwhen input is [24] the target: 43\nwhen input is [24, 43] the target: 58\nwhen input is [24, 43, 58] the target: 5\nwhen input is [24, 43, 58, 57] the target: 57\nwhen input is [24, 43, 58, 57] the target: 1\n...\n```\n",
    "frame_1272.jpg": "```python\nprint('-----')\nfor b in range(batch_size):  # batch dimension\n    for t in range(block_size):  # time dimension\n        context = xb[b, t+1]\n        target = yb[b, t]\n        print(f\"when input is {context.tolist()} the target: {target}\")\n\ninputs:\ntorch.Size([4, 8])\ntensor([[24, 43, 58,  5, 57,  1, 46, 43],\n        [54,  6,  1,  58, 46, 39,  1,  0],\n        [52, 58,  1,  58, 46, 43,  1,  0],\n        [25, 27, 10,  0, 21,  1, 41,  1]])\n\ntargets:\ntorch.Size([4, 8])\ntensor([[43, 58,  5,  1, 46, 39,  1, 58],\n        [53, 56,  1, 46, 39,  1,  1, 46],\n        [58,  1, 58, 46, 39,  1,  46,  1],\n        [17, 27, 10,  0, 21,  1, 41,  1]])\n```\n",
    "frame_1303.jpg": "Code NA",
    "frame_1310.jpg": "```python\nfor b in range(batch_size):  # batch dimension\n    for t in range(block_size):  # time dimension\n        context = xb[b, t+1]\n        target = yb[b, t]\n        print(f\"when input is {context.tolist()} the target: {target}\")\n        \ninputs = torch.Size([4, 8])\ninputs_tensor = tensor([[24, 43, 58, 5, 57, 1, 46, 43],\n                         [44, 53, 56, 1, 58, 46, 39, 58],\n                         [52, 58, 1, 58, 46, 39, 58, 1],\n                         [25, 17, 27, 10, 0, 21, 1, 541]])\n\ntargets = torch.Size([4, 8])\ntargets_tensor = tensor([[43, 58, 5, 57, 1, 46, 43, 39],\n                          [53, 56, 1, 58, 46, 39, 58, 1],\n                          [58, 1, 58, 46, 51, 1, 54, 39],\n                          [17, 27, 10, 0, 21, 1, 54, 39]])\n```",
    "frame_1321.jpg": "```python\nfor b in range(batch_size):  # batch dimension\n    for t in range(block_size):  # time dimension\n        context = xb[b, t + 1]\n        target = yb[b, t]\n        print(f\"when input is {context.tolist()} the target: {target}\")\n\ninputs = torch.Size([4, 8])\ntensor([[24, 43, 58, 5, 57, 1, 46, 43],\n        [44, 53, 56, 1, 58, 49, 39, 51],\n        [52, 58, 1, 58, 46, 39, 58, 1],\n        [25, 17, 10, 0, 21, 1, 54]])\n\ntargets = torch.Size([4, 8])\ntensor([[43, 5, 57, 1, 46, 43, 39, 3],\n        [53, 56, 1, 58, 46, 39, 1],\n        [58, 1, 58, 46, 39, 1, 1],\n        [58, 1, 58, 46, 43, 1, 39]])\n```",
    "frame_1327.jpg": "```python\nprint(xb.shape)\nprint(xb)\nprint(yb.shape)\n\nprint('-----')\n\nfor b in range(batch_size):  # batch dimension\n    for t in range(block_size):  # time dimension\n        context = xb[b, t + 1]\n        target = yb[b, t]\n        print(f\"when input is {context.tolist()} the target: {target}\")\n```",
    "frame_1334.jpg": "```python\nprint(xb)  # our input to the transformer\ntensor([[24, 43, 58,  5, 57,  1, 46, 43],\n        [44, 53, 56,  1, 58, 46, 39, 58],\n        [52, 58,  1, 58, 46, 39, 58,  1],\n        [25, 17, 27, 10,  0, 21,  1, 54]])\n```",
    "frame_1349.jpg": "```python\nprint(xb) # our input to the transformer\ntensor([[24, 43, 58, 5, 57, 1, 46, 43],\n        [44, 53, 56, 1, 58, 46, 39, 58],\n        [52, 58, 1, 58, 46, 39, 58, 1],\n        [25, 17, 27, 10, 0, 21, 1, 54]])\n```",
    "frame_1355.jpg": "```python\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\ntorch.manual_seed(1337)\n\nclass BigramLanguageModel(nn.Module):\n    def __init__(self, vocab_size):\n        super().__init__()\n        # each token directly reads off the logits for the next token from a lookup table\n        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n\n    def forward(self, idx, targets):\n        # idx and targets are both (B,T) tensor of integers\n        logits = self.token_embedding_table(idx) # (B,T,C)\n\n        return logits\n\nm = BigramLanguageModel(vocab_size)\nout = m(xb, yb)\nprint(out.shape)\n```",
    "frame_1358.jpg": "```python\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\ntorch.manual_seed(1337)\n\nclass BigramLanguageModel(nn.Module):\n\n    def __init__(self, vocab_size):\n        super().__init__()\n        # each token directly reads off the logits for the next token from a lookup table\n        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n\n    def forward(self, idx, targets):\n        # idx and targets are both (B,T) tensor of integers\n        logits = self.token_embedding_table(idx) # (B,T,C)\n        return logits\n\nm = BigramLanguageModel(vocab_size)\nout = m(xb, yb)\nprint(out.shape)\n```",
    "frame_1406.jpg": "```python\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\ntorch.manual_seed(1337)\n\nclass BigramLanguageModel(nn.Module):\n\n    def __init__(self, vocab_size):\n        super().__init__()\n        # each token directly reads off the logits for the next token from a lookup table\n        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n\n    def forward(self, idx, targets):\n        # idx and targets are both (B,T) tensor of integers\n        logits = self.token_embedding_table(idx)  # (B,T,C)\n        return logits\n\nm = BigramLanguageModel(vocab_size)\nout = m(xb, yb)\nprint(out.shape)\n```",
    "frame_1425.jpg": "```python\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\ntorch.manual_seed(1337)\n\nclass BigramLanguageModel(nn.Module):\n    def __init__(self, vocab_size):\n        super().__init__()\n        # each token directly reads off the logits for the next token from a lookup table\n        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n    \n    def forward(self, idx, targets):\n        # idx and targets are both (B,T) tensor of integers\n        logits = self.token_embedding_table(idx) # (B,T,C)\n        return logits\n\nm = BigramLanguageModel(vocab_size)\nout = m(xb, yb)\nprint(out.shape)\n```",
    "frame_1438.jpg": "```python\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\ntorch.manual_seed(1337)\n\nclass BigramLanguageModel(nn.Module):\n\n    def __init__(self, vocab_size):\n        super().__init__()\n        # each token directly reads off the logits for the next token from a lookup table\n        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n\n    def forward(self, idx, targets):\n        # idx and targets are both (B,T) tensor of integers\n        logits = self.token_embedding_table(idx)  # (B,T,D)\n\n        return logits\n\nm = BigramLanguageModel(vocab_size)\nout = m(xb, yb)\nprint(out.shape)\n```",
    "frame_1462.jpg": "```python\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\ntorch.manual_seed(1337)\n\nclass BigramLanguageModel(nn.Module):\n\n    def __init__(self, vocab_size):\n        super().__init__()\n        # each token directly reads off the logits for the next token from a lookup table\n        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n\n    def forward(self, idx, targets):\n        # idx and targets are both (B,T) tensor of integers\n        logits = self.token_embedding_table(idx)  # (B,T,C)\n\n        return logits\n\nm = BigramLanguageModel(vocab_size)\nout = m(xb, yb)\n```",
    "frame_1486.jpg": "```python\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\ntorch.manual_seed(1337)\n\nclass BigramLanguageModel(nn.Module):\n\n    def __init__(self, vocab_size):\n        super().__init__()\n        # each token directly reads off the logits for the next token from a lookup table\n        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n\n    def forward(self, idx, targets):\n        # idx and targets are both (B,T) tensor of integers\n        logits = self.token_embedding_table(idx)  # (B,T,C)\n        return logits\n\nm = BigramLanguageModel(vocab_size)\nout = m(xb, yb)\nprint(out.shape)\n```",
    "frame_1491.jpg": "```python\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\ntorch.manual_seed(1337)\n\nclass BigramLanguageModel(nn.Module):\n\n    def __init__(self, vocab_size):\n        super().__init__()\n        # each token directly reads off the logits for the next token from a lookup table\n        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n\n    def forward(self, idx, targets):\n        # idx and targets are both (B,T) tensor of integers\n        logits = self.token_embedding_table(idx) # (B,T,C)\n        return logits\n\nm = BigramLanguageModel(vocab_size)\nout = m(xb, yb)\nprint(out.shape)\n\ntorch.Size([4, 8, 65])\n```",
    "frame_1501.jpg": "```python\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\ntorch.manual_seed(1337)\n\nclass BigramLanguageModel(nn.Module):\n\n    def __init__(self, vocab_size):\n        super().__init__()\n        # each token directly reads off the logits for the next token from a lookup table\n        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n\n    def forward(self, idx, targets):\n        # idx and targets are both (B,T) tensor of integers\n        logits = self.token_embedding_table(idx) # (B,T,C)\n\n        return logits\n\nm = BigramLanguageModel(vocab_size)\nout = m(xb, yb)\nprint(out.shape)\n```",
    "frame_1532.jpg": "```python\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\ntorch.manual_seed(1337)\n\nclass BigramLanguageModel(nn.Module):\n    def __init__(self, vocab_size):\n        super().__init__()\n        # each token directly reads off the logits for the next token from a lookup table\n        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n\n    def forward(self, idx, targets):\n        # idx and targets are both (B,T) tensor of integers\n        logits = self.token_embedding_table(idx)  # (B,T,C)\n        loss = F.cross_entropy(logits, targets)\n        return logits\n\nm = BigramLanguageModel(vocab_size)\nout = m(xb, yb)\nprint(out.shape)\n```",
    "frame_1555.jpg": "```python\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\ntorch.manual_seed(1337)\n\nclass BigramLanguageModel(nn.Module):\n    def __init__(self, vocab_size):\n        super().__init__()\n        # each token directly reads off the logits for the next token from a lookup table\n        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n\n    def forward(self, idx, targets):\n        # idx and targets are both (B,T) tensor of integers\n        logits = self.token_embedding_table(idx) # (B,T,C)\n        loss = F.cross_entropy(logits, targets)\n        return logits, loss\n\nm = BigramLanguageModel(vocab_size)\nlogits, loss = m(xb, yb)\nprint(logits.shape)\n```",
    "frame_1556.jpg": "```python\nm = BigramLanguageModel(vocab_size)\nlogits, loss = m(xb, yb)\nprint(logits.shape)\n```",
    "frame_1557.jpg": "```python\ndef forward(self, idx, targets):\n    # idx and targets are both (B,T) tensor of integers\n    logits = self.token_embedding_table(idx)  # (B,T,C)\n    loss = F.cross_entropy(logits, targets)\n\n    return logits, loss\n\nm = BigramLanguageModel(vocab_size)\nlogits, loss = m(xb, yb)\nprint(logits.shape)\n```",
    "frame_1561.jpg": "```python\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\ntorch.manual_seed(1337)\n\nclass BigramLanguageModel(nn.Module):\n\n    def __init__(self, vocab_size):\n        super().__init__()\n        # each token directly reads off the logits for the next token from a lookup table\n        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n\n    def forward(self, idx, targets):\n        # idx and targets are both (B,T) tensor of integers\n        logits = self.token_embedding_table(idx) # (B,T,C)\n\n        loss = F.cross_entropy(logits, targets)\n\n        return logits, loss\n\nm = BigramLanguageModel(vocab_size)\nlogits, loss = m(xb, yb)\nprint(logits.shape)\n```",
    "frame_1562.jpg": "Code NA",
    "frame_1607.jpg": "```python\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\ntorch.manual_seed(1337)\n\nclass BigramLanguageModel(nn.Module):\n\n    def __init__(self, vocab_size):\n        super().__init__()\n        # each token directly reads off the logits for the next token from a lookup table\n        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n\n    def forward(self, idx, targets):\n        # idx and targets are both (B,T) tensor of integers\n        logits = self.token_embedding_table(idx) # (B,T,C)\n\n        loss = F.cross_entropy(logits, targets)\n\n        return logits, loss\n\nm = BigramLanguageModel(vocab_size)\nlogits, loss = m(xb, yb)\nprint(logits.shape)\n```",
    "frame_1636.jpg": "```python\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\ntorch.manual_seed(1337)\n\nclass BigramLanguageModel(nn.Module):\n\n    def __init__(self, vocab_size):\n        super().__init__()\n        # each token directly reads off the logits for the next token from a lookup table\n        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n\n    def forward(self, idx, targets):\n        # idx and targets are both (B,T) tensor of integers\n        logits = self.token_embedding_table(idx) # (B,T,C)\n        \n        B, T, C = logits.shape\n        logits = logits.view(B*T, C)\n        loss = F.cross_entropy(logits, targets)\n\n        return logits, loss\n\nm = BigramLanguageModel(vocab_size)\nlogits, loss = m(xb, yb)\nprint(logits.shape)\n```",
    "frame_1646.jpg": "```python\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\ntorch.manual_seed(1337)\n\nclass BigramLanguageModel(nn.Module):\n\n    def __init__(self, vocab_size):\n        super().__init__()\n        # each token directly reads off the logits for the next token from a lookup table\n        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n\n    def forward(self, idx, targets):\n        # idx and targets are both (B,T) tensor of integers\n        logits = self.token_embedding_table(idx) # (B,T,C)\n        B, T, C = logits.shape\n        logits = logits.view(B*T, C)  # (constant) C: Any\n        loss = F.cross_entropy(logits, targets)\n\n        return logits, loss\n\nm = BigramLanguageModel(vocab_size)\nlogits, loss = m(xb, y)\nprint(logits.shape)\n```",
    "frame_1649.jpg": "```python\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\ntorch.manual_seed(1337)\n\nclass BigramLanguageModel(nn.Module):\n\n    def __init__(self, vocab_size):\n        super().__init__()\n        # each token directly reads off the logits for the next token from a lookup table\n        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n\n    def forward(self, idx, targets):\n        # idx and targets are both (B,T) tensor of integers\n        logits = self.token_embedding_table(idx) # (B,T,C)\n\n        B, T, C = logits.shape\n        logits = logits.view(B*T, C)\n        loss = F.cross_entropy(logits, targets)\n\n        return logits, loss\n\nm = BigramLanguageModel(vocab_size)\nlogits, loss = m(xb, yb)\nprint(logits.shape)\n```",
    "frame_1653.jpg": "Code NA",
    "frame_1656.jpg": "```python\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\ntorch.manual_seed(1337)\n\nclass BigramLanguageModel(nn.Module):\n    def __init__(self, vocab_size):\n        super().__init__()\n        # each token directly reads off the logits for the next token from a lookup table\n        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n\n    def forward(self, idx, targets):\n        # idx and targets are both (B,T) tensor of integers\n        logits = self.token_embedding_table(idx) # (B,T,C)\n        \n        B, T, C = logits.shape\n        logits = logits.view(B*T, C)\n        loss = F.cross_entropy(logits, targets)\n        \n        return logits, loss\n\nm = BigramLanguageModel(vocab_size)\nlogits, loss = m(xb, yb)\nprint(logits.shape)\n```",
    "frame_1678.jpg": "```python\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\ntorch.manual_seed(1337)\n\nclass BigramLanguageModel(nn.Module):\n    def __init__(self, vocab_size):\n        super().__init__()\n        # each token directly reads off the logits for the next token from a lookup table\n        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n\n    def forward(self, idx, targets):\n        # idx and targets are both (B, T) tensor of integers\n        logits = self.token_embedding_table(idx)  # (B,T,C)\n\n        B, T, C = logits.shape\n        logits = logits.view(B*T, C)\n        targets = targets.view(B*T)\n\n        loss = F.cross_entropy(logits, targets)\n\n        return logits, loss\n\nm = BigramLanguageModel(vocab_size)\nlogits, loss = m(xb, yb)\nprint(logits.shape)\n```",
    "frame_1681.jpg": "Code NA",
    "frame_1685.jpg": "```python\nfrom torch.nn import functional as F\ntorch.manual_seed(1337)\n\nclass BigramLanguageModel(nn.Module):\n\n    def __init__(self, vocab_size):\n        super().__init__()\n        # each token directly reads off the logits for the next token from a lookup table\n        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n\n    def forward(self, idx, targets):\n        # idx and targets are both (B,T) tensor of integers\n        logits = self.token_embedding_table(idx) # (B,T,C)\n\n        B, T, C = logits.shape\n        logits = logits.view(B*T, C)\n        targets = targets.view(B*T)\n        loss = F.cross_entropy(logits, targets)\n\n        return logits, loss\n\nm = BigramLanguageModel(vocab_size)\nlogits, loss = m(xb, yb)\nprint(logits.shape)\n```",
    "frame_1686.jpg": "```python\ntargets = targets.view(BK*T)\nloss = F.cross_entropy(logits, targets)\n\nreturn logits, loss\n\nm = BigramLanguageModel(vocab_size)\nlogits, loss = m(xb, yb)\nprint(logits.shape)\n\ntorch.Size([32, 65])\n```",
    "frame_1687.jpg": "```python\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\ntorch.manual_seed(1337)\n\nclass BigramLanguageModel(nn.Module):\n\n    def __init__(self, vocab_size):\n        super().__init__()\n        # each token directly reads off the logits for the next token from a lookup table\n        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n\n    def forward(self, idx, targets):\n        # idx and targets are both (B,T) tensor of integers\n        logits = self.token_embedding_table(idx) # (B,T,C)\n\n        B, T, C = logits.shape\n        logits = logits.view(B*T, C)\n        targets = targets.view(B*T)\n        loss = F.cross_entropy(logits, targets)\n\n        return logits, loss\n\nm = BigramLanguageModel(vocab_size)\nlogits, loss = m(xb, yb)\nprint(logits.shape)\n```",
    "frame_1690.jpg": "```python\nimport torch.nn as nn\nfrom torch.nn import functional as F\ntorch.manual_seed(1337)\n\nclass BigramLanguageModel(nn.Module):\n\n    def __init__(self, vocab_size):\n        super().__init__()\n        # each token directly reads off the logits for the next token from a lookup table\n        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n\n    def forward(self, idx, targets):\n        # idx and targets are both (B,T) tensor of integers\n        logits = self.token_embedding_table(idx) # (B,T,C)\n        \n        B, T, C = logits.shape\n        logits = logits.view(B*T, C)\n        targets = targets.view(B*T)\n        loss = F.cross_entropy(logits, targets)\n\n        return logits, loss\n\nm = BigramLanguageModel(vocab_size)\nlogits, loss = m(xb, yb)\nprint(logits.shape)\nprint(loss)\n\ntorch.Size([32, 65])\ntensor(4.8786, grad_fn=<NllLossBackward0>)\n```",
    "frame_1696.jpg": "```python\nfrom torch.nn import functional as F\ntorch.manual_seed(1337)\n\nclass BigramLanguageModel(nn.Module):\n\n    def __init__(self, vocab_size):\n        super().__init__()\n        # each token directly reads off the logits for the next token from a lookup table\n        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n\n    def forward(self, idx, targets):\n        # idx and targets are both (B,T) tensor of integers\n        logits = self.token_embedding_table(idx)  # (B,T,C)\n\n        B, T, C = logits.shape\n        logits = logits.view(B*T, C)\n        targets = targets.view(B*T)\n        loss = F.cross_entropy(logits, targets)\n\n        return logits, loss\n\nm = BigramLanguageModel(vocab_size)\nlogits, loss = m(xb, yb)\nprint(logits.shape)\nprint(loss)\n```",
    "frame_1722.jpg": "```python\nfrom torch.nn import functional as F\ntorch.manual_seed(1337)\n\nclass BigramLanguageModel(nn.Module):\n\n    def __init__(self, vocab_size):\n        super().__init__()\n        # each token directly reads off the logits for the next token from a lookup table\n        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n\n    def forward(self, idx, targets):\n        # idx and targets are both (B,T) tensor of integers\n        logits = self.token_embedding_table(idx) # (B,T,C)\n\n        B, T, C = logits.shape\n        logits = logits.view(B*T, C)\n        targets = targets.view(B*T)\n        loss = F.cross_entropy(logits, targets)\n\n        return logits, loss\n\nm = BigramLanguageModel(vocab_size)\nlogits, loss = m(xb, yb)\nprint(logits.shape)\nprint(loss)\n```",
    "frame_1724.jpg": "```python\nfrom torch.nn import functional as F\ntorch.manual_seed(1337)\n\nclass BigramLanguageModel(nn.Module):\n\n    def __init__(self, vocab_size):\n        super().__init__()\n        # each token directly reads off the logits for the next token from a lookup table\n        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n\n    def forward(self, idx, targets):\n        # idx and targets are both (B,T) tensor of integers\n        logits = self.token_embedding_table(idx)  # (B,T,C)\n\n        B, T, C = logits.shape\n        logits = logits.view(B*T, C)\n        targets = targets.view(B*T)\n        loss = F.cross_entropy(logits, targets)\n\n        return logits, loss\n\nm = BigramLanguageModel(vocab_size)\nlogits, loss = m(xb, yb)\nprint(logits.shape)\nprint(loss)\n```",
    "frame_1728.jpg": "```python\nfrom torch.nn import functional as F\ntorch.manual_seed(1337)\n\nclass BigramLanguageModel(nn.Module):\n\n    def __init__(self, vocab_size):\n        super().__init__()\n        # each token directly reads off the logits for the next token from a lookup table\n        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n\n    def forward(self, idx, targets):\n        # idx and targets are both (B,T) tensor of integers\n        logits = self.token_embedding_table(idx) # (B,T,C)\n\n        B, T, C = logits.shape\n        logits = logits.view(B, T, C)\n        targets = targets.view(B*T)\n        loss = F.cross_entropy(logits, targets)\n\n        return logits, loss\n\nm = BigramLanguageModel(vocab_size)\nlogits, loss = m(xb, yb)\nprint(logits.shape)\nprint(loss)\n```",
    "frame_1741.jpg": "```python\nimport torch.nn as nn\nfrom torch.nn import functional as F\ntorch.manual_seed(1337)\n\nclass BigramLanguageModel(nn.Module):\n    def __init__(self, vocab_size):\n        super().__init__()\n        # each token directly reads off the logits for the next token from a lookup table\n        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n        \n    def forward(self, idx, targets):\n        # idx and targets are both (B,T) tensor of integers\n        logits = self.token_embedding_table(idx) # (B,T,C)\n        \n        B, T, C = logits.shape\n        logits = logits.view(B*T, C)\n        targets = targets.view(B*T)\n        loss = F.cross_entropy(logits, targets)\n\n        return logits, loss\n\nm = BigramLanguageModel(vocab_size)\nlogits, loss = m(xb, yb)\nprint(logits.shape)\nprint(loss)\n```",
    "frame_1746.jpg": "```python\nfrom torch.nn import functional as F\nimport torch\n\nclass BigramLanguageModel(nn.Module):\n    def __init__(self, vocab_size):\n        super().__init__()\n        # each token directly reads off the logits for the next token from a lookup table\n        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n\n    def forward(self, idx, targets):\n        # idx and targets are both (B,T) tensor of integers\n        logits = self.token_embedding_table(idx)  # (B,T,C)\n        B, T, C = logits.shape\n        logits = logits.view(B*T, C)\n        targets = targets.view(B*T)\n        loss = F.cross_entropy(logits, targets)\n        return logits, loss\n\nm = BigramLanguageModel(vocab_size)\nlogits, loss = m(xb, yb)\nprint(logits.shape)\nprint(loss)\n```",
    "frame_1749.jpg": "```python\nclass BigramLanguageModel(nn.Module):\n    def __init__(self, vocab_size):\n        super().__init__()\n        # each token directly reads off the logits for the next token from a lookup table\n        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n\n    def forward(self, idx, targets):\n        # idx and targets are both (B,T) tensor of integers\n        logits = self.token_embedding_table(idx)  # (B,T,C)\n        B, T, C = logits.shape\n        logits = logits.view(B, T, C)\n        targets = targets.view(B, T)\n        loss = F.cross_entropy(logits, targets)\n        \n        return logits, loss\n\n    def generate(self, idx, max_new_tokens):\n        # idx is (B, T) array of indices in the current context\n        for _ in range(max_new_tokens):\n            # get the predictions\n            logits, loss = self(idx)\n            # focus only on the last time step\n            logits = logits[:, -1, :]  # becomes (B, C)\n            # apply softmax to get probabilities\n            probs = F.softmax(logits, dim=-1)  # (B, C)\n            # sample from the distribution\n            idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n            # append sampled index to the running sequence\n            idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n        return idx\n```",
    "frame_1758.jpg": "```python\ndef __init__(self, vocab_size):\n    super().__init__()\n    # each token directly reads off the logits for the next token from a lookup table\n    self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n\ndef forward(self, idx, targets):\n    # idx and targets are both (B,T) tensor of integers\n    logits = self.token_embedding_table(idx)  # (B,T,C)\n\n    B, T, C = logits.shape\n    logits = logits.view(B * T, C)\n    targets = targets.view(B * T)\n    loss = F.cross_entropy(logits, targets)\n\n    return logits, loss\n\ndef generate(self, idx, max_new_tokens):\n    # idx is (B, T) array of indices in the current context\n    for _ in range(max_new_tokens):\n        # get the predictions\n        logits, loss = self(idx)  # focus only on the last time step\n        logits = logits[:, -1, :]  # becomes (B, C)\n        # apply softmax to get probabilities\n        probs = F.softmax(logits, dim=-1)  # (B, C)\n        # sample from the distribution\n        idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n        # append sampled index to the running sequence\n        idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n    return idx\n```\n",
    "frame_1764.jpg": "```python\ndef forward(self, idx, targets):\n    # idx and targets are both (B,T) tensor of integers\n    logits = self.token_embedding_table(idx) # (B,T,C)\n    B, T, C = logits.shape\n    logits = logits.view(B*T, C)\n    targets = targets.view(B*T)\n    loss = F.cross_entropy(logits, targets)\n\n    return logits, loss\n\ndef generate(self, idx, max_new_tokens):\n    # idx is (B, T) array of indices in the current context\n    for _ in range(max_new_tokens):\n        # get the predictions\n        logits, loss = self(idx)\n        # focus only on the last time step\n        logits = logits[:, -1, :] # becomes (B, C)\n        # apply softmax to get probabilities\n        probs = F.softmax(logits, dim=-1) # (B, C)\n        # sample from the distribution\n        idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n        idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n\n    return idx\n```",
    "frame_1774.jpg": "```python\ndef forward(self, idx, targets):\n    logits = self.token_embedding_table(idx)  # (B,T,C)\n    B, T, C = logits.shape\n    logits = logits.view(B*T, C)\n    targets = targets.view(B*T)\n    loss = F.cross_entropy(logits, targets)\n    return logits, loss\n\ndef generate(self, idx, max_new_tokens):\n    # idx is (B, T) array of indices in the current context\n    for _ in range(max_new_tokens):\n        logits, loss = self(idx)  # get the predictions\n        logits = logits[:, -1, :]  # becomes (B, C)\n        # apply softmax to get probabilities\n        probs = F.softmax(logits, dim=-1)  # (B, C)\n        # sample from the distribution\n        idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n        idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n    return idx\n\nm = BigramLanguageModel(vocab_size)\nlogits, loss = m(xb, yb)\nprint(logits.shape)\n```",
    "frame_1776.jpg": "```python\ndef forward(self, idx, targets):\n    logits = self.token_embedding_table(idx)  # (B, T, C)\n    B, T, C = logits.shape\n    logits = logits.view(B*T, C)\n    targets = targets.view(B*T)\n    loss = F.cross_entropy(logits, targets)\n\n    return logits, loss\n\ndef generate(self, idx, max_new_tokens):\n    # idx is (B, T) array of indices in the current context\n    for _ in range(max_new_tokens):\n        # get the predictions\n        logits, loss = self(idx)\n        # focus only on the last time step\n        logits = logits[:, -1, :]  # becomes (B, C)\n        # apply softmax to get probabilities\n        probs = F.softmax(logits, dim=-1)  # (B, C)\n        # sample from the distribution\n        idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n        idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n\n    return idx\n```",
    "frame_1780.jpg": "```python\ndef forward(self, idx, targets):\n    logits = self.token_embedding_table(idx)  # (B,T,C)\n    targets = targets.view(B*T)\n    loss = F.cross_entropy(logits, targets)\n    return logits, loss\n\ndef generate(self, idx, max_new_tokens):\n    for _ in range(max_new_tokens):\n        logits, loss = self(idx)  # get the predictions\n        logits = logits[:, -1, :]  # becomes (B, C)\n        probs = F.softmax(logits, dim=-1)  # (B, C)\n        idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n        idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n    return idx\n```",
    "frame_1783.jpg": "```python\ndef forward(self, idx, targets):\n    # idx and targets are both (B,T) tensor of integers\n    logits = self.token_embedding_table(idx)  # (B,T,C)\n    B, T, C = logits.shape\n    logits = logits.view(B*T, C)\n    targets = targets.view(B*T)\n    loss = F.cross_entropy(logits, targets)\n\n    return logits, loss\n\ndef generate(self, idx, max_new_tokens):\n    # idx is (B, T) array of indices in the current context\n    for _ in range(max_new_tokens):\n        # get the predictions\n        logits, loss = self(idx)\n        # focus only on the last time step\n        logits = logits[:, -1, :]  # becomes (B, C)\n        # apply softmax to get probabilities\n        probs = F.softmax(logits, dim=-1)  # (B, C)\n        # sample from the distribution\n        idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n        # append sampled indices to the running sequence\n        idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n    return idx\n```\n",
    "frame_1784.jpg": "```python\ndef generate(self, idx, max_new_tokens):\n    # idx is (B, T) array of indices in the current context\n    for _ in range(max_new_tokens):\n        # get the predictions\n        logits, loss = self(idx)\n        # focus only on the last time step\n        logits = logits[:, -1, :]  # becomes (B, C)\n        # apply softmax to get probabilities\n        probs = F.softmax(logits, dim=-1)  # (B, C)\n        # sample from the distribution\n        idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n        # append sampled indices to the running sequence\n        idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n    return idx\n\nm = BigramLanguageModel(vocab_size)\nlogits, loss = m(xb, yb)\nprint(logits.shape)\nprint(loss)\n```",
    "frame_1786.jpg": "```python\n# each token directly reads off the logits for the next token from a lookup table\nself.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n\ndef forward(self, idx, targets):\n    # idx and targets are both (B,T) tensor of integers\n    logits = self.token_embedding_table(idx) # (B,T,C)\n\n    B, T, C = logits.shape\n    logits = logits.view(B*T, C)\n    targets = targets.view(B*T)\n    loss = F.cross_entropy(logits, targets)\n\n    return logits, loss\n\ndef generate(self, idx, max_new_tokens):\n    # idx is (B, T) array of indices in the current context\n    for _ in range(max_new_tokens):\n        # get the predictions\n        logits, loss = self(idx)\n        # focus only on the last time step\n        logits = logits[:, -1, :]  # becomes (B, C)\n        # apply softmax to get probabilities\n        probs = F.softmax(logits, dim=-1)  # (B, C)\n        # sample from the distribution\n        idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n        # append sampled index to the running sequence\n        idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n    return idx\n\nm = BigramLanguageModel(vocab_size)\nlogits, loss = m(xb, yb)\nprint(logits.shape)\nprint(loss)\n```",
    "frame_1802.jpg": "```python\n# each token directly reads off the logits for the next token from a lookup table\nself.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n\ndef forward(self, idx, targets):\n    # idx and targets are both (B,T) tensor of integers\n    logits = self.token_embedding_table(idx) # (B,T,C)\n\n    B, T, C = logits.shape\n    logits = logits.view(B*T, C)\n    targets = targets.view(B*T)\n    loss = F.cross_entropy(logits, targets)\n\n    return logits, loss\n\ndef generate(self, idx, max_new_tokens):\n    # idx is (B, T) array of indices in the current context\n    for _ in range(max_new_tokens):\n        # get the predictions\n        logits, loss = self(idx)\n        # focus only on the last time step\n        logits = logits[:, -1, :]  # becomes (B, C)\n        # apply softmax to get probabilities\n        probs = F.softmax(logits, dim=-1)  # (B, C)\n        # sample from the distribution\n        idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n        # append sampled index to the running sequence\n        idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n\n    return idx\n\nm = BigramLanguageModel(vocab_size)\nlogits, loss = m(xb, yb)\nprint(logits.shape)\nprint(loss)\n```",
    "frame_1808.jpg": "```python\n# each token directly reads off the logits for the next token from a lookup table\nself.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n\ndef forward(self, idx, targets):\n    # idx and targets are both (B, T) tensor of integers\n    logits = self.token_embedding_table(idx) # (B,T,C)\n    B, T, C = logits.shape\n    logits = logits.view(B, T, C)\n    targets = targets.view(B, T)\n    loss = F.cross_entropy(logits, targets)\n\n    return logits, loss\n\ndef generate(self, idx, max_new_tokens):\n    # idx is (B, T) array of indices in the current context\n    for _ in range(max_new_tokens):\n        # get the predictions\n        logits, loss = self(idx)\n        # focus only on the last time step\n        logits = logits[:, -1, :]  # becomes (B, C)\n        # apply softmax to get probabilities\n        probs = F.softmax(logits, dim=-1)  # (B, C)\n        # sample from the distribution\n        idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n        # append sampled index to the running sequence\n        idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n    return idx\n\nm = BigramLanguageModel(vocab_size)\nlogits, loss = m(x, y)\nprint(logits.shape)\nprint(loss)\n```",
    "frame_1811.jpg": "```python\nself.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n\ndef forward(self, idx, targets):\n    # idx and targets are both (B,T) tensor of integers\n    logits = self.token_embedding_table(idx)  # (B,T,C)\n\n    B, T, C = logits.shape\n    logits = logits.view(B, T, C)\n    targets = targets.view(B*T)\n    loss = F.cross_entropy(logits, targets)\n\n    return logits, loss\n\ndef generate(self, idx, max_new_tokens):\n    # idx is (B, T) array of indices in the current context\n    for _ in range(max_new_tokens):\n        # get the predictions\n        logits, loss = self.forward(idx)\n        # focus only on the last time step\n        logits = logits[:, -1, :]  # becomes (B, C)\n        # apply softmax to get probabilities\n        probs = F.softmax(logits, dim=-1)  # (B, C)\n        # sample from the distribution\n        idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n        # append sampled index to the running sequence\n        idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n\n    return idx\n\nm = BigramLanguageModel(vocab_size)\nlogits, loss = m(xb, yb)\nprint(logits.shape)\n```",
    "frame_1842.jpg": "```python\nself.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n\ndef forward(self, idx, targets):\n    # idx and targets are both (B,T) tensor of integers\n    logits = self.token_embedding_table(idx) # (B,T,C)\n    \n    B, T, C = logits.shape\n    logits = logits.view(B*T, C)\n    targets = targets.view(B*T)\n    loss = F.cross_entropy(logits, targets)\n    \n    return logits, loss\n\ndef generate(self, idx, max_new_tokens):\n    # idx is (B, T) array of indices in the current context\n    for _ in range(max_new_tokens):\n        # get the predictions\n        logits, loss = self(idx)\n        # focus only on the last time step\n        logits = logits[:, -1, :]  # becomes (B, C)\n        # apply softmax to get probabilities\n        probs = F.softmax(logits, dim=-1)  # (B, C)\n        # sample from the distribution\n        idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n        # append sampled index to the running sequence\n        idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n\n    return idx\n\nm = BigramLanguageModel(vocab_size)\nlogits, loss = m(xb, yb)\nprint(logits.shape)\n```",
    "frame_1850.jpg": "```python\nself.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n\ndef forward(self, idx, targets):\n    # idx and targets are both (B,T) tensor of integers\n    logits = self.token_embedding_table(idx)  # (B,T,C)\n    B, T, C = logits.shape\n    logits = logits.view(B*T, C)\n    targets = targets.view(B*T)\n    loss = F.cross_entropy(logits, targets)\n    return logits, loss\n\ndef generate(self, idx, max_new_tokens):\n    # idx is (B, T) array of indices in the current context\n    for _ in range(max_new_tokens):\n        # get the predictions\n        logits, loss = self(idx)\n        # focus only on the last time step\n        logits = logits[:, -1, :]  # becomes (B, C)\n        # apply softmax to get probabilities\n        probs = F.softmax(logits, dim=-1)  # (B, C)\n        # sample from the distribution\n        idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n        # append sampled index to the running sequence\n        idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n    return idx\n\nm = BigramLanguageModel(vocab_size)\nlogits, loss = m(xb, yb)\nprint(logits.shape)\nprint(loss)\n```",
    "frame_1878.jpg": "```\n# idx and targets are both (B,T) tensor of integers\nlogits = self.token_embedding_table(idx) # (B,T,C)\n\nB, T, C = logits.shape\nlogits = logits.view(B*T, C)\ntargets = targets.view(B*T)\nloss = F.cross_entropy(logits, targets)\n\nreturn logits, loss\n\ndef generate(self, max_new_tokens):\n    # idx is (B, T) array of indices in the current context\n    for _ in range(max_new_tokens):\n        # get the predictions\n        logits = logits[:,-1,:] # becomes (B, C)\n        # apply softmax to get probabilities\n        probs = F.softmax(logits, dim=-1) # (B, C)\n        # sample from the distribution\n        idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n        # append sampled index (variable) idx_next: Tensor\n        idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n    return idx\n\nm = BigramLanguageModel(vocab_size)\nlogits, loss = m(xb, yb)\nprint(logits.shape)\nprint(loss)\n```",
    "frame_1885.jpg": "```python\nlogits = self.token_embedding_table(idx)  # (B,T,C)\n\nB, T, C = logits.shape\nlogits = logits.view(B*T, C)\ntargets = targets.view(B*T)\nloss = F.cross_entropy(logits, targets)\n\nreturn logits, loss\n\ndef generate(self, idx, max_new_tokens):\n    # idx is (B, T) array of indices in the current context\n    for _ in range(max_new_tokens):\n        # get the predictions\n        logits, loss = self(idx)\n        # focus only on the last time step\n        logits = logits[:, -1, :]  # becomes (B, C)\n        # apply softmax to get probabilities\n        probs = F.softmax(logits, dim=-1)  # (B, C)\n        # sample from the distribution\n        idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n        # append sampled index to the running sequence\n        idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n\n    return idx\n\nm = BigramLanguageModel(vocab_size)\nlogits, loss = m(xb, yb)\nprint(logits.shape)\nprint(loss)\n\ntorch.Size([32, 65])\ntensor(4.8769, grad_fn=<NllLossBackward>)\n```",
    "frame_1888.jpg": "```python\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\ntorch.manual_seed(1337)\n\nclass BigramLanguageModel(nn.Module):\n    def __init__(self, vocab_size):\n        super().__init__()\n        # each token directly reads off the logits for the next token from a lookup table\n        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n\n    def forward(self, idx, targets):\n        # idx and targets are both (B, T) tensor of integers\n        logits = self.token_embedding_table(idx)  # (B, T, C)\n        B, T, C = logits.shape\n        logits = logits.view(B * T, C)\n        targets = targets.view(B * T)\n        loss = F.cross_entropy(logits, targets)\n\n        return logits, loss\n\n    def generate(self, idx, max_new_tokens):\n        # idx is (B, T) array of indices in the current context\n        for _ in range(max_new_tokens):\n            # get the predictions\n            logits, loss = self(idx)  # focus only on the last time step\n            logits = logits[:, -1, :]  # becomes (B, C)\n            # apply softmax to get probabilities\n            probs = F.softmax(logits, dim=-1)  # (B, C)\n            # sample from the distribution\n            idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n```",
    "frame_1904.jpg": "```python\nimport torch.nn as nn\nfrom torch.nn import functional as F\n\nclass BigramLanguageModel(nn.Module):\n\n    def __init__(self, vocab_size):\n        super().__init__()\n        # each token directly reads off the logits for the next token from a lookup table\n        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n\n    def forward(self, idx, targets=None):\n        # idx and targets are both (B,T) tensor of integers\n        logits = self.token_embedding_table(idx) # (B,T,C)\n\n        if targets is not None:\n            B, T, C = logits.shape\n            logits = logits.view(B*T, C)\n            targets = targets.view(B*T)\n            loss = F.cross_entropy(logits, targets)\n\n            return logits, loss\n\n    def generate(self, idx, max_new_tokens):\n        # idx is (B, T) array of indices in the current context\n        for _ in range(max_new_tokens):\n            logits, loss = self(idx)\n            # focus only on the last time step\n            logits = logits[:, -1, :] # becomes (B, C)\n            # apply softmax to get probabilities\n            probs = F.softmax(logits, dim=-1) # (B, C)\n            # sample from the distribution\n```",
    "frame_1914.jpg": "```python\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\ntorch.manual_seed(1337)\n\nclass BigramLanguageModel(nn.Module):\n\n    def __init__(self, vocab_size):\n        super().__init__()\n        # each token directly reads off the logits for the next token from a lookup table\n        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n\n    def forward(self, idx, targets=None):\n        # idx and targets are both (B,T) tensor of integers\n        logits = self.token_embedding_table(idx)  # (B,T,C)\n\n        if targets is None:\n            loss = None\n        else:\n            B, T, C = logits.shape\n            logits = logits.view(B*T, C)\n            targets = targets.view(B*T)\n            loss = F.cross_entropy(logits, targets)\n\n        return logits, loss\n\n    def generate(self, idx, max_new_tokens):\n        # idx is (B, T) array of indices in the current context\n        for _ in range(max_new_tokens):\n            # get the predictions\n            logits, _ = self(idx) \n            # focus only on the last time step\n            logits = logits[:, -1, :]  # becomes (B, C)\n            # apply softmax to get probabilities\n            probs = F.softmax(logits, dim=-1)  # (B, C)\n```\n",
    "frame_1915.jpg": "```python\ndef forward(self, idx, targets=None):\n    # idx and targets are both (B,T) tensor of integers\n    logits = self.token_embedding_table(idx)  # (B,T,C)\n\n    if targets is None:\n        loss = None\n    else:\n        B, T, C = logits.shape\n        logits = logits.view(B*T, C)\n        targets = targets.view(B*T)\n        loss = F.cross_entropy(logits, targets)\n\n    return logits, loss\n\ndef generate(self, idx, max_new_tokens):\n    # idx is (B, T) array of indices in the current context\n    for _ in range(max_new_tokens):\n        # get the predictions\n        logits, loss = self(idx)\n        # focus only on the last time step\n        logits = logits[:, -1, :]  # becomes (B, C)\n        # apply softmax to get probabilities\n        probs = F.softmax(logits, dim=-1)  # (B, C)\n        # sample from the distribution\n        idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n        # append sampled indices to the running sequence\n        idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n    \n    return idx\n```",
    "frame_1918.jpg": "Here is the extracted code from the image:\n\n```python\ndef forward(self, idx, targets=None):\n    # idx and targets are both (B,T) tensor of integers\n    logits = self.token_embedding_table(idx) # (B,T,C)\n\n    if targets is None:\n        loss = None\n    else:\n        B,T,C = logits.shape\n        logits = logits.view(B*T, C)\n        targets = targets.view(B*T)\n        loss = F.cross_entropy(logits, targets)\n\n    return logits, loss\n\ndef generate(self, idx, max_new_tokens):\n    # idx is (B, T) array of indices in the current context\n    for _ in range(max_new_tokens):\n        # get the predictions\n        logits, loss = self(idx) \n        # focus only on the last time step\n        logits = logits[:, -1, :] # becomes (B, C)\n        # apply softmax to get probabilities\n        probs = F.softmax(logits, dim=-1) # (B, C)\n        # sample from the distribution\n        idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n        # append sampled index to the running sequence\n        idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n\n    return idx\n```\n\nLet me know if you need anything else!",
    "frame_1922.jpg": "```python\n# idx and targets are both (B,T) tensor of integers\nlogits = self.token_embedding_table(idx) # (B,T,C)\n\nif targets is None:\n    loss = None\nelse:\n    B, T, C = logits.shape\n    logits = logits.view(B*T, C)\n    targets = targets.view(B*T)\n    loss = F.cross_entropy(logits, targets)\n\nreturn logits, loss\n\ndef generate(self, idx, max_new_tokens):\n    # idx is (B, T) array of indices in the current context\n    for _ in range(max_new_tokens):\n        # get the predictions\n        logits, loss = self(idx)\n        # focus only on the last time step\n        logits = logits[:, -1, :] # becomes (B, C)\n        # apply softmax to get probabilities\n        probs = F.softmax(logits, dim=-1) # (B, C)\n        # sample from the distribution\n        idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n        # append sampled index to the running sequence\n        idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n\n    return idx\n\nm = BigramLanguageModel(vocab_size)\nlogits, loss = m(xb, yb)\nprint(logits.shape)\nprint(loss)\n```",
    "frame_1929.jpg": "```python\nB, T, C = logits.shape\nlogits = logits.view(B*T, C)\ntargets = targets.view(B*T)\nloss = F.cross_entropy(logits, targets)\n\nreturn logits, loss\n\ndef generate(self, idx, max_new_tokens):\n    # idx is (B, T) array of indices in the current context\n    for _ in range(max_new_tokens):\n        # get the predictions\n        logits, loss = self(idx)\n        # focus only on the last time step\n        logits = logits[:, -1, :]  # becomes (B, C)\n        # apply softmax to get probabilities\n        probs = F.softmax(logits, dim=-1)  # (B, C)\n        # sample from the distribution\n        idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n        # append sampled index to the running sequence\n        idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n\n    return idx\n\nm = BigramLanguageModel(vocab_size)\nlogits, loss = m(xb, yb)\nprint(logits.shape)\nprint(loss)\n\ntorch.Size([32, 65])\ntensor(4.8786, grad_fn=<NllLossBackward>)\n```",
    "frame_1934.jpg": "```python\ndef generate(self, idx, max_new_tokens):\n    # idx is (B, T) array of indices in the current context\n    for _ in range(max_new_tokens):\n        # get the predictions\n        logits, loss = self(idx)\n        # focus only on the last time step\n        logits = logits[-1, :, :]  # becomes (B, C)\n        # apply softmax to get probabilities\n        probs = F.softmax(logits, dim=-1)  # (B, C)\n        # sample from the distribution\n        idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n        # append sampled index to the running sequence\n        idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n    return idx\n\nm = BigramLanguageModel(vocab_size)\nlogits, loss = m(xb, yb)\nprint(logits.shape)\nprint(loss)\nprint(decode(m.generate(torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))\n```\n",
    "frame_1937.jpg": "Here is the extracted developer code from the image:\n\n```python\ndef generate(self, idx, max_new_tokens):\n    # idx is (B, T) array of indices in the current context\n    for _ in range(max_new_tokens):\n        # get the predictions\n        logits, loss = self(idx)\n        # focus only on the last time step\n        logits = logits[:, -1, :]  # becomes (B, C)\n        # apply softmax to get probabilities\n        probs = F.softmax(logits, dim=-1)  # (B, C)\n        # sample from the distribution\n        idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n        # append sampled index to the running sequence\n        idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n    return idx\n\nm = BigramLanguageModel(vocab_size)\nlogits, loss = m(xb, yb)\nprint(logits.shape)\nprint(loss)\n\nprint(decode(m.generate(torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))\n```",
    "frame_1948.jpg": "```python\ndef generate(self, idx, max_new_tokens):\n    # idx is (B, T) array of indices in the current context\n    for _ in range(max_new_tokens):\n        # get the predictions\n        logits, loss = self(idx)\n        # focus only on the last time step\n        logits = logits[:, -1, :]  # becomes (B, C)\n        # apply softmax to get probabilities\n        probs = F.softmax(logits, dim=-1)  # (B, C)\n        # sample from the distribution\n        idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n        # append sampled index to the running sequence\n        idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n    return idx\n\nm = BigramLanguageModel(vocab_size)\nlogits, loss = m(xb, yb)\nprint(logits.shape)\nprint(loss)\n\nidx = torch.zeros((1, 1), dtype=torch.long)\nprint(decode(m.generate(idx, max_new_tokens=100)[0].tolist()))\n```",
    "frame_1951.jpg": "```python\ndef generate(self, idx, max_new_tokens):\n    # idx is (B, T) array of indices in the current context\n    for _ in range(max_new_tokens):\n        # get the predictions\n        logits, loss = self(idx)  # focus only on the last time step\n        logits = logits[:, -1, :]  # becomes (B, C)\n        \n        # apply softmax to get probabilities\n        probs = F.softmax(logits, dim=-1)  # (B, C)\n        \n        # sample from the distribution\n        idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n        \n        # append sampled index to the running sequence\n        idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n\n    return idx\n```\n",
    "frame_1959.jpg": "```python\ndef generate(self, idx, max_new_tokens):\n    # idx is (B, T) array of indices in the current context\n    for _ in range(max_new_tokens):\n        # get the predictions\n        logits, loss = self(idx)\n        # focus only on the last time step\n        logits = logits[:, -1, :]  # becomes (B, C)\n        # apply softmax to get probabilities\n        probs = F.softmax(logits, dim=-1)  # (B, C)\n        # sample from the distribution\n        idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n        # append sampled index to the running sequence\n        idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n    return idx\n\nm = BigramLanguageModel(vocab_size)\nlogits, loss = m(xb, yb)\nprint(logits.shape)\nprint(loss)\n\nidx = torch.zeros((1, 1), dtype=torch.long)\nprint(decode(m.generate(idx, max_new_tokens=100)[0].tolist()))\n```",
    "frame_1962.jpg": "```\ndef forward(self, idx, targets=None):\n    # idx and targets are both (B,T) tensor of integers\n    logits = self.token_embedding_table(idx)  # (B,T,C)\n\n    if targets is None:\n        loss = None\n    else:\n        B, T, C = logits.shape\n        logits = logits.view(B*T, C)\n        targets = targets.view(B*T)\n        loss = F.cross_entropy(logits, targets)\n\n    return logits, loss\n\ndef generate(self, idx, max_new_tokens):\n    # idx is (B, T) array of indices in the current context\n    for _ in range(max_new_tokens):\n        # get the predictions\n        logits, loss = self(idx)\n        # focus only on the last time step\n        logits = logits[:, -1, :]  # becomes (B, C)\n        # apply softmax to get probabilities\n        probs = F.softmax(logits, dim=-1)  # (B, C)\n        # sample from the distribution\n        idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n        # append sampled index to the running sequence\n        idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n    return idx\n```",
    "frame_1963.jpg": "```python\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\ntorch.manual_seed(1337)\n\nclass BigramLanguageModel(nn.Module):\n    \n    def __init__(self, vocab_size):\n        super().__init__()\n        # each token directly reads off the logits for the next token from a lookup table\n        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n```",
    "frame_1964.jpg": "```\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\ntorch.manual_seed(1337)\n\nclass DianaSourceModel(nn.Module):\n    ...\n```",
    "frame_1967.jpg": "```python\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\ntorch.manual_seed(1337)\n\nclass BigramLanguageModel(nn.Module):\n\n    def __init__(self, vocab_size):\n        super().__init__()\n        # each token directly reads off the logits for the next token from a lookup table\n        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n\n    def forward(self, idx, targets=None):\n        # idx and targets are both (B,T) tensor of integers\n        logits = self.token_embedding_table(idx) # (B,T,C)\n```",
    "frame_1968.jpg": "```python\nif targets is None:\n    loss = None\nelse:\n    B, T, C = logits.shape\n    logits = logits.view(B*T, C)\n    targets = targets.view(B*T)\n    loss = F.cross_entropy(logits, targets)\n\nreturn logits, loss\n\ndef generate(self, idx, max_new_tokens):\n    # idx is (B, T) array of indices in the current context\n    def __init__(self, vocab_size):\n        import torch.nn as nn\n        import torch.nn.functional as F\n\n        class Model(nn.Module):\n            def __init__(self):\n                super().__init__()\n                self.conv1 = nn.Conv2d(1, 20, 5)\n                self.conv2 = nn.Conv2d(20, 5)\n                \n            def forward(self, x):\n                x = F.relu(self.conv1(x))\n                return F.relu(self.conv2(x))\n\n    m = BigramLanguageModel(vocab_size)\n    logits, loss = m(xb, yb)\n    print(logits.shape)\n\n    idx = torch.zeros((1, 1), dtype=torch.long)\n    print(decode(m.generate(idx, max_new_tokens=100)[0].tolist()))\n```",
    "frame_1969.jpg": "```python\ndef generate(self, idx, max_new_tokens):\n    # idx is (B, T) array of indices in the current context\n    for _ in range(max_new_tokens):\n        # get the predictions\n        logits, loss = self(idx)\n        # focus only on the last time step\n        logits = logits[:, -1, :]  # becomes (B, C)\n        # apply softmax to get probabilities\n        probs = F.softmax(logits, dim=-1)  # (B, C)\n        # sample from the distribution\n        idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n        # append sampled index to the running sequence\n        idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n\n    return idx\n\nm = BigramLanguageModel(vocab_size)\nlogits, loss = m(xb, yb)\nprint(logits.shape)\nprint(loss)\n\nidx = torch.zeros((1, 1), dtype=torch.long)\nprint(decode(m.generate(idx, max_new_tokens=100)[0].tolist()))\n```",
    "frame_1970.jpg": "```python\nB, T, C = logits.shape\nlogits = logits.view(B*T, C)\ntargets = targets.view(B*T, C)\nlosses = F.cross_entropy(logits, targets)\n\nreturn logits, loss\n\ndef generate(self, idx, max_new_tokens):\n    # idx is (B, T) array of indices in the current context\n    for _ in range(max_new_tokens):\n        # get the predictions\n        logits, loss = self(idx)\n        # focus only on the last time step\n        logits = logits[:, -1, :]  # becomes (B, C)\n        # apply softmax to get probabilities\n        probs = F.softmax(logits, dim=-1)  # (B, C)\n        # sample from the distribution\n        idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n        # append sampled index to the running sequence\n        idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n    \n    return idx\n\nm = BigramLanguageModel(vocab_size)\nlogits, loss = m(xb, yb)\nprint(logits.shape)\nprint(loss)\n\nidx = torch.zeros((1, 1), dtype=torch.long)\nprint(decode(m.generate(idx, max_new_tokens=100)[0].tolist()))\n```",
    "frame_1973.jpg": "```python\ndef generate(self, idx, max_new_tokens):\n    # idx is (B, T) array of indices in the current context\n    for _ in range(max_new_tokens):\n        # get the predictions\n        logits, loss = self(idx)\n        # focus only on the last time step\n        logits = logits[:, -1, :]  # becomes (B, C)\n        # apply softmax to get probabilities\n        probs = F.softmax(logits, dim=-1)  # (B, C)\n        # sample from the distribution\n        idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n        # append sampled index to the running sequence\n        idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n\n    return idx\n\nm = BigramLanguageModel(vocab_size)\nlogits, loss = m(xb, yb)\nprint(logits.shape)\nprint(loss)\n\nidx = torch.zeros((1, 1), dtype=torch.long)\nprint(decode(m.generate(idx, max_new_tokens=100)[0].tolist()))\n```",
    "frame_1998.jpg": "```python\ndef generate(self, idx, max_new_tokens):\n    # idx is (B, T) array of indices in the current context\n    for _ in range(max_new_tokens):\n        # get the predictions\n        logits, loss = self(idx)\n        # focus only on the last time step\n        logits = logits[:, -1, :]  # becomes (B, C)\n        # apply softmax to get probabilities\n        probs = F.softmax(logits, dim=-1)  # (B, C)\n        # sample from the distribution\n        idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n        # append sampled index to the running sequence\n        idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n    return idx\n\nm = BigramLanguageModel(vocab_size)\nlogits, loss = m(xb, yb)\nprint(logits.shape)\nprint(loss)\n\nidx = torch.zeros((1, 1), dtype=torch.long)\nprint(decode(m.generate(idx, max_new_tokens=100)[0].tolist()))\n```\n",
    "frame_2001.jpg": "```python\ndef generate(self, idx, max_new_tokens):\n    # idx is (B, T) array of indices in the current context\n    for _ in range(max_new_tokens):\n        # get the predictions\n        logits, loss = self(idx)\n        # focus only on the last time step\n        logits = logits[:, -1, :]  # becomes (B, C)\n        # apply softmax to get probabilities\n        probs = F.softmax(logits, dim=-1)  # (B, C)\n        # sample from the distribution\n        idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n        # append sampled index to the running sequence\n        idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n    return idx\n\nm = BigramLanguageModel(vocab_size)\nlogits, loss = m(xf)\nprint(logits.shape)\nprint(loss)\n\nidx = torch.zeros((1, 1), dtype=torch.long)\nprint(decode(m.generate(idx, max_new_tokens=100)[0].tolist()))\n```",
    "frame_2005.jpg": "Here is the extracted developer code from the image:\n\n```python\nlogits = logits.view(B*T, C)\ntargets = targets.view(B*T)\nloss = F.cross_entropy(logits, targets)\n\ndef generate(self, idx, max_new_tokens):\n    # idx is (B, T) array of indices in the current context\n    for _ in range(max_new_tokens):\n        # get the predictions\n        logits, loss = self(idx)\n        # focus only on the last time step\n        logits = logits[:, -1, :]  # becomes (B, C)\n        # apply softmax to get probabilities\n        probs = F.softmax(logits, dim=-1)  # (B, C)\n        # sample from the distribution\n        idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n        # append sampled index to the running sequence\n        idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n    return idx\n\nm = BigramLanguageModel(vocab_size)\nlogits, loss = m(xb, y)\nprint(logits.shape)\nprint(loss)\n\nidx = torch.zeros((1, 1), dtype=torch.long)\nprint(decode(m.generate(idx, max_new_tokens=100)[0].tolist()))\n```\n\nIf you need further assistance, feel free to ask!",
    "frame_2020.jpg": "```python\ndef generate_indices(max_new_tokens):\n    idx = (B, T) array of indices in the current context\n    for _ in range(max_new_tokens):\n        # get the predictions\n        logits, loss = self(idx)\n        # focus only on the last time step\n        logits = logits[:, -1, :]  # becomes (B, C)\n        # apply softmax to get probabilities\n        probs = F.softmax(logits, dim=-1)  # (B, C)\n        # sample from the distribution\n        idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n        # append sampled index to the running sequence\n        idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n    return idx\n\nm = BigramLanguageModel(vocab_size)\nlogits, loss = m(xb, yb)\nprint(logits.shape)\nprint()\n\nprint(decode(m.generate(idx=torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))\n\ntorch.Size([32, 65])\ntenSr(4.8786, grad_fn=<NllLossBackward>)\n```\n",
    "frame_2025.jpg": "```python\ndef forward(self, idx, targets=None):\n    # idx and targets are both (B,T) tensor of integers\n    logits = self.token_embedding_table(idx)  # (B,T,C)\n    \n    if targets is None:\n        loss = None\n    else:\n        B, T, C = logits.shape\n        logits = logits.view(B*T, C)\n        targets = targets.view(B*T)\n        loss = F.cross_entropy(logits, targets)\n\n    return logits, loss\n\ndef generate(self, idx, max_new_tokens):\n    # idx is (B, T) array of indices in the current context\n    for _ in range(max_new_tokens):\n        # get the predictions\n        logits, loss = self(idx)\n        # focus only on the last time step\n        logits = logits[:, -1, :]  # becomes (B, C)\n        # apply softmax to get probabilities\n        probs = F.softmax(logits, dim=-1)  # (B, C)\n        # sample from the distribution\n        idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n        # append sampled index to the running sequence\n        idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n    return idx\n\nm = BiqarmLanuageModel(vocab_size)\n```",
    "frame_2026.jpg": "```python\nimport torch\nfrom torch.nn import functional as F\ntorch.manual_seed(1337)\n\nclass BigramLanguageModel(nn.Module):\n\n    def __init__(self, vocab_size):\n        super().__init__()\n        # each token directly reads off the logits for the next token from a lookup table\n        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n\n    def forward(self, idx, targets=None):\n        # idx and targets are both (B,T) tensor of integers\n        logits = self.token_embedding_table(idx)  # (B,T,C)\n\n        if targets is None:\n            loss = None\n        else:\n            B, T, C = logits.shape\n            logits = logits.view(B*T, C)\n            targets = targets.view(B*T)\n            loss = F.cross_entropy(logits, targets)\n\n        return logits, loss\n\n    def generate(self, idx, max_new_tokens):\n        # idx is (B,T) array of indices in the current context\n        for _ in range(max_new_tokens):\n            logits, loss = self(idx)\n            # focus only on the last time step\n            logits = logits[:, -1, :]  # becomes (B, C)\n            # apply softmax to get probabilities\n            probs = F.softmax(logits, dim=-1)  # (B, C)\n```",
    "frame_2027.jpg": "Here's the extracted developer code from the image:\n\n```python\nif targets is None:\n    loss = None\nelse:\n    B, T, C = logits.shape\n    logits = logits.view(B*T)\n    targets = targets.view(B*T)\n    loss = F.cross_entropy(logits, targets)\n\nreturn logits, loss\n\ndef generate(self, idx, max_new_tokens):\n    # idx is (B, T) array of indices in the current context\n    for _ in range(max_new_tokens):\n        # get the predictions\n        logits, loss = self(idx)\n        # focus only on the last time step\n        logits = logits[:, -1, :]  # becomes (B, C)\n        # apply softmax to get probabilities\n        probs = F.softmax(logits, dim=-1)  # (B, C)\n        # sample from the distribution\n        idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n        # append sampled index to the running sequence\n        idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n\n    return idx\n\nm = BigramLanguageModel(vocab_size)\nlogits, loss = m(xb, yb)\nprint(logits.shape)\nprint(loss)\n\nprint(decode(m.generate(idx=torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))\n```\n\nLet me know if you need anything else!",
    "frame_2028.jpg": "```python\ndef generate(self, idx, max_new_tokens):\n    # idx is (B, T) array of indices in the current context\n    for _ in range(max_new_tokens):\n        # get the predictions\n        logits, loss = self(idx)\n        # focus only on the last time step\n        logits = logits[:, -1, :] # becomes (B, C)\n        # apply softmax to get probabilities\n        probs = F.softmax(logits, dim=-1) # (B, C)\n        # sample from the distribution\n        idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n        # append sampled index to the running sequence\n        idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n    return idx\n\nm = BigramLanguageModel(vocab_size)\nlogits, loss = m(xb, yb)\nprint(logits.shape)\nprint(loss)\n\nprint(decode(m.generate(idx=torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))\n```\n\n",
    "frame_2031.jpg": "```python\nloss = None\nelse:\n    B, T, C = logits.shape\n    logits = logits.view(B*T, C)\n    targets = targets.view(B*T, )\n    loss = F.cross_entropy(logits, targets)\n\nreturn logits, loss\n\ndef generate(self, idx, max_new_tokens):\n    # idx is (B, T) array of indices in the current context\n    for _ in range(max_new_tokens):\n        # get the predictions\n        logits, loss = self(idx)\n        # focus only on the last time step\n        logits = logits[:, -1, :] # becomes (B, C)\n        # apply softmax to get probabilities\n        probs = F.softmax(logits, dim=-1) # (B, C)\n        # sample from the distribution\n        idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n        # append sampled index to the running sequence\n        idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n\nreturn idx\n\nm = BigramLanguageModel(vocab_size)\nlogits, loss = m(xb, yb)\nprint(logits.shape)\nprint(loss)\n\nprint(decode(m.generate(idx=torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))\n```",
    "frame_2034.jpg": "```python\ndef generate(self, idx, max_new_tokens):\n    # idx is (B, T) array of indices in the current context\n    for _ in range(max_new_tokens):\n        # get the predictions\n        logits, loss = self(idx)\n        # focus only on the last time step\n        logits = logits[:, -1, :]  # becomes (B, C)\n        # apply softmax to get probabilities\n        probs = F.softmax(logits, dim=-1)  # (B, C)\n        # sample from the distribution\n        idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n        idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n    return idx\n```",
    "frame_2035.jpg": "```python\nif targets is None:\n    loss = None\nelse:\n    B, T, C = logits.shape\n    logits = logits.view(B*T, C)\n    targets = targets.view(B*T)\n    loss = F.cross_entropy(logits, targets)\n\nreturn logits, loss\n\ndef generate(idx: Tensor, dim: Optional[int] = None, _stacklevel: int = 3, dtype: Optional[DTYPE] = None) -> Tensor:\n    # idx is (B, T)\n    for _ in range(num_samples):\n        # focus\n        logits, _ = self(idx)\n        logits = logits[:, -1, :]  # (B, C)\n        probs = F.softmax(logits, dim=-1)  # (B, C)\n        idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n        idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n\n    return idx\n\nm = BigramLanguageModel(vocab_size)\nlogits, loss = m(xb, yb)\nprint(logits.shape)\nprint(loss)\nprint(decode(m.generate(idx=torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))\n```",
    "frame_2041.jpg": "```python\nloss = F.cross_entropy(logits, targets)\n\ndef generate(self, idx, max_new_tokens):\n    # idx is (B, T) array of indices in the current context\n    for _ in range(max_new_tokens):\n        # get the predictions\n        logits, loss = self(idx)\n        # focus only on the last time step\n        logits = logits[:, -1, :]  # becomes (B, C)\n        # apply softmax to get probabilities\n        probs = F.softmax(logits, dim=-1)  # (B, C)\n        # sample from the distribution\n        idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n        # append sampled index to the running sequence\n        idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n    return idx\n\nm = BigramLanguageModel(vocab_size)\nlogits, loss = m(xb, yb)\nprint(logits.shape)\nprint(loss)\nprint(decode(m.generate(idx=torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))\n```",
    "frame_2047.jpg": "```python\n# idx and targets are both (B, T) tensor of integers\nlogits = self.token_embedding_table(idx) # (B,T,C)\n\nif targets is None:\n    loss = None\nelse:\n    B, T, C = logits.shape\n    logits = logits.view(B*T, C)\n    targets = targets.view(B*T)\n    loss = F.cross_entropy(logits, targets)\n\nreturn logits, loss\n\ndef generate(self, idx, max_new_tokens):\n    # idx is (B, T) array of indices in the current context\n    for _ in range(max_new_tokens):\n        # get the predictions\n        logits, loss = self(idx)\n        # focus only on the last time step\n        logits = logits[:, -1, :] # becomes (B, C)\n        # apply softmax to get probabilities\n        probs = F.softmax(logits, dim=-1) # (B, C)\n        # sample from the distribution\n        idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n        # append sampled index to the running sequence\n        idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n\n    return idx\n\nm = BigramLanguageModel(vocab_size)\nlogits, loss = m(xb, yb)\nprint(logits.shape)\nprint(loss)\n\nprint(decode(m.generate(idx=torch.zeros((1, 1), dtype=torch.long)), max_new_tokens=100)[0].tolist())\n```",
    "frame_2066.jpg": "```python\nloss = None\n\nB, T, C = logits.shape\nlogits = logits.view(B, T, C)\ntargets = targets.view(B*T)\nloss = F.cross_entropy(logits, targets)\n\nreturn logits, loss\n\ndef generate(self, idx, max_new_tokens):\n    # idx is (B, T) array of indices in the current context\n    for _ in range(max_new_tokens):\n        # get the predictions\n        logits, loss = self(idx)\n        # focus only on the last time step\n        logits = logits[:, -1, :]  # becomes (B, C)\n        # apply softmax to get probabilities\n        probs = F.softmax(logits, dim=-1)  # (B, C)\n        # sample from the distribution\n        idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n        # append sampled index to the running sequence\n        idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n    return idx\n\nm = BigramLanguageModel(vocab_size)\nlogits, loss = m(xb, yb)\nprint(logits.shape)\nprint(loss)\n\nprint(decode(m.generate(idx=torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))\n```",
    "frame_2069.jpg": "```python\nloss = None\n\nB, T, C = logits.shape\nlogits = logits.view(B*T, C)\ntargets = targets.view(B*T)\nloss = F.cross_entropy(logits, targets)\n\nreturn logits, loss\n\ndef generate(self, idx, max_new_tokens):\n    # idx is (B, T) array of indices in the current context\n    for _ in range(max_new_tokens):\n        # get the predictions\n        logits, loss = self(idx)\n        # focus only on the last time step\n        logits = logits[:, -1, :]  # becomes (B, C)\n        # apply softmax to get probabilities\n        probs = F.softmax(logits, dim=-1)  # (B, C)\n        # sample from the distribution\n        idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n        # append sampled index to the running sequence\n        idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n\n    return idx\n\nm = BigramLanguageModel(vocab_size)\nlogits, loss = m(xb, yb)\nprint(logits.shape)\nprint(loss)\n\nprint(decode(m.generate(idx=torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))\n```",
    "frame_2070.jpg": "```python\ndef generate(self, idx, max_new_tokens):\n    # idx is (B, T) array of indices in the current context\n    for _ in range(max_new_tokens):\n        # get the predictions\n        logits, loss = self(idx)\n        # logits only on the last time step\n        logits = logits[:, -1, :]  # becomes (B, C)\n        # apply softmax to get probabilities\n        probs = F.softmax(logits, dim=-1)  # (B, C)\n        # sample from the distribution\n        idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n        # append sampled index to the running sequence\n        idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n    return idx\n\nm = BigramLanguageModel(vocab_size)\nlogits, loss = m(xb, yb)\nprint(logits.shape)\nprint(loss)\n\nprint(decode(m.generate(idx=torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))\n```\n",
    "frame_2072.jpg": "```python\nloss = F.cross_entropy(logits, targets)\n\ndef generate(self, idx, max_new_tokens):\n    # idx is (B, T) array of indices in the current context\n    for i in range(max_new_tokens):\n        # get the predictions\n        logits, loss = self(idx)\n        # focus only on the last time step\n        logits = logits[:, -1, :]  # becomes (B, C)\n        # apply softmax to get probabilities\n        probs = F.softmax(logits, dim=-1)  # (B, C)\n        # sample from the distribution\n        idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n        # append sampled index to the running sequence\n        idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n    return idx\n\nm = BigramLanguageModel(vocab_size)\nlogits, loss = m(xb, yb)\nprint(loss.shape)\nprint(loss)\n\nprint(decode(m.generate(idx=torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))\n```\n",
    "frame_2073.jpg": "Here is the extracted code from the image:\n\n```python\ndef generate(self, idx, max_new_tokens):\n    # idx is (B, T) array of indices in the current context\n    for _ in range(max_new_tokens):\n        # get the predictions\n        logits, loss = self(idx)\n        # focus only on the last time step\n        logits = logits[:, -1, :]  # becomes (B, C)\n        # apply softmax to get probabilities\n        probs = F.softmax(logits, dim=-1)  # (B, C)\n        # sample from the distribution\n        idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n        # append sampled index to the running sequence\n        idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n    return idx\n\nm = BigramLanguageModel(vocab_size)\nlogits, loss = m(xb, yb)\nprint(logits.shape)\nprint(loss)\n\nprint(decode(m.generate(idx=torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))\n```\n\nPlease let me know if you need anything else!",
    "frame_2094.jpg": "```python\nlogits = logits[:, -1, :]  # becomes (B, C)\n# apply softmax to get probabilities\nprobs = F.softmax(logits, dim=-1)  # (B, C)\n# sample from the distribution\nidx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n# append sampled index to the running sequence\nidx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n\nreturn idx\n\nm = BigramLanguageModel(vocab_size)\nlogits, loss = m(xb, yb)\nprint(logits.shape)\nprint(loss)\n\nprint(decode(m.generate(idx=torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))\n\ntorch.Size([32, 65])\ntensor(4.5640, grad_fn=<NllLossBackward0>)\n```",
    "frame_2100.jpg": "```python\n# apply softmax to get probabilities\nprobs = F.softmax(logits, dim=-1)  # (B, C)\n# sample from the distribution\nidx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n# append sampled index to the running sequence\nidx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n\nreturn idx\n\nm = BigramLanguageModel(vocab_size)\nlogits, loss = m(xb, yb)\nprint(logits.shape)\nprint(loss)\n\nprint(decode(m.generate(idx=torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))\n\ntorch.Size([32, 65])\ntensor(4.5640, grad_fn=<NllLossBackward>)\n\n# create a PyTorch optimizer\noptimizer = torch.optim.Adam(m.parameters(), lr=1e-3)\n```",
    "frame_2101.jpg": "```\nm = BigramLanguageModel(vocab_size)\nlogits, loss = m(xb, yb)\nprint(logits.shape)\nprint(loss)\n\nprint(decode(m.generate(idx=torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))\n\noptimizer = torch.optim.Adam(m.parameters(), lr=1e-3)\n```",
    "frame_2115.jpg": "```python\n# create a PyTorch optimizer\noptimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)\n```",
    "frame_2130.jpg": "```python\nidx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\nidx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\nreturn idx\n\nm = BigramLanguageModel(vocab_size)\nlogits, loss = m(xb, yb)\nprint(logits.shape)\nprint(loss)\n\nprint(decode(m.generate(idx=torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))\n\ntorch.Size([32, 65])\ntensor(4.5640, grad_fn=<NllLossBackward0>)\n# create a PyTorch optimizer\noptimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)\n```",
    "frame_2132.jpg": "```python\nidx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\nreturn idx\n\nm = BigramLanguageModel(vocab_size)\nlogits, loss = m(xb, yb)\nprint(logits.shape)\nprint(loss)\n\nprint(decode(m.generate(idx=torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))\n\noptimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)\n```",
    "frame_2136.jpg": "Here is the extracted developer code from the image:\n\n```python\n# append sampled index to the running sequence\nidx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\nreturn idx\n\nm = BigramLanguageModel(vocab_size)\nlogits, loss = m(xb, yb)\nprint(logits.shape)\nprint(loss)\n\nprint(decode(m.generate(idx=torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))\n\noptimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)\n```\n\nIf you need anything else, let me know!",
    "frame_2137.jpg": "```\n# append sampled index to the running sequence\nidx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\nreturn idx\n\nm = BigramLanguageModel(vocab_size)\nlogits, loss = m(xb, yb)\nprint(logits.shape)\nprint(loss)\n\nprint(decode(m.generate(idx=torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))\n\noptimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)\n```",
    "frame_2138.jpg": "```python\nm = BigramLanguageModel(vocab_size)\nlogits, loss = m(xb, yb)\nprint(logits.shape)\nprint(loss)\n\nprint(decode(m.generate(idx=torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))\n\ntorch.Size([32, 65])\noptimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)\n\nbatch_size = 32\nfor steps in range(100):\n    xb, yb = get_batch('train')\n    logits, loss = m(xb, yb)\n    optimizer.zero_grad(set_to_none=True)\n    loss.backward()\n    optimizer.step()\n\nprint(loss.item())\n```",
    "frame_2159.jpg": "```python\n# create a PyTorch optimizer\noptimizer = torch.optim.Adam(m.parameters(), lr=1e-3)\n\nbatch_size = 32\nfor steps in range(100):\n    # sample a batch of data\n    xb, yb = get_batch('train')\n    \n    # evaluate the loss\n    logits, loss = m(xb, yb)\n    optimizer.zero_grad(set_to_none=True)\n    loss.backward()\n    optimizer.step()\n    \n    print(loss.item())\n```",
    "frame_2173.jpg": "Code NA",
    "frame_2179.jpg": "```\n# create a PyTorch optimizer\noptimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)\n\nbatch_size = 32\nfor steps in range(100):\n    # sample a batch of data\n    xb, yb = get_batch('train')\n\n    # evaluate the loss\n    logits, loss = m(xb, yb)\n    optimizer.zero_grad(set_to_none=True)\n    loss.backward()\n    optimizer.step()\n\n    print(loss.item())\n```",
    "frame_2187.jpg": "Code NA",
    "frame_2190.jpg": "```python\n# create a PyTorch optimizer\noptimizer = torch.optim.Adam(m.parameters(), lr=1e-3)\n\nbatch_size = 32\nfor steps in range(1000):\n    # sample a batch of data\n    xb, yb = get_batch('train')\n\n    # evaluate the loss\n    logits, loss = m(xb, yb)\n    optimizer.zero_grad(set_to_none=True)\n    loss.backward()\n    optimizer.step()\n\nprint(loss.item())\n```",
    "frame_2194.jpg": "```python\nprint(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))\n\ntorch.Size([32, 65])\ntensor(4.5640, grad_fn=<NllLossBackward0>)\n\n# create a PyTorch optimizer\noptimizer = torch.optim.Adam(m.parameters(), lr=1e-3)\n\nbatch_size = 32\nfor steps in range(1000):\n    # sample a batch of data\n    xb, yb = get_batch('train')\n    \n    # evaluate the loss\n    logits, loss = m(xb, yb)\n    optimizer.zero_grad(set_to_none=True)\n    loss.backward()\n    optimizer.step()\n\nprint(loss.item())\n```",
    "frame_2199.jpg": "```python\nprint(decode(m.generate(idx=torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))\n\n# create a PyTorch optimizer\noptimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)\n\nbatch_size = 32\nfor steps in range(1000):\n    # sample a batch of data\n    xb, yb = get_batch('train')\n    \n    # evaluate the loss\n    logits, loss = m(xb, yb)\n    optimizer.zero_grad(set_to_none=True)\n    loss.backward()\n    optimizer.step()\n\nprint(loss.item())\n```",
    "frame_2201.jpg": "```python\n# create a PyTorch optimizer\noptimizer = torch.optim.Adam(m.parameters(), lr=1e-3)\n\nbatch_size = 32\nfor steps in range(1000):\n\n    # sample a batch of data\n    xb, yb = get_batch('train')\n\n    # evaluate the loss\n    logits, loss = m(xb, yb)\n    optimizer.zero_grad(set_to_none=True)\n    loss.backward()\n    optimizer.step()\n\nprint(loss.item())\n```",
    "frame_2202.jpg": "Here is the extracted developer code from the image:\n\n```python\nprint(decode(m.generate(idx=torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))\n\n# create a PyTorch optimizer\noptimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)\n\nbatch_size = 32\nfor steps in range(1000):\n    # sample a batch of data\n    xb, yb = get_batch('train')\n\n    # evaluate the loss\n    logits, loss = m(xb, yb)\n    optimizer.zero_grad(set_to_none=True)\n    loss.backward()\n    optimizer.step()\n\nprint(loss.item())\n```",
    "frame_2205.jpg": "```python\nprint(decode(m.generate(idx=torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))\n\ntorch.Size([32, 65])\ntensor(4.5640, grad_fn=<NllLossBackward0>)\n\n# create a PyTorch optimizer\noptimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)\n\nbatch_size = 32\nfor steps in range(1000):\n    # sample a batch of data\n    xb, yb = get_batch('train')\n\n    # evaluate the loss\n    logits, loss = m(xb, yb)\n    optimizer.zero_grad(set_to_none=True)\n    loss.backward()\n    optimizer.step()\n\nprint(loss.item())\n```",
    "frame_2208.jpg": "```python\n# create a PyTorch optimizer\noptimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)\n\nbatch_size = 32\nfor steps in range(1000):\n    # sample a batch of data\n    xb, yb = get_batch('train')\n    \n    # evaluate the loss\n    logits, loss = m(xb, yb)\n    optimizer.zero_grad(set_to_none=True)\n    loss.backward()\n    optimizer.step()\n\nprint(loss.item())\n```",
    "frame_2210.jpg": "```python\nprint(decode(m.generate(idx=torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))\n\ntorch.Size([32, 65])\ntensor(4.5640, grad_fn=<NllLossBackward0>)\n\n# create a PyTorch optimizer\noptimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)\n\nbatch_size = 32\nfor steps in range(10000):\n    # sample a batch of data\n    xb, yb = get_batch('train')\n    # evaluate the loss\n    logits, loss = m(xb, yb)\n    optimizer.zero_grad(set_to_none=True)\n    loss.backward()\n    optimizer.step()\n\nprint(loss.item())\n```",
    "frame_2211.jpg": "```python\n# create a PyTorch optimizer\noptimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)\n\nbatch_size = 32\nfor steps in range(10000):\n    # sample a batch of data\n    xb, yb = get_batch('train')\n\n    # evaluate the loss\n    logits, loss = m(xb, yb)\n    optimizer.zero_grad(set_to_none=True)\n    loss.backward()\n    optimizer.step()\n\nprint(loss.item())\n```",
    "frame_2213.jpg": "Here is the extracted developer code from the image:\n\n```python\nm = BigramLanguageModel(vocab_size)\nlogits = m(xb, yb)\nprint(logits.shape)\nprint(loss)\n\nprint(decode(m.generate(idx=torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))\n\n# create a PyTorch optimizer\noptimizer = torch.optim.Adam(m.parameters(), lr=1e-3)\n\nbatch_size = 32\nfor steps in range(10000):\n    # sample a batch of data\n    xb, yb = get_batch('train')\n\n    # evaluate the loss\n    logits, loss = m(xb, yb)\n    optimizer.zero_grad(set_to_none=True)\n    loss.backward()\n    optimizer.step()\n\n    print(loss.item())\n```",
    "frame_2216.jpg": "```python\noptimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)\n\nbatch_size = 32\nfor steps in range(10000):\n    # sample a batch of data\n    xb, yb = get_batch('train')\n    \n    # evaluate the loss\n    logits, loss = m(xb, yb)\n    optimizer.zero_grad(set_to_none=True)\n    loss.backward()\n    optimizer.step()\n\nprint(loss.item())\n\nprint(decode(m.generate(idx=torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))\n```",
    "frame_2217.jpg": "Here is the extracted code from the image:\n\n```python\nbatch_size = 32\nfor steps in range(10000):\n    # sample a batch of data\n    xb, yb = get_batch('train')\n\n    # evaluate the loss\n    logits, loss = m(xb, yb)\n    optimizer.zero_grad(set_to_none=True)\n    loss.backward()\n    optimizer.step()\n\nprint(loss.item())\n\nprint(decode(m.generate(idx=torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))\n```",
    "frame_2228.jpg": "```python\n# create a PyTorch optimizer\noptimizer = torch.optim.Adam(m.parameters(), lr=1e-3)\n\nbatch_size = 32\nfor steps in range(10000):\n    # sample a batch of data\n    xb, yb = get_batch('train')\n    \n    # evaluate the loss\n    logits, loss = m(xb, yb) \n    optimizer.zero_grad(set_to_none=True)\n    loss.backward()\n    optimizer.step()\n    \n    print(loss.item())\n\nprint(decode(m.generate(idx=torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))\n```",
    "frame_2229.jpg": "The extracted developer code from the image is:\n\n```python\n# create a PyTorch optimizer\noptimizer = torch.optim.Adam(m.parameters(), lr=1e-3)\n\nbatch_size = 32\nfor steps in range(10000):\n    # sample a batch of data\n    xb, yb = get_batch('train')\n\n    # evaluate the loss\n    logits, loss = m(xb, yb)\n    optimizer.zero_grad(set_to_none=True)\n    loss.backward()\n    optimizer.step()\n\nprint(loss.item())\n\nprint(decode(m.generate(idx=torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))\n```",
    "frame_2231.jpg": "```python\n# create a PyTorch optimizer\noptimizer = torch.optim.Adam(m.parameters(), lr=1e-3)\n\nbatch_size = 32\nfor steps in range(10000):\n    # sample a batch of data\n    xb, yb = get_batch('train')\n    \n    # evaluate the loss\n    logits, loss = m(xb, yb)\n    optimizer.zero_grad(set_to_none=True)\n    loss.backward()\n    optimizer.step()\n    \n    print(loss.item())\n\nprint(decode(m.generate(idx=torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))\n```",
    "frame_2234.jpg": "```python\n# get the predictions\nlogits, loss = sel(idx)\n# focus only on the last time step\nlogits = logits[-1, :, :]  # becomes (B, C)\n# apply softmax to get probabilities\nprobs = F.softmax(logits, dim=-1)  # (B, C)\n# sample from the distribution\nidx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n# append sampled index to the running sequence\nidx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n\nreturn idx\n\nm = BigramLanguageModel(vocab_size)\nlogits, loss = m(xb, yb)\nprint(logits.shape)\nprint(loss)\n\nprint(decode(m.generate(idx=torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))\n\ntorch.Size([32, 65])\ntensor(4.5640, grad_fn=<NllLossBackward>)\n\n# create a PyTorch optimizer\noptimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)\n\nbatch_size = 32\nfor steps in range(10000):\n    # sample a batch of data\n    xh, yb = get_batch('train')\n```",
    "frame_2236.jpg": "```python\n# create a PyTorch optimizer\noptimizer = torch.optim.Adam(m.parameters(), lr=1e-3)\n\nbatch_size = 32\n\nfor steps in range(10000):\n    # sample a batch of data\n    xb, yb = get_batch('train')\n    \n    # evaluate the loss\n    logits, loss = m(xb, yb)\n    optimizer.zero_grad(set_to_none=True)\n    loss.backward()\n    optimizer.step()\n\nprint(loss.item())\n\nprint(decode(m.generate(idx=torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))\n```",
    "frame_2240.jpg": "```python\n# create a PyTorch optimizer\noptimizer = torch.optim.Adam(m.parameters(), lr=1e-3)\n\nbatch_size = 32\n\nfor steps in range(10000):\n    # sample a batch of data\n    xb, yb = get_batch('train')\n\n    # evaluate the loss\n    logits, loss = m(xb, yb)\n    optimizer.zero_grad(set_to_none=True)\n    loss.backward()\n    optimizer.step()\n\nprint(loss.item())\n\nprint(decode(m.generate(idx=torch.zeros((1, 1), dtype=torch.long), max_new_tokens=300)[0].tolist()))\n```",
    "frame_2248.jpg": "```python\nprint(loss.item())\nprint(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[0].tolist()))\n```",
    "frame_2251.jpg": "```python\nlogits, loss = m(xb, yb)\noptimizer.zero_grad(set_to_none=True)\nloss.backward()\noptimizer.step()\n\nprint(loss.item())\n\nprint(decode(m.generate(idx=torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[0].tolist()))\n```",
    "frame_2253.jpg": "```python\nprint(decode(m.generate(idx=torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[0].tolist()))\n```",
    "frame_2255.jpg": "Here is the extracted developer code from the image:\n\n```python\nprint(decode(m.generate(idx=torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[0].tolist()))\n```",
    "frame_2256.jpg": "```python\n# create a PyTorch optimizer\noptimizer = torch.optim.Adam(m.parameters(), lr=1e-3)\n\nbatch_size = 32\nfor steps in range(10000):\n    # sample a batch of data\n    xb, yb = get_batch('train')\n\n    # evaluate the loss\n    logits, loss = m(xb, yb)\n    optimizer.zero_grad(set_to_none=True)\n    loss.backward()\n    optimizer.step()\n\nprint(loss.item())\n```",
    "frame_2257.jpg": "```\ndef generate(self, idx, max_new_tokens):\n    # idx is (B, T) array of indices in the current context\n    for _ in range(max_new_tokens):\n        # get the predictions\n        logits, loss = self(idx)\n        # focus only on the last time step\n        logits = logits[:, -1, :]  # becomes (B, C)\n        # apply softmax to get probabilities\n        probs = F.softmax(logits, dim=-1)  # (B, C)\n        # sample from the distribution\n        idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n        # append sampled index to the running sequence\n        idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n    return idx\n\nm = BigramLanguageModel(vocab_size)\nlogits, loss = m(xb, yb)\nprint(logits.shape)\nprint(loss)\n\nprint(decode(m.generate(idx=torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))\n```",
    "frame_2258.jpg": "```python\nself.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n\ndef forward(self, idx, targets=None):\n    # idx and targets are both (B, T) tensor of integers\n    logits = self.token_embedding_table(idx)  # (B, T, C)\n\n    if targets is None:\n        loss = None\n    else:\n        B, T, C = logits.shape\n        targets = targets.view(B*T)\n        loss = F.cross_entropy(logits, targets)\n\n    return logits, loss\n\ndef generate(self, idx, max_new_tokens):\n    # idx is (B, T) array of indices in the current context\n    for _ in range(max_new_tokens):\n        # get the predictions\n        logits, loss = self(idx)\n        # focus only on the last time step\n        logits = logits[:, -1, :]  # becomes (B, C)\n        # apply softmax to get probabilities\n        probs = F.softmax(logits, dim=-1)  # (B, C)\n        # sample from the distribution\n        idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n        # append sampled indices to the running sequence\n        idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n\n    return idx\n\nm = BigramLanguageModel(vocab_size)\nlogits, loss = m(xb, yb)\nprint(logits.shape)\n```",
    "frame_2260.jpg": "```python\nlogits = logits[:, -1, :]  # becomes (B, C)\n# apply softmax to get probabilities\nprobs = F.softmax(logits, dim=-1)  # (B, C)\n# sample from the distribution\nidx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\nidx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\nreturn idx\n\nm = BigramLanguageModel(vocab_size)\nlogits, loss = m(xb, yb)\nprint(logits.shape)\nprint(loss)\n\nprint(decode(m.generate(idx=torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))\n\ntorch.Size([32, 65])\ntensor(4.5640, grad_fn=<NllLossBackward0>)\n# create a PyTorch optimizer\noptimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)\n```",
    "frame_2261.jpg": "Here is the extracted developer code from the image:\n\n```python\nlogits, loss = self(idx)\n# focus only on the last time step\nlogits = logits[:, -1, :]  # becomes (B, C)\n# apply softmax to get probabilities\nprobs = F.softmax(logits, dim=-1)  # (B, C)\n# sample from the distribution\nidx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n# append sampled index to the running sequence\nidx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\nreturn idx\n\nm = BigramLanguageModel(vocab_size)\nlogits, loss = m(xb, yb)\nprint(logits.shape)\nprint(loss)\n\nprint(decode(m.generate(idx=torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))\n\n# create a PyTorch optimizer\noptimizer = torch.optim.Adam(m.parameters(), lr=1e-3)\n\nbatch_size = 32\nfor steps in range(10000):\n    # sample a batch of data\n    xb, yb = get_batch('train')\n```\n\nIf more details or specific sections are needed, let me know!",
    "frame_2263.jpg": "```\nlogits, loss = m(xb, yb)\noptimizer.zero_grad(set_to_none=True)\nloss.backward()\noptimizer.step()\n\nprint(loss.item())\n\nprint(decode(m.generate(idx=torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[0].tolist()))\n```",
    "frame_2264.jpg": "```python\nprint(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[0].tolist()))\n```",
    "frame_2273.jpg": "```python\nprint(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[0].tolist()))\n```",
    "frame_2281.jpg": "```python\n# create a PyTorch optimizer\noptimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)\n\nbatch_size = 32\nfor steps in range(10000):\n    # sample a batch of data\n    xb, yb = get_batch('train')\n    \n    # evaluate the loss\n    logits, loss = m(xb, yb)\n    optimizer.zero_grad(set_to_none=True)\n    loss.backward()\n    optimizer.step()\n\nprint(loss.item())\n``` \n\n",
    "frame_2286.jpg": "```python\n# apply softmax to get probabilities\nprobs = F.softmax(logits, dim=-1)  # (B, C)\n# sample from the distribution\nidx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n# append sampled index to the running sequence\nreturn idx\n\nmodel = BigramLanguageModel(vocab_size)\noptimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n\nfor iter in range(max_iters):\n    # every once in a while evaluate the loss on train and val sets\n    if iter % eval_interval == 0:\n        losses = estimate_loss()\n        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n\n    # sample a batch of data\n    xb, yb = get_batch('train')\n    \n    # evaluate the loss\n    logits, loss = model(xb, yb)\n    optimizer.zero_grad(set_to_none=True)\n    loss.backward()\n    optimizer.step()\n\n# generate from the model\ncontext = torch.zeros((1, 1), dtype=torch.long, device=device)\nprint(decode(m.generate(context, max_new_tokens=500)[0].tolist()))\n```",
    "frame_2287.jpg": "```python\n@torch.no_grad()\ndef estimate_loss():\n    model.eval()\n    losses = {}\n    for split in ['train', 'val']:\n        losses[split] = torch.zeros(eval_iters)\n        for k in range(eval_iters):\n            X, Y = get_batch(split)\n            logits, loss = model(X, Y)\n            losses[split][k] = loss.item()\n    out[split] = losses.mean()\n    return out\n\n# super simple bigram model\nclass BigramLanguageModel(nn.Module):\n\n    def __init__(self, vocab_size):\n        super().__init__()\n        # each token directly reads off the logits for the next token from a lookup table\n        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n\n    def forward(self, idx, targets=None):\n        # idx and targets are both (B,T) tensor of integers\n        logits = self.token_embedding_table(idx)  # (B,T,C)\n        if targets is None:\n            loss = None\n        else:\n            B, T, C = logits.shape\n            logits = logits.view(B*T, C)\n            targets = targets.view(B*T)\n            loss = F.cross_entropy(logits, targets)\n\n        return logits, loss\n```",
    "frame_2288.jpg": "```python\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\n\n# hyperparameters\nbatch_size = 32 # how many independent sequences will we process in parallel?\nblock_size = 8 # what is the maximum context length for predictions?\nmax_iters = 3000\neval_interval = 300\nlearning_rate = 1e-2\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\neval_iters = 200\n\ntorch.manual_seed(1337)\n\n# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\nwith open('input.txt', 'r', encoding='utf-8') as f:\n    text = f.read()\nchars = sorted(list(set(text)))\nvocab_size = len(chars)\n\n# create a mapping from characters to integers\nstoi = {ch: i for i, ch in enumerate(chars)}\nitos = {i: ch for i, ch in enumerate(chars)}\nencode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\ndecode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n\n# Train and test splits\ndata = torch.tensor(encode(text), dtype=torch.long)\nn = int(0.9 * len(data)) # first 90% will be train, rest val\ntrain_data = data[:n]\nval_data = data[n:]\n\n# data loading\ndef get_batch(split):\n```",
    "frame_2293.jpg": "Here's the extracted code from the image:\n\n```python\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\n\n# hyperparameters\nbatch_size = 32  # how many independent sequences will we process in parallel?\nblock_size = 8   # what is the maximum context length for predictions?\nmax_iters = 3000\neval_interval = 300\nlearning_rate = 1e-2\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\neval_iters = 200\n\ntorch.manual_seed(1337)\n\n# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\nwith open('input.txt', 'r', encoding='utf-8') as f:\n    text = f.read()\n\n# create a mapping from characters to integers\nchars = sorted(list(set(text)))\nvocab_size = len(chars)\nstoi = {ch: i for i, ch in enumerate(chars)}  # encoder: take a string, output a list of integers\nitos = {i: ch for i, ch in enumerate(chars)}  # decoder: take a list of integers, output a string\n\n# Train and test splits\ndata = torch.tensor(encode(text), dtype=torch.long)\nn = int(0.9 * len(data))  # first 90% will be train, rest val\ntrain_data = data[:n]\nval_data = data[n:]\n\n# data loading\ndef get_batch(split):\n    ...\n```\nThis is a partial code, and it continues after the last line.",
    "frame_2301.jpg": "```python\nblock_size = 8 # what is the maximum context length for predictions?\nmax_iters = 300\neval_interval = 1e-2\nlearning_rate = 1e-2\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\neval_iters = 200\n\ntorch.manual_seed(1337)\n\n# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\nwith open('input.txt', 'r', encoding='utf-8') as f:\n    text = f.read()\n\n# here are all the unique characters that occur in this text\nchars = sorted(list(set(text)))\nvocab_size = len(chars)\n\n# create a mapping from characters to integers\nstoi = { ch:i for i,ch in enumerate(chars) }\nitos = { i:ch for i,ch in enumerate(chars) }\nencode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\ndecode = lambda l: ''.join(itos[i] for i in l) # decoder: take a list of integers, output a string\n\n# Train and test splits\ndata = torch.tensor(encode(text), dtype=torch.long)\nn = int(0.9*len(data)) # first 90% will be train, rest val\ntrain_data = data[:n]\nval_data = data[n:]\n\n# data loading\ndef get_batch(split):\n    # generate a small batch of data of inputs x and targets y\n    data = train_data if split == 'train' else val_data\n    ix = torch.randint(len(data), (batch_size,))\n    x = torch.stack([data[i:i+block_size] for i in ix])\n    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n```",
    "frame_2306.jpg": "Here is the extracted developer code from the image:\n\n```python\n# here are all the unique characters that occur in this text\nchars = sorted(list(set(text)))\nvocab_size = len(chars)\n\n# create a mapping from characters to integers\nstoi = { ch:i for i,ch in enumerate(chars) }\nitos = { i:ch for i,ch in enumerate(chars) }\nencode = lambda s: [stoi[c] for c in s]  # encoder: take a string, output a list of integers\ndecode = lambda l: ''.join(itos[i] for i in l)  # decoder: take a list of integers, output a string\n\n# Train and test splits\ndata = torch.tensor(encode(text), dtype=torch.long)\nn = int(0.9 * len(data))  # first 90% will be train, rest val_data\ntrain_data = data[:n]\nval_data = data[n:]\n\n# data loading\ndef get_batch(split):\n    generate a small batch of data of inputs x and targets y\n    data = train_data if split == 'train' else val_data\n    ...\n```\n\nIf you need further assistance, let me know!",
    "frame_2307.jpg": "```python\nwith open('input.txt', 'r', encoding='utf-8') as f:\n    text = f.read()\n\n# here are all the unique characters that occur in this text\nchars = sorted(list(set(text)))\nvocab_size = len(chars)\n\n# create a mapping from characters to integers\nstoi = {ch: i for i, ch in enumerate(chars)}\nitos = {i: ch for i, ch in enumerate(chars)}\nencode = lambda s: [stoi[c] for c in s]  # encoder: take a string, output a list of integers\ndecode = lambda l: ''.join(itos[i] for i in l)  # decoder: take a list of integers, output a string\n\n# Train and test splits\ndata = torch.tensor(encode(text), dtype=torch.long)\nn = int(0.9 * len(data))  # first 90% will be train, rest val\ntrain_data = data[:n]\nval_data = data[n:]\n\n# data loading\ndef get_batch(split):\n    # generate a small batch of data of inputs x and targets y\n    data = train_data if split == 'train' else val_data\n    ix = torch.randint(len(data) - block_size, (batch_size,))\n    x = torch.stack([data[i:i + block_size] for i in ix])\n    y = torch.stack([data[i + 1:i + block_size + 1] for i in ix])\n    x, y = x.to(device), y.to(device)\n    return x, y\n\n@torch.no_grad()\ndef estimate_loss():\n    out = {}\n    model.eval()\n    for split in ['train', 'val']:\n        losses = torch.zeros(eval_iters)\n        for k in range(eval_iters):\n            X, Y = get_batch(split)\n            logits = model(X, Y)\n            loss = loss_fn(logits, Y)\n            losses[k] = loss.item()\n        out[split] = losses.mean()\n    return out\n```",
    "frame_2308.jpg": "```python\ndata = torch.tensor(encode(text), dtype=torch.long)\nn = int(0.9*len(data))  # first 90% will be train, rest val\ntrain_data = data[:n]\nval_data = data[n:]\n\n@torch.no_grad()\ndef estimate_loss():\n    model.eval()\n    losses = torch.zeros(eval_iters)\n    for split in ['train', 'val']:\n        X, Y = get_batch(split)\n        losses += model(X, Y)\n```",
    "frame_2309.jpg": "```python\n@torch.no_grad()\ndef estimate_loss():\n    out = {}\n    model.eval()\n    for split in ['train', 'val']:\n        losses = torch.zeros(eval_iters)\n        for k in range(eval_iters):\n            X, Y = get_batch(split)\n            logits, loss = model(X, Y)\n            losses[k] = loss.item()\n        out[split] = losses.mean()\n    return out\n\n# super simple bigram model\nclass BigramLanguageModel(nn.Module):\n\n    def __init__(self, vocab_size):\n        super().__init__()\n        # each token directly reads off the logits for the next token from a lookup table\n        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n\n    def forward(self, idx, targets=None):\n        # idx and targets are both (B,T) tensor of integers\n```",
    "frame_2311.jpg": "```python\ndef get_batch(split):\n    # generate a small batch of data of inputs x and targets y\n    data = train_data if split == 'train' else val_data\n    ix = torch.randint(len(data), (batch_size,))\n    x = torch.stack([data[i:i+block_size] for i in ix])\n    y = x.to(device)\n    return x, y\n\n@torch.no_grad()\ndef estimate_loss():\n    out = {}\n    model.eval()\n    for split in ['train', 'val']:\n        losses = torch.zeros(eval_iters)\n        for k in range(eval_iters):\n            X, Y = get_batch(split)\n            logits, loss = model(X, Y)\n            losses[k] = loss.item()\n        out[split] = losses.mean()\n    model.train()\n    return out\n\n# super simple bigram model\nclass BigramLanguageModel(nn.Module):\n    def __init__(self, vocab_size):\n        super().__init__()\n        # each token directly reads off the logits for the next token from a lookup table\n        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n\n    def forward(self, idx, targets=None):\n        # idx and targets are both (B,T) tensor of integers\n        ...\n```",
    "frame_2314.jpg": "```python\n@torch.no_grad()\ndef estimate_loss():\n    out = {}\n    model.eval()\n    for split in ['train', 'val']:\n        losses = torch.zeros(eval_iters)\n        for k in range(eval_iters):\n            X, Y = get_batch(split)\n            logits = model(X, Y)\n            losses[k] = loss.item()\n        out[split] = losses.mean()\n    return out\n\n# super simple bigram model\nclass BigramLanguageModel(nn.Module):\n\n    def __init__(self, vocab_size):\n        super().__init__()\n        # each token directly reads off the logits for the next token from a lookup table\n        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n\n    def forward(self, idx, targets=None):\n        # idx and targets are both (B,T) tensor of integers\n```",
    "frame_2315.jpg": "```python\n# data loading\ndef get_batch(split):\n    # generate a small batch of data of inputs x and targets y\n    data = train_data if split == 'train' else val_data\n    ix = torch.randint(len(data), (block_size,))\n    x = torch.stack([data[i:i+block_size] for i in ix])\n    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n    return x, y\n\n@torch.no_grad()\ndef estimate_loss():\n    out = {}\n    model.eval()\n    for split in ['train', 'val']:\n        losses = torch.zeros(eval_iters)\n        for k in range(eval_iters):\n            X, Y = get_batch(split)\n            logits, loss = model(X, Y)\n            losses[k] = loss.item()\n        out[split] = losses.mean()\n    return out\n\n# super simple bigram model\nclass BigramLanguageModel(nn.Module):\n\n    def __init__(self, vocab_size):\n        super().__init__()\n        # each token directly reads off the logits for the next token from a lookup table\n        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n\n    def forward(self, idx, targets=None):\n        # idx and targets are both (B,T) tensor of integers\n        pass  # remaining code not visible\n```",
    "frame_2319.jpg": "```python\n@torch.no_grad()\ndef estimate_loss():\n    out = {}\n    for split in ['train', 'val']:\n        losses = torch.zeros(eval_iters)\n        for k in range(eval_iters):\n            X, Y = get_batch(split)\n            logits, loss = model(X, Y)\n            losses[k] = loss.item()\n        out[split] = losses.mean()\n    return out\n\n# super simple bigram model\nclass BigramLanguageModel(nn.Module):\n    def __init__(self, vocab_size):\n        super().__init__()\n        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n\n    def forward(self, idx, targets=None):\n        logits = self.token_embedding_table(idx) @ self.token_embedding_table.T\n        if targets is None:\n            loss = None\n        else:\n            B, T, C = logits.shape\n```\n",
    "frame_2322.jpg": "```python\n# super simple bigram model\nclass BigramLanguageModel(nn.Module):\n\n    def __init__(self, vocab_size):\n        super().__init__()\n        # each token directly reads off the logits for the next token from a lookup table\n        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n\n    def forward(self, idx, targets=None):\n        # idx and targets are both (B, T) tensor of integers\n        logits = self.token_embedding_table(idx)  # (B,T,C)\n        \n        if targets is None:\n            loss = None\n        else:\n            B, T, C = logits.shape\n            logits = logits.view(B*T, C)\n            targets = targets.view(B*T)\n            loss = F.cross_entropy(logits, targets)\n\n        return logits, loss\n\n    def generate(self, idx, max_new_tokens):\n        # idx is (B, T) array of indices in the current context\n        for _ in range(max_new_tokens):\n            # get the predictions\n            logits, loss = self(idx)\n            # focus only on the last time step\n            logits = logits[:, -1, :]  # becomes (B, C)\n            # apply softmax to get probabilities\n            probs = F.softmax(logits, dim=-1)  # (B, C)\n            # sample from the distribution\n            idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n            idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n```",
    "frame_2325.jpg": "```python\ndef __init__(self, vocab_size):\n    super().__init__()\n    # each token directly reads off the logits for the next token from a lookup table\n    self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n\ndef forward(self, idx, targets=None):\n    # idx and targets are both (B,T) tensor of integers\n    logits = self.token_embedding_table(idx)  # (B,T,C)\n\n    if targets is None:\n        loss = None\n    else:\n        B, T, C = logits.shape\n        targets = logits.view(B*T, C)\n        loss = F.cross_entropy(logits, targets)\n\n    return logits, loss\n\ndef generate(self, idx, max_new_tokens):\n    # idx is (B, T) array of indices in the current context\n    for _ in range(max_new_tokens):\n        # get the predictions\n        logits, loss = self(idx)\n        # Focus only on the last time step\n        logits = logits[:, -1, :]  # becomes (B, C)\n        # apply softmax to get probabilities\n        probs = F.softmax(logits, dim=1)  # (B, C)\n        # sample from the distribution\n        idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n        # append sampled index to the running sequence\n        idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n    return idx\n```",
    "frame_2326.jpg": "```python\nloss = F.cross_entropy(logits, targets)\n\ndef generate(self, idx, max_new_tokens):\n    idx = (B, T)  # array of indices in the current context\n    for _ in range(max_new_tokens):\n        # get the predictions\n        logits, loss = self(idx)\n        # focus only on the last time step\n        logits = logits[:, -1, :]  # becomes (B, C)\n        # apply softmax to get probabilities\n        probs = F.softmax(logits, dim=-1)  # (B, C)\n        # sample from the distribution\n        idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n        # append sampled index to the running sequence\n        idx = torch.cat((idx, idx_next), dim=1)\n    return idx\n\nmodel = BigramLanguageModel(vocab_size)\nm = model.to(device)\n\n# create a PyTorch optimizer\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n\nfor iter in range(max_iters):\n    # every once in a while evaluate the loss on train and val sets\n    if iter % eval_interval == 0:\n        losses = estimate_loss()\n        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n\n    # sample a batch of data\n    xb, yb = get_batch('train')\n\n    # evaluate the loss\n    logits, loss = model(xb, yb)\n```",
    "frame_2327.jpg": "```\n# apply softmax to get probabilities\nprobs = F.softmax(logits, dim=-1)  # (B, C)\n# sample from the distribution\nidx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n# append sampled index to the running sequence\nidx = torch.cat((idx, idx_next), dim=1)\nreturn idx\n\nmodel = BigramLanguageModel(vocab_size)\nmodel = model.to(device)\n\n# create a PyTorch optimizer\noptimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n\nfor iter in range(max_iters):\n    # every once in a while evaluate the loss on train and val sets\n    if iter % eval_interval == 0:\n        losses = estimate_loss()\n        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n\n    # sample a batch of data\n    xb, yb = get_batch('train')\n\n    # evaluate the loss\n    logits, loss = model(xb, yb)\n    optimizer.zero_grad(set_to_none=True)\n    loss.backward()\n    optimizer.step()\n\n# generate from the model\ncontext = torch.zeros((1, 1), dtype=torch.long, device=device)\nprint(decode(m.generate(context, max_new_tokens=500)[0].tolist()))\n```",
    "frame_2337.jpg": "```python\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\n\n# hyperparameters\nbatch_size = 32  # how many independent sequences will we process in parallel?\nblock_size = 8  # what is the maximum context length for predictions?\nmax_iters = 10000\neval_interval = 100\nlearning_rate = 1e-2\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\neval_iters = 200\n\ntorch.manual_seed(1337)\n\n# load data\nwith open('input.txt', 'r', encoding='utf-8') as f:\n    text = f.read()\n\n# here are all the unique characters that occur in this text\nchars = sorted(list(set(text)))\nvocab_size = len(chars)\n\n# create a mapping from characters to integers\nstoi = {ch: i for i, ch in enumerate(chars)}\nitos = {i: ch for i, ch in enumerate(chars)}\nencode = lambda s: [stoi[c] for c in s]  # encoder: take a string, output a list of integers\ndecode = lambda l: ''.join([itos[i] for i in l])  # decoder: take a list of integers, output a string\n\n# Train and test splits\ndata = torch.tensor(encode(text), dtype=torch.long)\nn = int(0.9 * len(data))  # first 90% will be train, rest val\ntrain_data = data[:n]\nval_data = data[n:]\n\n# data loading\ndef get_batch(split):\n    ...\n```",
    "frame_2349.jpg": "```python\neval_iters = 200\n\ntorch.manual_seed(1337)\n\n# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\nwith open('input.txt', 'r', encoding='utf-8') as f:\n    text = f.read()\n\n# here are all the unique characters that occur in this text\nchars = sorted(list(set(text)))\nvocab_size = len(chars)\n\n# create a mapping from characters to integers\nstoi = {c: i for i, c in enumerate(chars)}\nitos = {i: c for i, c in enumerate(chars)}\nencode = lambda s: [stoi[c] for c in s]  # encoder: take a string, output a list of integers\ndecode = lambda l: ''.join([itos[i] for i in l])  # decoder: take a list of integers, output a string\n\n# Train and test splits\ndata = torch.tensor(encode(text), dtype=torch.long)\n# first 90% will be train, rest val\ntrain_data = data[:n]\nval_data = data[n:]\n\n# data loading\ndef get_batch(split):\n    # generate a small batch of data of inputs x and targets y\n    data = train_data if split == 'train' else val_data\n    ix = torch.randint(len(data) - block_size, (batch_size,))\n    x = torch.stack([data[i:i+block_size] for i in ix])\n    y = torch.stack([data[i+1:i+1+block_size] for i in ix])\n    x, y = x.to(device), y.to(device)\n    return x, y\n\n@torch.no_grad()\ndef estimate_loss():\n    out = {}\n```",
    "frame_2350.jpg": "```python\n# create a mapping from characters to integers\nstoi = {c: i for i, c in enumerate(chars)}\nitos = {i: c for i, c in enumerate(chars)}\nencode = lambda s: [stoi[c] for c in s]  # encoder: take a string, output a list of integers\ndecode = lambda l: ''.join(itos[i] for i in l)  # decoder: take a list of integers, output a string\n\n# Train and test splits\ndata = torch.tensor(encode(text), dtype=torch.long)\nn = int(0.9 * len(data))  # first 90% will be train, rest val\ntrain_data = data[:n]\nval_data = data[n:]\n\n# data loading\ndef get_batch(split):\n    # generate a small batch of data of inputs x and targets y\n    data = train_data if split == 'train' else val_data\n    ix = torch.randint(len(data) - block_size, (batch_size,))\n    x = torch.stack([data[i:i + block_size] for i in ix])\n    y = x.to(device)\n    return x, y\n\n@torch.no_grad()\ndef estimate_loss():\n    out = 0\n    model.eval()\n    for split in ['train', 'val']:\n        losses = torch.zeros(eval_iters)\n        for k in range(eval_iters):\n            X, Y = get_batch(split)\n            logits, loss = model(X, Y)\n            losses[k] = loss.item()\n        out[split] = losses.mean()\n    model.train()\n    return out\n```\n",
    "frame_2354.jpg": "```python\nix = torch.randint(len(data) - block_size, (batch_size,))\ny = torch.stack([data[i:i+block_size] for i in ix])\nX, y = x.to(device), y.to(device)\n\n@torch.no_grad()\ndef estimate_loss():\n    out = {}\n    model.eval()\n    for split in ['train', 'val']:\n        losses = torch.zeros(eval_iters)\n        for k in range(eval_iters):\n            X, Y = get_batch(split)\n            logits, loss = model(X, Y)\n            losses[k] = loss.item()\n        out[split] = losses.mean()\n    return out\n\n# super simple bigram model\nclass BigramLanguageModel(nn.Module):\n    def __init__(self, vocab_size):\n        super().__init__()\n        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n\n    def forward(self, idx, targets=None):\n        logits = self.token_embedding_table(idx) # (B,T,C)\n        if targets is None:\n            loss = None\n        else:\n            B, T, C = logits.shape\n```\n",
    "frame_2355.jpg": "```python\ndef forward(self, idx, targets=None):\n    # idx and targets are both (B, T) tensor of integers\n    logits = self.token_embedding_table(idx)  # (B, T, C)\n\n    if targets is None:\n        loss = None\n    else:\n        B, T, C = logits.shape\n        logits = logits.view(B*T, C)\n        targets = targets.view(B*T)\n        loss = F.cross_entropy(logits, targets)\n\n    return logits, loss\n\ndef generate(self, idx, max_new_tokens):\n    # idx is (B, T) array of indices in the current context\n    for _ in range(max_new_tokens):\n        # get the predictions\n        logits, loss = self.self(idx)\n        # focus only on the last time step\n        logits = logits[:, -1, :]  # (B, C)\n        # apply softmax to get probabilities\n        probs = F.softmax(logits, dim=-1)  # (B, C)\n        # sample from the distribution\n        idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n        # append sampled index to the running sequence\n        idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n    return idx\n\nmodel = BigramLanguageModel(vocab_size)\nm = model.to(device)\n\n# create a PyTorch optimizer\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n\nfor iter in range(max_iters):\n```\n",
    "frame_2356.jpg": "```python\nmodel = BigramLanguageModel(vocab_size)\nm = model.to(device)\n\n# create a PyTorch optimizer\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n\nfor iter in range(max_iters):\n    # every once in a while evaluate the loss on train and val sets\n    if iter % eval_interval == 0:\n        losses = estimate_loss()\n        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n\n    # sample a batch of data\n    xb, yb = get_batch('train')\n\n    # evaluate the loss\n    logits, loss = model(xb, yb)\n    optimizer.zero_grad(set_to_none=True)\n    loss.backward()\n    optimizer.step()\n\n    # generate from the model\n```",
    "frame_2360.jpg": "```python\ndef forward(self, idx, targets=None):\n    # idx and targets are both (B,T) tensor of integers\n    logits = self.token_embedding_table(idx)  # (B,T,C)\n    if targets is None:\n        loss = None\n    else:\n        B, T, C = logits.shape\n        logits = logits.view(B, T, C)\n        targets = targets.view(B, T)\n        loss = F.cross_entropy(logits, targets)\n\n    return logits, loss\n\ndef generate(self, idx, max_new_tokens):\n    # idx is (B, T) array of indices in the current context\n    for _ in range(max_new_tokens):\n        # get the predictions\n        logits, loss = self(idx)\n        # focus only on the last time step\n        logits = logits[:, -1, :]  # (B, C)\n        # apply softmax to get probabilities\n        probs = F.softmax(logits, dim=-1)  # (B, C)\n        # sample from the distribution\n        idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n        # append sampled index to the running sequence\n        idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n\n    return idx\n\nmodel = BigramLanguageModel(vocab_size)\nm = model.to(device)\n\n# create a PyTorch optimizer\noptimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n\nfor iter in range(max_iters):\n```\n",
    "frame_2361.jpg": "Here is the extracted code from the image:\n\n```python\ndef __init__(self, vocab_size):\n    super().__init__()\n    # each token directly reads off the logits for the next token from a lookup table\n    self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n\ndef forward(self, idx, targets=None):\n    # idx and targets are both (B, T) tensor of integers\n    logits = self.token_embedding_table(idx)  # (B, T, C)\n\n    if targets is None:\n        loss = None\n    else:\n        B, T, C = logits.shape\n        logits = logits.view(B*T, C)\n        targets = targets.view(B*T)\n        loss = F.cross_entropy(logits, targets)\n\n    return logits, loss\n\ndef generate(self, idx, max_new_tokens):\n    # idx is (B, T) array of indices in the current context\n    for _ in range(max_new_tokens):\n        # get the predictions\n        logits, loss = self(idx)\n        # focus only on the last time step\n        logits = logits[:, -1, :]  # (B, C)\n        probs = F.softmax(logits, dim=1)  # (B, C)\n        # sample from the distribution\n        idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n        # append sampled index to the running sequence\n        idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n\n    return idx\n\nmodel = BigramLanguageModel(vocab_size)\n```",
    "frame_2369.jpg": "```python\ndef __init__(self, vocab_size):\n    super().__init__()\n    # each token directly reads off the logits for the next token from a lookup table\n    self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n\ndef forward(self, idx, targets=None):\n    # idx and targets are both (B,T) tensor of integers\n    logits = self.token_embedding_table(idx) # (B,T,C)\n    \n    if targets is None:\n        loss = None\n    else:\n        B, T, C = logits.shape\n        logits = logits.view(B*T, C)\n        targets = targets.view(B*T)\n        loss = F.cross_entropy(logits, targets)\n    \n    return logits, loss\n\ndef generate(self, idx, max_new_tokens):\n    # idx is (B,T) array of indices in the current context\n    for _ in range(max_new_tokens):\n        # get the predictions\n        logits, loss = self(idx)\n        # focus only on the last time step\n        logits = logits[:, -1, :] # becomes (B, C)\n        # apply softmax to get probabilities\n        probs = F.softmax(logits, dim=1) # (B, C)\n        # sample from the distribution\n        idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n        # append sampled index to the running sequence\n        idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n    \n    return idx\n\nmodel = BigramLanguageModel(vocab_size)\n```",
    "frame_2370.jpg": "```python\nif targets is None:\n    loss = None\nelse:\n    B, T, C = logits.shape\n    logits = logits.view(B*T, C)\n    targets = targets.view(B*T)\n    loss = F.cross_entropy(logits, targets)\n\nreturn logits, loss\n\ndef generate(self, idx, max_new_tokens):\n    # idx is (B, T) array of indices in the current context\n    for _ in range(max_new_tokens):\n        # get the predictions\n        logits, loss = self(idx)\n        # focus only on the last time step\n        logits = logits[:, -1, :]  # becomes (B, C)\n        # apply softmax to get probabilities\n        probs = F.softmax(logits, dim=-1)\n        # sample from the distribution\n        idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n        # append sampled index to the running sequence\n        idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n    return idx\n\nmodel = BigramLanguageModel(vocab_size)\nm = model.to(device)\n\n# create a PyTorch optimizer\noptimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n\nfor iter in range(max_iters):\n    # every once in a while evaluate the loss on train and val sets\n    if iter % eval_interval == 0:\n        losses = estimate_loss()\n        print(f\"iter {iter}: train loss {losses['train']: .4f}, val loss {losses['val']: .4f}\")\n```",
    "frame_2371.jpg": "```python\n# apply softmax to get probabilities\nprobs = F.softmax(logits, dim=-1)  # (B, C)\n# sample from the distribution\nidx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n# append sampled index to the running sequence\nidx = torch.cat((idx, idx_next), dim=1)\nreturn idx\n\nmodel = BigramLanguageModel(vocab_size)\nmodel = model.to(device)\n\n# create a PyTorch optimizer\noptimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n\nfor iter in range(max_iters):\n    # every once in a while evaluate the loss on train and val sets\n    if iter % eval_interval == 0:\n        losses = estimate_loss()\n        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n\n    # sample a batch of data\n    xb, yb = get_batch('train')\n\n    # evaluate the loss\n    logits, loss = model(xb, yb)\n    optimizer.zero_grad(set_to_none=True)\n    loss.backward()\n    optimizer.step()\n\n# generate from the model\ncontext = torch.zeros((1, 1), dtype=torch.long, device=device)\nprint(decode(m.generate(context, max_new_tokens=500)[0].tolist()))\n```",
    "frame_2372.jpg": "```python\nmodel = BigramLanguageModel(vocab_size)\nmodel = model.to(device)\n\noptimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n\nfor iter in range(max_iters):\n    if iter % eval_interval == 0:\n        losses = estimate_loss()\n        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n\n    xb, yb = get_batch('train')\n\n    logits, loss = model(xb, yb)\n    optimizer.zero_grad(set_to_none=True)\n    loss.backward()\n    optimizer.step()\n\ncontext = torch.zeros((1, 1), dtype=torch.long, device=device)\nprint(decode(m.generate(context, max_new_tokens=500)[0].tolist()))\n```",
    "frame_2374.jpg": "```python\n# apply softmax to get probabilities\nprobs = F.softmax(logits, dim=-1)  # (B, C)\n# sample from the distribution\nidx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n# append sampled index to the running sequence\nidx = torch.cat((idx, idx_next), dim=1)\nreturn idx\n\nmodel = BigramLanguageModel(vocab_size)\nmodel = model.to(device)\n\n# create a PyTorch optimizer\noptimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n\nfor iter in range(max_iters):\n    # every once in a while evaluate the loss on train and val sets\n    if iter % eval_interval == 0:\n        losses = estimate_loss()\n        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n\n    # sample a batch of data\n    xb, yb = get_batch('train')\n\n    # evaluate the loss\n    logits, loss = model(xb, yb)\n    optimizer.zero_grad(set_to_none=True)\n    loss.backward()\n    optimizer.step()\n\n    # generate from the model\n    context = torch.zeros((1, 1), dtype=torch.long, device=device)\n    print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))\n```",
    "frame_2382.jpg": "```python\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\n\n# hyperparameters\nbatch_size = 32  # # how many independent sequences will we process in parallel?\nblock_size = 8   # what is the maximum context length for predictions?\nmax_iters = 3000\neval_interval = 300\nlearning_rate = 1e-2\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\neval_iters = 200\n# \ntorch.manual_seed(1337)\n\n# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\nwith open('input.txt', 'r', encoding='utf-8') as f:\n    text = f.read()\n\n# here are all the unique characters that occur in this text\nchars = sorted(list(set(text)))\nvocab_size = len(chars)\n\n# create a mapping from characters to integers\nstoi = {ch: i for i, ch in enumerate(chars)}  # encoder: take a string, output a list of integers\nitos = {i: ch for i, ch in enumerate(chars)}  # decoder: take a list of integers, output a string\n\n# Train and test splits\ndata = torch.tensor([stoi[c] for c in text], dtype=torch.long)\nn = int(0.9 * len(data))  # first 90% will be train, rest val\ntrain_data = data[:n]\nval_data = data[n:]\n\n# data loading\ndef get_batch(split):\n```",
    "frame_2385.jpg": "```python\nlosses = torch.zeros(eval_liters)\nfor k in range(eval_iters):\n    X, Y = get_batch(split)\n    losses[k] = model(X, Y)\nlosses = losses.mean()\n\n# super simple bigram model\nclass BigramLanguageModel(nn.Module):\n\n    def __init__(self, vocab_size):\n        super().__init__()\n        # each token directly reads off the logits for the next token from a lookup table\n        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n\n    def forward(self, idx, targets=None):\n        # idx and targets are both (B,T) tensor of integers\n        logits = self.token_embedding_table(idx) # (B,T,C)\n\n        if targets is None:\n            loss = None\n        else:\n            B, T, C = logits.shape\n            logits = logits.view(B*T, C)\n            targets = targets.view(B*T)\n            loss = F.cross_entropy(logits, targets)\n\n        return logits, loss\n\n    def generate(self, idx, max_new_tokens):\n        # idx is (B, T) array of indices in the current context\n        for _ in range(max_new_tokens):\n            # get the predictions\n            logits, loss = self(idx)\n```",
    "frame_2386.jpg": "```python\nmodel = BigramLanguageModel(vocab_size)\nm = model.to(device)\n\n# create a PyTorch optimizer\noptimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n\nfor iter in range(max_iters):\n    # every once in a while evaluate the loss on train and val sets\n    if iter % eval_interval == 0:\n        losses = estimate_loss()\n        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n\n    # sample a batch of data\n    xb, yb = get_batch('train')\n\n    # evaluate the loss\n    logits, loss = model(xb, yb)\n    optimizer.zero_grad(set_to_none=True)\n    loss.backward()\n    optimizer.step()\n\n# generate from the model\ncontext = torch.zeros((1, 1), dtype=torch.long, device=device)\nprint(decode(m.generate(context, max_new_tokens=500)[0].tolist()))\n```",
    "frame_2387.jpg": "Here's the extracted code from the image:\n\n```python\n# create a PyTorch optimizer\noptimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n\nfor iter in range(max_iters):\n    # every once in a while evaluate the loss on train and val sets\n    if iter % eval_interval == 0:\n        losses = estimate_loss()\n        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n\n    # sample a batch of data\n    xb, yb = get_batch('train')\n\n    # evaluate the loss\n    logits, loss = model(xb, yb)\n    optimizer.zero_grad(set_to_none=True)\n    loss.backward()\n    optimizer.step()\n\n# generate from the model\ncontext = torch.zeros((1, 1), dtype=torch.long, device=device)\nprint(decode(m.generate(context, max_new_tokens=500)[0].tolist()))\n```",
    "frame_2389.jpg": "```python\n# create a PyTorch optimizer\noptimizer = torch.optim.Adam(m.parameters(), lr=1e-3)\n\nbatch_size = 32\nfor steps in range(10000):\n    # sample a batch of data\n    xb, yb = get_batch('train')\n\n    # evaluate the loss\n    logits, loss = m(xb, yb)\n    optimizer.zero_grad(set_to_none=True)\n    loss.backward()\n    optimizer.step()\n\nprint(loss.item())\n\nprint(decode(m.generate(idx=torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[0].tolist()))\n```",
    "frame_2398.jpg": "```python\n# create a PyTorch optimizer\noptimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)\n\nbatch_size = 32\nfor steps in range(10000):\n    # sample a batch of data\n    xb, yb = get_batch('train')\n    \n    # evaluate the loss\n    logits, loss = m(xb, yb)\n    optimizer.zero_grad(set_to_none=True)\n    loss.backward()\n    optimizer.step()\n\nprint(loss.item())\n\nprint(decode(m.generate(idx=torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[0].tolist()))\n```",
    "frame_2400.jpg": "```\n# apply softmax to get probabilities\nprobs = F.softmax(logits, dim=-1)  # (B, C)\n# sample from the distribution\nidx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n# append sampled index to the running sequence\nidx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\nreturn idx\n\nmodel = BigramLanguageModel(vocab_size)\nmodel = model.to(device)\n\n# create a PyTorch optimizer\noptimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n\nfor iter in range(max_iters):\n    # every once in a while evaluate the loss on train and val sets\n    if iter % eval_interval == 0:\n        losses = estimate_loss()\n        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n\n    # sample a batch of data\n    xb, yb = get_batch('train')\n\n    # evaluate the loss\n    logits, loss = model(xb, yb)\n    optimizer.zero_grad(set_to_none=True)\n    loss.backward()\n    optimizer.step()\n\n    # generate from the model\n    context = torch.zeros((1, 1), dtype=torch.long, device=device)\n    print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))\n```",
    "frame_2408.jpg": "```python\n# super simple bigram model\nclass BigramLanguageModel(nn.Module):\n\n    def __init__(self, vocab_size):\n        super().__init__()\n        # each token directly reads off the logits for the next token from a lookup table\n        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n\n    def forward(self, idx, targets=None):\n        # idx and targets are both (B,T) tensor of integers\n        logits = self.token_embedding_table(idx)  # (B,T,C)\n\n        if targets is None:\n            loss = None\n        else:\n            B, T, C = logits.shape\n            logits = logits.view(B*T, C)\n            targets = targets.view(B*T)\n            loss = F.cross_entropy(logits, targets)\n\n        return logits, loss\n\n    def generate(self, idx, max_new_tokens):\n        # idx is (B, T) array of indices in the current context\n        for _ in range(max_new_tokens):\n            # get the predictions\n            logits, loss = self(idx)\n            # focus only on the last time step\n            logits = logits[:, -1, :]  # becomes (B, C)\n            # apply softmax to get probabilities\n            probs = F.softmax(logits, dim=-1)  # (B, C)\n            # sample from the distribution\n            idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n            idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n```",
    "frame_2409.jpg": "```python\n@torch.no_grad()\ndef estimate_loss():\n    out = {}\n    model.eval()\n    for split in ['train', 'val']:\n        losses = torch.zeros(eval_iters)\n        for k in range(eval_iters):\n            X, Y = get_batch(split)\n            logits, loss = model(X, Y)\n            losses[k] = loss.item()\n        out[split] = losses.mean()\n    return out\n```",
    "frame_2414.jpg": "```python\n@torch.no_grad()\ndef estimate_loss():\n    out = {}\n    model.eval()\n    for split in ['train', 'val']:\n        losses = torch.zeros(eval_iters)\n        for k in range(eval_iters):\n            X, Y = get_batch(split)\n            logits, loss = model(X, Y)\n            losses[k] = loss.item()\n        out[split] = losses.mean()\n    return out\n\n# super simple bigram model\nclass BigramLanguageModel(nn.Module):\n\n    def __init__(self, vocab_size):\n        super().__init__()\n        # each token directly reads off the logits for the next token from a lookup table\n        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n\n    def forward(self, idx, targets=None):\n        # idx and targets are both (B,T) tensor of integers\n```",
    "frame_2427.jpg": "```python\n# create a PyTorch optimizer\noptimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n\nfor iter in range(max_iters):\n    # every once in a while evaluate the loss on train and val sets\n    if iter % eval_interval == 0:\n        losses = estimate_loss()\n        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n\n    # sample a batch of data\n    xb, yb = get_batch('train')\n\n    # evaluate the loss\n    logits, loss = model(xb, yb)\n    optimizer.zero_grad(set_to_none=True)\n    loss.backward()\n    optimizer.step()\n\n    # generate from the model\n    context = torch.zeros((1, 1), dtype=torch.long, device=device)\n    print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))\n```",
    "frame_2433.jpg": "```python\n# Train and test splits\ndata = torch.tensor(encode(text), dtype=torch.long)\nn = int(0.9 * len(data))  # first 90% will be train, rest val\ntrain_data = data[:n]\nval_data = data[n:]\n\n# data loading\ndef get_batch(split):\n    # generate a small batch of data of inputs x and targets y\n    data = train_data if split == 'train' else val_data\n    ix = torch.randint(len(data) - block_size, (batch_size,))\n    x = torch.stack([data[i:i + block_size] for i in ix])\n    y = torch.stack([data[i + 1:i + block_size + 1] for i in ix])\n    x, y = x.to(device), y.to(device)\n    return x, y\n\n@torch.no_grad()\ndef estimate_loss():\n    out = {}\n    model.eval()\n    for split in ['train', 'val']:\n        losses = torch.zeros(eval_iters)\n        for k in range(eval_iters):\n            x, y = get_batch(split)\n            logits, loss = model(x, y)\n            losses[k] = loss.item()\n        out[split] = losses.mean()\n    model.train()\n    return out\n\n# super simple bigram model\nclass BigramLanguageModel(nn.Module):\n    def __init__(self, vocab_size):\n        super().__init__()\n```\n",
    "frame_2445.jpg": "```python\n@torch.no_grad()\ndef estimate_loss():\n    out = {}\n    model.eval()\n    losses = torch.zeros(eval_iters)\n    for split in ['train', 'val']:\n        for k in range(eval_iters):\n            X, Y = get_batch(split)\n            logits, loss = model(X, Y)\n            losses[k] = loss.item()\n        out[split] = losses.mean()\n    return out\n\n# super simple bigram model\nclass BigramLanguageModel(nn.Module):\n\n    def __init__(self, vocab_size):\n        super().__init__()\n        # each token directly reads off the logits for the next token from a lookup table\n        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n\n    def forward(self, idx, targets=None):\n        # idx and targets are both (B,T) tensor of integers\n        logits = self.token_embedding_table(idx)  # (B,T,C)\n        if targets is None:\n            loss = None\n        else:\n            B, T, C = logits.shape\n            logits = logits.view(B * T, C)\n            loss = F.cross_entropy(logits, targets)\n\n        return logits, loss\n```",
    "frame_2446.jpg": "```python\nlosses = torch.zeros(eval_iters)\nfor k in range(eval_iters):\n    X, Y = get_batch(split)\n    logits, loss = model(X, Y)\n    losses[k] = loss.item()\nloss = losses.mean()\n\n# super simple bigram model\nclass BigramLanguageModel(nn.Module):\n\n    def __init__(self, vocab_size):\n        super().__init__()\n        # each token directly reads off the logits for the next token from a lookup table\n        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n\n    def forward(self, idx, targets=None):\n        logits = self.token_embedding_table(idx) # (B,T,C)\n\n        if targets is None:\n            loss = None\n        else:\n            B, T, C = logits.shape\n            logits = logits.view(B*T, C)\n            targets = targets.view(B*T)\n            loss = F.cross_entropy(logits, targets)\n\n        return logits, loss\n\n    def generate(self, idx, max_new_tokens):\n        # idx is (B,T) array of indices in the current context\n        for _ in range(max_new_tokens):\n            # get the predictions\n            logits, loss = self(idx)\n```",
    "frame_2460.jpg": "```python\n@torch.no_grad()\ndef estimate_loss():\n    out = {}\n    model.eval()\n    for split in ['train', 'val']:\n        losses = torch.zeros(eval_iters)\n        for k in range(eval_iters):\n            X, Y = get_batch(split)\n            logits = model(X)\n            loss = model.loss.item()\n            losses[k] = loss.item()\n        out[split] = losses.mean()\n    return out\n\n# super simple bigram model\nclass BigramLanguageModel(nn.Module):\n    def __init__(self, vocab_size):\n        super().__init__()\n        # each token directly reads off the logits for the next token from a lookup table\n        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n\n    def forward(self, idx, targets=None):\n        # idx and targets are both (B,T) tensor of integers\n        logits = self.token_embedding_table(idx)  # (B,T,C)\n```",
    "frame_2461.jpg": "```python\n# Train and test splits\ndata = torch.tensor(encode(text), dtype=torch.long)\ndata = int(0.9 * len(data))  # first 90% will be train, rest val\ntrain_data = data[:n]\nval_data = data[n:]\n\n# data loading\ndef get_batch(split):\n    # generate a small batch of data of inputs x and targets y\n    data = train_data if split == 'train' else val_data\n    ix = torch.randint(len(data), (block_size, batch_size,))\n    x = torch.stack([data[i:i+block_size] for i in ix])\n    y = x.clone()\n    y[:, :-1], y[:, -1] = x[:, 1:], x[:, :-1]\n    return x, y.to(device)\n\n@torch.no_grad()\ndef estimate_loss():\n    out = {}\n    model.eval()\n    for split in ['train', 'val']:\n        losses = torch.zeros(eval_iters)\n        for k in range(eval_iters):\n            x, y = get_batch(split)\n            logits, loss = model(x, y)\n            losses[k] = loss.item()\n        out[split] = losses.mean()\n    model.train()\n    return out\n\n# super simple bigram model\nclass BigramLanguageModel(nn.Module):\n    def __init__(self, vocab_size):\n        super().__init__()\n        # each token directly reads off the logits for the next token from a lookup table\n```\n",
    "frame_2468.jpg": "Here is the extracted developer code from the image:\n\n```python\n# Train and test splits\ndata = torch.tensor(encode(text), dtype=torch.long)\nn = int(0.9 * len(data))  # first 90% will be train, rest val\ntrain_data = data[:n]\nval_data = data[n:]\n\n# data loading\ndef get_batch(split):\n    # generate a small batch of data of inputs x and targets y\n    data = train_data if split == 'train' else val_data\n    ix = torch.randint(len(data), block_size, (batch_size,))\n    x = torch.stack([data[i:i+block_size] for i in ix])\n    y = x.to(device), y.to(device)\n    return x, y\n\n@torch.no_grad()\ndef estimate_loss():\n    out = {}\n    model.eval()\n    losses = torch.zeros(eval_iters)\n    \n    for split in ['train', 'val']:\n        for k in range(eval_iters):\n            x, y = get_batch(split)\n            logits, loss = model(x, y)\n            losses[k] = loss.item()\n        out[split] = losses.mean()\n    \n    model.train()\n    return out\n\n# super simple bigram model\nclass BigramLanguageModel(nn.Module):\n    def __init__(self, vocab_size):\n        super().__init__()\n        # each token directly reads off the logits for the next token from a lookup table\n```\n\nIf you are looking for something else, please specify!",
    "frame_2469.jpg": "```\n# Train and test splits\ndata = torch.tensor(encode(text), dtype=torch.long)\nn = int(len(data))  # first 90% will be train, rest val\ntrain_data = data[:n]\nval_data = data[n:]\n\n# data loading\ndef get_batch(splt):\n    # generate a small batch of data of inputs x and targets y\n    data = train_data if splt == 'train' else val_data\n    ix = torch.randint(len(data), (block_size, batch_size,))\n    x = torch.stack([data[i:i+block_size] for i in ix])\n    y = x.clone()\n    y = y.to(device)\n    return x, y\n\n@torch.no_grad()\ndef estimate_loss():\n    out = {}\n    model.eval()\n    for splt in ['train', 'val']:\n        losses = torch.zeros(eval_iters)\n        for k in range(eval_iters):\n            x, y = get_batch(splt)\n            logits = model(x)\n            loss = model(x, y)\n            losses[k] = loss.item()\n        out[splt] = losses.mean()\n    model.train()\n    return out\n\n# super simple bigram model\nclass BigramLanguageModel(nn.Module):\n    def __init__(self, vocab_size):\n        super().__init__()\n        # each token directly reads off the logits for the next token from a lookup table\n```",
    "frame_2477.jpg": "```python\n@torch.no_grad()\ndef estimate_loss():\n    out = *\n    model.eval()\n    for split in ['train', 'val']:\n        losses = torch.zeros(eval_iters)\n        for k in range(eval_iters):\n            X, Y = get_batch(split)\n            logits = model(X)\n            loss = loss.item()\n            losses[k] = losses.mean()\n    model.train()\n    return out\n\n# super simple bigram model\nclass BigramLanguageModel(nn.Module):\n    def __init__(self, vocab_size):\n        super().__init__()\n        # each token directly reads off the logits for the next token from a lookup table\n```",
    "frame_2479.jpg": "```python\n@torch.no_grad()\ndef estimate_loss():\n    out = 0\n    model.eval()\n    losses = torch.zeros(eval_iters)\n    for split in ['train', 'val']:\n        ix = torch.randint(len(data), block_size, (batch_size,))\n        x = torch.stack([data[i:i+block_size] for i in ix])\n        y, x = x.to(device), y.to(device)\n        return x\n    model.train()\n    return out\n```",
    "frame_2484.jpg": "```python\n# Train and test splits\ndata = torch.tensor(encode(text), dtype=torch.long)\nn = int(0.9 * len(data))  # first 90% will be train, rest val\ntrain_data = data[:n]\nval_data = data[n:]\n\n# data loading\ndef get_batch(split):\n    # generate a small batch of data of inputs x and targets y\n    data = train_data if split == 'train' else val_data\n    ix = torch.randint(len(data) - block_size, (batch_size,))\n    x = torch.stack([data[i:i + block_size] for i in ix])\n    y = torch.stack([data[i + 1:i + block_size + 1] for i in ix])\n    x, y = x.to(device), y.to(device)\n    return x, y\n\n@torch.no_grad()\ndef estimate_loss():\n    out = {}\n    model.eval()\n    for split in ['train', 'val']:\n        losses = torch.zeros(eval(batch_size))\n        for k in range(eval(num_batches)):\n            x, y = get_batch(split)\n            logits, loss = model(x, y)\n            losses[k] = loss.item()\n        out[split] = losses.mean()\n    model.train()\n    return out\n\n# super simple bigram model\nclass BigramLanguageModel(nn.Module):\n    def __init__(self, vocab_size):\n        super().__init__()\n        # each token directly reads off the logits for the next token from a lookup table\n```\n",
    "frame_2489.jpg": "```\ndata = torch.tensor(encode(text), dtype=torch.long)\nn = int(0.9 * len(data))  # first 90% will be train, rest val\ntrain_data = data[:n]\nval_data = data[n:]\n\n# data loading\ndef get_batch(split):\n    # generate a small batch of data of inputs x and targets y\n    data = train_data if split == 'train' else val_data\n    ix = torch.randint(len(data) - block_size, (batch_size,))\n    x = torch.stack([data[i:i+block_size] for i in ix])\n    y = x.to(device), y.to(device)\n    return x, y\n\n@torch.no_grad()\ndef estimate_loss():\n    out = {}\n    model.eval()\n    for split in ['train', 'val']:\n        losses = torch.tensor([])  # Initialize empty tensor\n        for k in range(iterations):  # Assuming iterations is defined somewhere\n            x, y = get_batch(split)\n            logits = model(x)\n            loss = loss_fn(logits, y)  # Assuming loss_fn is defined\n            losses = torch.cat((losses, loss.unsqueeze(0)))  # Concatenate losses\n        out[split] = losses.mean()\n    model.train()\n    return out\n\n# super simple bigram model\nclass BigramLanguageModel(nn.Module):\n    def __init__(self, vocab_size):\n        super().__init__()\n        # initialization code here...\n\n# each token directly reads off the logits for the next token from a lookup table\n```",
    "frame_2499.jpg": "```python\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\n\n# hyperparameters\nbatch_size = 32  # how many independent sequences will we process in parallel?\nblock_size = 8  # what is the maximum context length for predictions?\nmax_iters = 3000\neval_interval = 300\nlearning_rate = 1e-2\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\neval_iters = 200\n\ntorch.manual_seed(1337)\n\n# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\nwith open('input.txt', 'r', encoding='utf-8') as f:\n    text = f.read()\n\n# create a mapping from characters to integers\nchars = sorted(list(set(text)))\nvocab_size = len(chars)\nstoi = {ch: i for i, ch in enumerate(chars)}  # encoder: take a string, output a list of integers\nitos = {i: ch for i, ch in enumerate(chars)}  # decoder: take a list of integers, output a string\n\n# Train and test splits\ndata = torch.tensor(encode(text), dtype=torch.long)\nn = int(0.9 * len(data))  # first 90% will be train, rest val\ntrain_data = data[:n]\nval_data = data[n:]\n\n# data loading\ndef get_batch(split):\n    ...\n```\n",
    "frame_2500.jpg": "```python\ndef __init__(self, vocab_size):\n    super().__init__()\n    # each token directly reads off the logits for the next token from a lookup table\n    self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n\ndef forward(self, idx, targets=None):\n    # idx and targets are both (B,T) tensor of integers\n    if targets is None:\n        loss = None\n    else:\n        B, T, C = logits.shape\n        logits = logits.view(B*T, C)\n        targets = targets.view(B*T)\n        loss = F.cross_entropy(logits, targets)\n    return logits, loss\n\ndef generate(self, idx, max_new_tokens):\n    # idx is (B, T) array of indices in the current context\n    for _ in range(max_new_tokens):\n        # get the predictions\n        logits, loss = self(idx)\n        # focus only on the last time step\n        logits = logits[:, -1, :]  # becomes (B, C)\n        probs = F.softmax(logits, dim=-1)  # (B, C)\n        # sample from the distribution\n        idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n        # append sampled index to the running sequence\n        idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n    return idx\n```",
    "frame_2501.jpg": "```python\n# apply softmax to get probabilities\nprobs = F.softmax(logits, dim=-1)  # (B, C)\n# sample from the distribution\nidx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n# append sampled index to the running sequence\nidx = torch.cat((idx, idx_next), dim=1)\nreturn idx\n\nmodel = BigramLanguageModel(vocab_size)\nmodel = model.to(device)\n\n# create a PyTorch optimizer\noptimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n\nfor iter in range(max_iters):\n    # every once in a while evaluate the loss on train and val sets\n    if iter % eval_interval == 0:\n        losses = estimate_loss()\n        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n\n    # sample a batch of data\n    xb, yb = get_batch('train')\n\n    # evaluate the loss\n    logits, loss = model(xb, yb)\n    optimizer.zero_grad(set_to_none=True)\n    loss.backward()\n    optimizer.step()\n\n# generate from the model\ncontext = torch.zeros((1, 1), dtype=torch.long, device=device)\nprint(decode(m.generate(context, max_new_tokens=500)[0].tolist()))\n```",
    "frame_2502.jpg": "```python\n# create a PyTorch optimizer\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n\nfor iter in range(max_iters):\n    # every once in a while evaluate the loss on train and val sets\n    if iter % eval_interval == 0:\n        losses = estimate_loss()\n        print(f\"{iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n\n    # sample a batch of data\n    xb, yb = get_batch('train')\n\n    # evaluate the loss\n    logits, loss = model(xb, yb)\n    optimizer.zero_grad(set_to_none=True)\n    loss.backward()\n    optimizer.step()\n\n    # generate from the model\n    context = torch.zeros((1, 1), dtype=torch.long, device=device)\n    print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))\n```",
    "frame_2513.jpg": "Here's the extracted code from the image:\n\n```python\n# create a PyTorch optimizer\noptimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n\nfor iter in range(max_iters):\n    # every once in a while evaluate the loss on train and val sets\n    if iter % eval_interval == 0:\n        losses = estimate_loss()\n        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n\n    # sample a batch of data\n    xb, yb = get_batch('train')\n\n    # evaluate the loss\n    log_probs = model(xb, yb)\n    optimizer.zero_grad(set_to_none=True)\n    loss.backward()\n    optimizer.step()\n\n    # generate from the model\n    context = torch.zeros((1, 1), dtype=torch.long, device=device)\n```\n\nIf there were no code present, I would have returned 'Code NA'.",
    "frame_2514.jpg": "```python\n# create a PyTorch optimizer\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n\nfor iter in range(max_iters):\n    # every once in a while evaluate the loss on train and val sets\n    if iter % eval_interval == 0:\n        losses = estimate_loss()\n        print(f\"step {iter}: train loss {losses['train']:1.4f}, val loss {losses['val']:1.4f}\")\n\n    # sample a batch of data\n    xb, yb = get_batch('train')\n\n    # evaluate the loss\n    logits, loss = model(xb, yb)\n    optimizer.zero_grad(set_to_none=True)\n    loss.backward()\n    optimizer.step()\n\n# generate from the model\ncontext = torch.zeros((1, 1), dtype=torch.long, device=device)\n```",
    "frame_2530.jpg": "```python\n# create a PyTorch optimizer\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n\nfor iter in range(max_iters):\n    # every once in a while evaluate the loss on train and val sets\n    if iter % eval_interval == 0:\n        losses = estimate_loss()\n        print(f\"{iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n\n    # sample a batch of data\n    xb, yb = get_batch('train')\n\n    # evaluate the loss\n    logits, loss = model(xb, yb)\n    optimizer.zero_grad(set_to_none=True)\n    loss.backward()\n    optimizer.step()\n\n# generate from the model\ncontext = torch.zeros((1, 1), dtype=torch.long, device=device)\nprint(decode(m.generate(context, max_new_tokens=500)[0].tolist()))\n```",
    "frame_2532.jpg": "```python\nmodel = BigramLanguageModel(vocab_size)\nm = model.to(device)\n\n# create a PyTorch optimizer\noptimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n\nfor iter in range(max_iters):\n    # every once in a while evaluate the loss on train and val sets\n    if iter % eval_interval == 0:\n        losses = estimate_loss()\n        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n\n    # sample a batch of data\n    xb, yb = get_batch('train')\n\n    # evaluate the loss\n    logits, loss = model(xb, yb)\n    optimizer.zero_grad(set_to_none=True)\n    loss.backward()\n    optimizer.step()\n\n# generate from the model\ncontext = torch.zeros((1, 1), dtype=torch.long, device=device)\nprint(decode(m.generate(context, max_new_tokens=500)[0].tolist()))\n```",
    "frame_2534.jpg": "```python\n# consider the following toy example:\ntorch.manual_seed(1337)\nB,T,C = 4,8,2  # batch, time, channels\nx = torch.randn(B,T,C)\nx.shape\ntorch.Size([4, 8, 2])\n```",
    "frame_2563.jpg": "```python\n# consider the following toy example:\ntorch.manual_seed(1337)\nB,T,C = 4,8,2 # batch, time, channels\nx = torch.randn(B,T,C)\nx.shape\ntorch.Size([4, 8, 2])\n```",
    "frame_2564.jpg": "```python\n# super simple bigram model\nclass BigramLanguageModel(nn.Module):\n\n    def __init__(self, vocab_size):\n        super().__init__()\n        # each token directly reads off the logits for the next token from a lookup table\n        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n\n    def forward(self, idx, targets=None):\n        # idx and targets are both (B,T) tensor of integers\n        logits = self.token_embedding_table(idx)  # (B,T,C)\n\n        if targets is None:\n            loss = None\n        else:\n            B, T, C = logits.shape\n            logits = logits.view(B*T, C)\n            targets = targets.view(B*T)\n            loss = F.cross_entropy(logits, targets)\n\n        return logits, loss\n\n    def generate(self, idx, max_new_tokens):\n        # idx is (B,T) array of indices in the current context\n        for _ in range(max_new_tokens):\n            # get the predictions\n            pass\n```\n",
    "frame_2565.jpg": "```python\n# consider the following toy example:\ntorch.manual_seed(1337)\nB,T,C = 4,8,2 # batch, time, channels\nx = torch.randn(B,T,C)\nx.shape\ntorch.Size([4, 8, 2])\n```",
    "frame_2577.jpg": "```python\n# consider the following toy example:\ntorch.manual_seed(1337)\nB,T,C = 4,8,2  # batch, time, channels\nx = torch.randn(B,T,C)\nx.shape\ntorch.Size([4, 8, 2])\n```",
    "frame_2589.jpg": "```python\n# consider the following toy example:\nB,T,C = 4, 8, 2  # batch, time, channels\nx = torch.randn(B,T,C)\nx.shape\ntorch.Size([4, 8, 2])\n```",
    "frame_2595.jpg": "```python\n# consider the following toy example:\nB,T,C = 4,8,2  # batch, time, channels\nx = torch.randn(B,T,C)\nx.shape\n\ntorch.Size([4, 8, 2])\n```",
    "frame_2599.jpg": "```python\n# consider the following toy example:\nB,T,C = 4,8,2  # batch, time, channels\nx = torch.randn(B,T,C)\nx.shape\ntorch.Size([4, 8, 2])\n```",
    "frame_2602.jpg": "```python\ntorch.manual_seed(1337)\nB,T,C = 4,8,2  # batch, time, channels\nx = torch.randn(B,T,C)\nx.shape\ntorch.Size([4, 8, 2])\n```",
    "frame_2606.jpg": "```python\n# consider the following toy example:\ntorch.manual_seed(1337)\nB,T,C = 4,8,2  # batch, time, channels\nx = torch.randn(B,T,C)\nx.shape\ntorch.Size([4, 8, 2])\n```",
    "frame_2629.jpg": "```python\n# consider the following toy example:\ntorch.manual_seed(1337)\nB, T, C = 4, 8, 2  # batch, time, channels\nx = torch.randn(B, T, C)\nx.shape\ntorch.Size([4, 8, 2])\n```",
    "frame_2634.jpg": "```python\n# consider the following toy example:\nB,T,C = 4,8,2  # batch, time, channels\nx = torch.randn(B,T,C)\nx.shape\ntorch.Size([4, 8, 2])\n```",
    "frame_2654.jpg": "```\n# consider the following toy example:\nB,T,C = 4,8,2  # batch, time, channels\nx = torch.randn(B,T,C)\nx.shape\ntorch.Size([4, 8, 2])\n```",
    "frame_2661.jpg": "```python\n# consider the following toy example:\ntorch.manual_seed(1337)\nB,T,C = 4,8,2  # batch, time, channels\nx = torch.randn(B,T,C)\nx.shape\n```",
    "frame_2665.jpg": "```python\ntorch.manual_seed(1337)\nB,T,C = 4,8,2  # batch, time, channels\nx = torch.randn(B,T,C)\nx.shape\n```",
    "frame_2667.jpg": "```python\n# consider the following toy example:\nB,T,C = 4,8,2  # batch, time, channels\nx = torch.randn(B,T,C)\nx.shape\n```",
    "frame_2673.jpg": "```python\n# consider the following toy example:\ntorch.manual_seed(1337)\nB,T,C = 4,8,2  # batch, time, channels\nx = torch.randn(B,T,C)\nx.shape\ntorch.Size([4, 8, 2])\n```",
    "frame_2687.jpg": "```python\n# consider the following toy example:\ntorch.manual_seed(1337)\nB,T,C = 4,8,2  # batch, time, channels\nx = torch.randn(B,T,C)\nx.shape\n```",
    "frame_2689.jpg": "```python\n# consider the following toy example:\ntorch.manual_seed(1337)\nB,T,C = 4,8,2  # batch, time, channels\nx = torch.randn(B,T,C)\nx.shape\ntorch.Size([4, 8, 2])\n```",
    "frame_2700.jpg": "```python\n# consider the following toy example:\ntorch.manual_seed(1337)\nB,T,C = 4,8,2  # batch, time, channels\nx = torch.randn(B,T,C)\nx.shape\n\ntorch.Size([4, 8, 2])\n```",
    "frame_2707.jpg": "```python\ntorch.manual_seed(1337)\nB,T,C = 4,8,2  # batch, time, channels\nx = torch.randn(B,T,C)\nx.shape\n\n# We want x[b,t] = mean_{i<=t} x[b,i]\nxbow = torch.zeros((B,T,C))\nfor b in range(B):\n    for t in range(T):\n        xprev = x[b,:t+1]  # (t,C)\n        xbow[b,t] = torch.mean(xprev, 0)\n```",
    "frame_2719.jpg": "```python\n# We want x[b,t] = mean_{i<=t} x[b,i]\nxbow = torch.zeros((B,T,C))\nfor b in range(B):\n    for t in range(T):\n        xprev = x[b,:t+1]  # (t,C)\n        xbow[t] = torch.mean(xprev, 0)\n```",
    "frame_2727.jpg": "```python\n# consider the following toy example:\ntorch.manual_seed(1337)\nB,T,C = 4,8,2  # batch, time, channels\nx = torch.randn(B,T,C)\nx.shape\n\ntorch.Size([4, 8, 2])\n\n# We want x[b,t] = mean_{i<=t} x[b,i]\nxbow = torch.zeros((B,T,C))\nfor b in range(B):\n    for t in range(T):\n        xprev = x[b,:t+1]  # (t,C)\n        xbow[t] = torch.mean(xprev, 0)\n```",
    "frame_2770.jpg": "```python\ntorch.manual_seed(1337)\nB,T,C = 4,8,2  # batch, time, channels\nx = torch.randn(B,T,C)\nx.shape\n\ntorch.Size([4, 8, 2]) \n\n# We want x[b,t] = mean_{i<=t} x[b,i]\nxbow = torch.zeros(B, T)  # Tensor: x\nfor b in range(B):\n    for t in range(T):\n        xprev = x[b, :t+1]  # (t,C)\n        xbow[b,t] = torch.mean(xprev, 0)\n```",
    "frame_2772.jpg": "```python\ntorch.manual_seed(1337)\nB,T,C = 4,8,2  # batch, time, channels\nx = torch.randn(B,T,C)\nx.shape\n\n# We want x[b,t] = mean_{i=t}^{t} x[b,i]\nxbow = torch.zeros((B,T,C))\nfor b in range(B):\n    for t in range(T):\n        xprev = x[b,:t+1]  # (t,C)\n        xbow[b,t] = torch.mean(xprev, 0)\n```",
    "frame_2789.jpg": "```python\n# consider the following toy example:\ntorch.manual_seed(1337)\nB, T, C = 4, 8, 2  # batch, time, channels\nx = torch.randn(B, T, C)\nx.shape\n\ntorch.Size([4, 8, 2])\n\n# We want x[b, t] = mean_{i<=t} x[b, i]\nxbow = torch.zeros((B, T, C))\nfor b in range(B):\n    for t in range(T):\n        xprev = x[b, :t + 1]  # (t, C)\n        xbow[b, t] = torch.mean(xprev, 0)\n```",
    "frame_2792.jpg": "Code NA",
    "frame_2794.jpg": "```python\n# We want x[b,t] = mean_{i<=t} x[b,i]\nxbow = torch.zeros((B,T,C))\nfor b in range(B):\n    for t in range(T):\n        xprev = x[b,t+1] # (t,C)\n        xbow[b,t] = torch.mean(xprev, 0)\n```",
    "frame_2806.jpg": "```python\n# We want x[b,t] = mean_{i<=t} x[b,i]\nxbow = torch.zeros((B,T,C))\nfor b in range(B):\n    for t in range(T):\n        xprev = x[b,:t+1]  # (t,C)\n        xbow[b,t] = torch.mean(xprev, 0)\n```",
    "frame_2808.jpg": "Here is the extracted developer code from the image:\n\n```python\n# We want x[b,t] = mean_{i<=t} x[b,i]\nxbow = torch.zeros((B,T,C))\nfor b in range(B):\n    for t in range(T):\n        xprev = x[b,t+1] # (t,C)\n        xbow[b,t] = torch.mean(xprev, 0)\n```\n\nIf you need anything else, feel free to ask!",
    "frame_2811.jpg": "```python\ntorch.Size([4, 8, 2])\n# We want x[b,t] = mean_{i<=t} x[b,i]\nxbow = torch.zeros((B,T,C))\nfor b in range(B):\n    for t in range(T):\n        xprev = x[b,t+1] # (t,C)\n        xbow[b,t] = torch.mean(xprev, 0)\n``` \n\nIf you need any further assistance, feel free to ask!",
    "frame_2817.jpg": "```python\n# We want x[b,t] = mean_{i<=t} x[b,i]\nxbow = torch.zeros((B,T,C))\nfor b in range(B):\n    for t in range(T):\n        xprev = x[b,:t+1]  # (t,C)\n        xbow[b,t] = torch.mean(xprev, 0)\n```",
    "frame_2825.jpg": "```python\n# We want x[b,t] = mean_{i <= t} x[b,i]\nxbow = torch.zeros((B, T, C))\nfor b in range(B):\n    for t in range(T):\n        xprev = x[b, :t + 1]  # (t, C)\n        xbow[b, t] = torch.mean(xprev, 0)\n```",
    "frame_2826.jpg": "Here is the extracted developer code from the image:\n\n```python\nxbow = torch.zeros((B,T,C))\nfor b in range(B):\n    for t in range(T):\n        xprev = x[b,t+1]  # (t,C)\n        xbow[b,t] = torch.mean(xprev, 0)\n```\n\nNote: The context of variables (like `B`, `T`, `C`, etc.) is not provided in the image.",
    "frame_2827.jpg": "```python\n# We want x[b,t] = mean_{i<=t} x[b,i]\nxbow = torch.zeros((B,T,C))\nfor b in range(B):\n    for t in range(T):\n        xprev = x[b,t+1]  # (t,C)\n        xbow[b,t] = torch.mean(xprev, 0)\n```\n",
    "frame_2833.jpg": "```python\n# consider the following toy example:\ntorch.manual_seed(1337)\nB,T,C = 4,8,2  # batch, time, channels\nx = torch.randn(B,T,C)\nx.shape\n\n# We want x[b,t] = mean_{i<=t} x[b,i]\nxbow = torch.zeros((B,T,C))\nfor b in range(B):\n    for t in range(T):\n        xprev = x[b,:t+1]  # (t,C)\n        xbow[b,t] = torch.mean(xprev, 0)\n```",
    "frame_2838.jpg": "```python\ntorch.manual_seed(1337)\nB,T,C = 4,8,2 # batch, time, channels\nx = torch.randn(B,T,C)\nx.shape\ntorch.Size([4, 8, 2])\n\n# We want x[b,t] = mean_{i<=t} x[b,i]\nxbow = torch.zeros((B,T,C))\nfor b in range(B):\n    for t in range(T):\n        xprev = x[b,:t+1] # (t,C)\n        xbow[b,t] = torch.mean(xprev, 0)\n```",
    "frame_2840.jpg": "```python\n# We want x[b,t] = mean_{i<=t} x[b,i]\nxbow = torch.zeros((B,T,C))\nfor b in range(B):\n    for t in range(T):\n        xprev = x[b,:t+1]  # (t,C)\n        xbow[b,t] = torch.mean(xprev, 0)\n```",
    "frame_2844.jpg": "```python\ntorch.manual_seed(42)\na = torch.ones(3, 3)\nb = torch.randint(0, 10, (3, 2)).float()\nc = a @ b\nprint('a=')\nprint(a)\nprint('---')\nprint('b=')\nprint(b)\nprint('---')\nprint('c=')\nprint(c)\n```",
    "frame_2923.jpg": "```python\ntorch.manual_seed(42)\na = torch.ones(3, 3)\nb = torch.randint(0, 10, (3, 2)).float()\nc = a @ b\nprint('a=')\nprint(a)\nprint('=')\nprint('b=')\nprint(b)\nprint('=')\nprint('c=')\nprint(c)\n```",
    "frame_2933.jpg": "```python\ntorch.manual_seed(42)\na = torch.ones(3, 3)\nb = torch.randint(0, 10, (3, 2)).float()\nc = a @ b\nprint('a')\nprint(a)\nprint('---')\nprint('b')\nprint(b)\nprint('---')\nprint('c')\nprint(c)\n```",
    "frame_2937.jpg": "```python\ntorch.tril(torch.ones(3, 3))\n\ntorch.manual_seed(42)\na = torch.ones(3, 3)\nb = torch.randint(0, 10, (3, 2)).float()\nc = a @ b\nprint('a=')\nprint(a)\nprint('---')\nprint('b=')\nprint(b)\nprint('---')\nprint('c=')\nprint(c)\n```",
    "frame_2943.jpg": "```python\nfor t in range(T):\n    xprev = x[b, t+1]  # (t, C)\n    xbow[b, t] = torch.mean(xprev, 0)\n\ntorch.tril(torch.ones(3, 3))\n\ntorch.manual_seed(42)\na = torch.ones(3, 3)\nb = torch.randint(0, 10, (3, 2)).float()\nc = a @ b\n\nprint('a=')\nprint(a)\nprint('---')\nprint('b=')\nprint(b)\nprint('---')\nprint('c=')\nprint(c)\n```",
    "frame_2956.jpg": "```python\ntorch.manual_seed(42)\na = torch.tril(torch.ones(3, 3))\nb = torch.randint(0, 10, (3, 2)).float()\nc = a @ b\nprint('a=')\nprint(a)\nprint('---')\nprint('b=')\nprint(b)\nprint('---')\nprint('c=')\nprint(c)\n```",
    "frame_2959.jpg": "Here is the extracted code from the image:\n\n```python\ntorch.manual_seed(42)\na = torch.tril(torch.ones(3, 3))\nb = torch.randint(0, 10, (3, 2)).float()\nc = a @ b\nprint('a=') \nprint(a) \nprint('b=') \nprint(b) \nprint('c=') \nprint(c)\n```",
    "frame_3041.jpg": "```python\ntorch.manual_seed(42)\na = torch.tril(torch.ones(3, 3))\nb = torch.randint(0, 10, (3, 2)).float()\nc = a @ b\nprint('a=')\nprint(a)\nprint('---')\nprint('b=')\nprint(b)\nprint('---')\nprint('c=')\nprint(c)\n```",
    "frame_3051.jpg": "```\ntorch.manual_seed(42)\na = torch.tril(torch.tensor([[1., 0., 0.],\n                              [1., 1., 0.],\n                              [1., 1., 1.]]))\na = a / torch.sum(a)\nb = torch.randint(0, 10, (3, 2)).float()\nc = a @ b\nprint('a=')\nprint(a)\nprint('---')\nprint('b=')\nprint(b)\nprint('---')\nprint('c=')\nprint(c)\n```",
    "frame_3056.jpg": "```python\ntorch.manual_seed(42)\na = torch.tril(torch.ones(3, 3))\na = a / torch.sum(a, keepdim=True)\nb = torch.randint(0, 10, (3, 2)).float()\nc = a @ b\nprint('a=')\nprint(a)\nprint('--')\nprint('b=')\nprint(b)\nprint('--')\nprint('c=')\nprint(c)\n```",
    "frame_3060.jpg": "```python\ntorch.manual_seed(42)\na = torch.tril(torch.ones(3, 3))\na = a / torch.sum(a, 1, keepdim=True)\nb = torch.randint(0, 10, (3, 2)).float()\nc = a @ b\nprint('a=')\nprint(a)\nprint('---')\nprint('b=')\nprint(b)\nprint('---')\nprint('c=')\nprint(c)\n```",
    "frame_3098.jpg": "```python\ntorch.manual_seed(42)\na = torch.tril(torch.ones(3, 3))\na = a / torch.sum(a, 1, keepdim=True)\nb = torch.randint(0, 10, (3, 2)).float()\nc = a @ b\n\nprint('a=')\nprint(a)\nprint('---')\nprint('b=')\nprint(b)\nprint('---')\nprint('c=')\nprint(c)\n```",
    "frame_3109.jpg": "```python\ntorch.manual_seed(42)\na = torch.tril(torch.ones(3, 3))\na = a / torch.sum(a, 1, keepdim=True)\nb = torch.randint(0, 10, (3, 2)).float()\nc = a @ b\nprint('a=')\nprint(a)\nprint('b=')\nprint(b)\nprint('c=')\nprint(c)\n```",
    "frame_3117.jpg": "```python\n# We want x[b,t] = mean_{i<=t} x[b,i]\nxbow = torch.zeros((B,T,C))\nfor b in range(B):\n    for t in range(T): # (t,C)\n        xprev = x[b,:t+1] \n        xbow[t] = torch.mean(xprev, 0)\n\ntorch.tril(torch.ones(3, 3))\n\ntorch.manual_seed(42)\na = torch.tril(torch.ones(3, 3))\na = a / torch.sum(a, 1, keepdim=True)\nb = torch.randint(0, 10, (3, 2)).float()\nc = a @ b\n\nprint('a=')\nprint(a)\nprint('---')\nprint('b=')\nprint(b)\nprint('---')\nprint('c=')\nprint(c)\n```",
    "frame_3121.jpg": "```python\ntorch.manual_seed(1337)\nB,T,C = 4,8,2 # batch, time, channels\nx = torch.randn(B,T,C)\nx.shape\n\n# We want x[b,t] = mean_{i<=t} x[b,i]\nxbow = torch.zeros((B,T,C))\nfor b in range(B):\n    for t in range(T):\n        xprev = x[b,:t+1] # (t,C)\n        xbow[b,t] = torch.mean(xprev, 0)\n\ntorch.tril(torch.ones(3, 3))\ntensor([[1., 0., 0.],\n        [1., 1., 0.],\n        [1., 1., 1.]])\na = torch.tril(torch.ones(3, 3))\na = a / torch.sum(a, keepdim=True)\n```\n",
    "frame_3123.jpg": "```python\ntorch.manual_seed(1337)\nB,T,C = 4,8,2  # batch, time, channels\nx = torch.randn(B,T,C)\nx.shape\n\n# We want x[b,t] = mean_{i <= t} x[b,i]\nxbow = torch.zeros((B,T,C))\nfor b in range(B):\n    for t in range(T):\n        xprev = x[b,:t+1]  # (t,C)\n        xbow[b,t] = torch.mean(xprev, 0)\n\ntorch.tril(torch.ones(3, 3))\ntensor([[1., 0., 0.],\n        [1., 1., 0.],\n        [1., 1., 1.]])\n        \ntorch.manual_seed(42)\na = torch.tril(torch.ones(3, 3))\na = a / torch.sum(a, 1, keepdim=True)\nb = torch.randint(0,10,(3,2)).float()\n```",
    "frame_3127.jpg": "```python\ntorch.manual_seed(1337)\nB,T,C = 4,8,2  # batch, time, channels\nx = torch.randn(B,T,C)\nx.shape\n\ntorch.Size([4, 8, 2])\n\n# We want x[b,t] = mean_{i<=t} x[b,i]\nxbow = torch.zeros((B,T,C))\nfor b in range(B):\n    for t in range(T):\n        xprev = x[b,:t+1]  # (t,C)\n        xbow[t,b] = torch.mean(xprev, 0)\n\nwei = torch.tril(torch.ones(T, T))\nwei = wei / wei.sum(1, keepdim=True)\n\ntorch.tril(torch.ones(3, 3))\n\ntensor([[1., 0., 0.],\n        [1., 1., 0.],\n        [1., 1., 1.]])\n        \ntorch.manual_seed(42)\na = torch.tril(torch.ones(3, 3))\na = a / torch.sum(a, 1, keepdim=True)\na = torch.randn((10,3,2)).float()\n```",
    "frame_3132.jpg": "```python\n# consider the following toy example:\ntorch.manual_seed(1337)\nB,T,C = 4,2,2  # batch, time, channels\nx = torch.randn(B,T,C)\nx.shape\n\ntorch.Size([4, 2, 2])\n\n# We want x[b,t] = mean_{i<=t} x[b,i]\nxbow = torch.zeros((B,T,C))\nfor b in range(B):\n    for t in range(T):\n        xprev = x[b,:t+1]  # (t,C)\n        xbow[b,t] = torch.mean(xprev, 0)\n\nwei = torch.tril(torch.ones(T, T))\nwei = wei / wei.sum(1, keepdim=True)\nwei\n\ntorch.tril(torch.ones(3, 3))\n```",
    "frame_3133.jpg": "Here's the extracted developer code from the image:\n\n```python\n# We want x[b,t] = mean_{i <= t} x[b,i]\nxbow = torch.zeros((B,T,C))\nfor b in range(B):\n    for t in range(T):\n        xprev = x[b,:t+1]  # (t,C)\n        xbow[b,t] = torch.mean(xprev, 0)\n\nwei = torch.tril(torch.ones(T, T))\nwei = wei / wei.sum(keepdim=True)\nwei\n\ntensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0001],\n        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])\n\ntorch.tril(torch.ones(3, 3))\n\ntensor([[1., 0., 0.],\n        [1., 1., 0.],\n        [1., 1., 1.]])\n\ntorch.manual_seed(42)\na = torch.tril(torch.ones(3, 3))\na = a / torch.sum(a, 1, keepdim=True)\nb = torch.randint(0, 10, (3, 2)).float()\nc = a @ b\n```\n\nLet me know if you need further assistance!",
    "frame_3137.jpg": "```python\n# We want x[b,t] = mean_{i<=t} x[b,i]\nxbow = torch.zeros((B,T,C))\nfor b in range(B):\n    for t in range(T):\n        xprev = x[b,i:t+1]  # (t,C)\n        xbow[b,t] = torch.mean(xprev, 0)\n\nwei = torch.tril(torch.ones(T, T))\nwei = wei / wei.sum(1, keepdim=True)\nwei\n\ntorch.tril(torch.ones(3, 3))\n\ntorch.manual_seed(42)\na = torch.tril(torch.ones(3, 3))\na = a / torch.sum(a, 1, keepdim=True)\nb = torch.randint(0, 10, (3, 2)).float()\n```",
    "frame_3142.jpg": "Here is the extracted code from the image:\n\n```python\n# We want x[b,t] = mean_{i<=t} x[b,i]\nxbox = torch.zeros((B, T, C))\nfor b in range(B):\n    for t in range(T):\n        xprev = x[b,:t+1]  # (t,C)\n        xbox[b,t] = torch.mean(xprev, 0)\n\nwei = torch.tril(torch.ones(T, T))\nwei = wei / wei.sum(1, keepdim=True)\nwei\n\ntorch.tril(torch.ones(3, 3))\ntensor([[1.0000, 0.0000, 0.0000],\n        [0.5000, 0.5000, 0.0000],\n        [0.3333, 0.3333, 0.3333]])\n        \ntorch.manual_seed(42)\na = a / torch.sum(a, 1, keepdim=True)\nb = torch.randint(0, 10, (3, 2)).float()\n```",
    "frame_3143.jpg": "```python\ntorch.manual_seed(42)\na = torch.tril(torch.ones(3, 3))\na = a / torch.sum(a, 1, keepdim=True)\nb = torch.randint(0, 10, (3, 2)).float()\nc = a @ b\nprint('a=')\nprint(a)\nprint('---')\nprint('b=')\nprint(b)\nprint('---')\nprint('c=')\nprint(c)\n```",
    "frame_3146.jpg": "```python\ntorch.manual_seed(1337)\nB, T, C = 4, 8, 2  # batch, time, channels\nx = torch.randn(B, T, C)\nx.shape\ntorch.Size([4, 8, 2])\n\n# We want x[b, t] = mean_{i<=t} x[b, i]\nxbow = torch.zeros((B, T, C))\nfor b in range(B):\n    for t in range(T):\n        xprev = x[b, :t + 1]  # (t, C)\n        xbow[b, t] = torch.mean(xprev, 0)\n\nwei = torch.tril(torch.ones(T, T))\nwei = wei / wei.sum(1, keepdim=True)\nwei\n\ntorch.tril(torch.ones(3, 3))\n```",
    "frame_3153.jpg": "```python\ntorch.manual_seed(1337)\nB, T, C = 4, 8, 2  # batch, time, channels\nx = torch.randn(B, T, C)\nx.shape\n\ntorch.Size([4, 8, 2])\n\n# We want x[b,t] = mean_{i<=t} x[b,i]\nxbow = torch.zeros((B, T, C))\nfor b in range(B):\n    for t in range(T):\n        xprev = x[b, :t + 1]  # (t, C)\n        xbow[b, t] = torch.mean(xprev, 0)\n\nwei = torch.tril(torch.ones(T, T))\nwei = wei / wei.sum(1, keepdim=True)\nxbow2 = \n```",
    "frame_3186.jpg": "```python\n# consider the following toy example:\ntorch.manual_seed(1337)\nB,T,C = 4,8,2  # batch, time, channels\nx = torch.randn(B,T,C)\nx.shape\n\ntorch.Size([4, 8, 2])\n\n# We want x[b,t] = mean_{i<=t} x[b,i]\nxbow = torch.zeros((B,T,C))\nfor b in range(B):\n    for t in range(T):\n        xprev = x[b,:t+1]  # (t,C)\n        xbow[b,t] = torch.mean(xprev, 0)\n\nwei = torch.tril(torch.ones(T, T))\nwei = wei / wei.sum(1, keepdim=True)\nxbow2 = wei @ x # (B, T) @ (B, T, C) --->\n\ntensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0001],\n        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])\n        \ntorch.tril(torch.ones(3, 3))\n```",
    "frame_3190.jpg": "```python\n# consider the following toy example:\ntorch.manual_seed(1337)\nB, T, C = 4, 8, 2\nx = torch.randn(B, T, C)\nx.shape\n\ntorch.Size([4, 8, 2])\n\n# We want x[b,t] = mean_{i<=t} x[b,i]\nxbow = torch.zeros((B,T,C))\nfor b in range(B):\n    for t in range(T):\n        xprev = x[b,:t+1] # (t,C)\n        xbow[b,t] = torch.mean(xprev, 0)\n\nwei = torch.tril(torch.ones(T, T))\nwei = wei / wei.sum(1, keepdim=True)\nxbow2 = wei @ x # (B, T) @ (B, T, C)\n\ntensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])\n```\n",
    "frame_3195.jpg": "```python\ntorch.manual_seed(42)\na = torch.tril(torch.ones(3, 3))\na = a / torch.sum(a, 1, keepdim=True)\nb = torch.randint(0, 10, (3, 2)).float()\nc = a @ b\nprint('a')\nprint(a)\nprint('---')\nprint('b')\nprint(b)\nprint('---')\nprint('c')\nprint(c)\n```",
    "frame_3198.jpg": "```python\n# consider the following toy example:\ntorch.manual_seed(1337)\nB, T, C = 4, 8, 2  # batch, time, channels\nx = torch.randn(B, T, C)\nx.shape\n\ntorch.Size([4, 8, 2])\n\n# We want x[b,t] = mean_{i<=t} x[b,i]\nxbow = torch.zeros((B, T, C))\nfor b in range(B):\n    for t in range(T):\n        xprev = x[b, :t+1]  # (t, C)\n        xbow[b, t] = torch.mean(xprev, 0)\n\nwei = torch.tril(torch.ones(T, T))\nwei = wei / wei.sum(1, keepdim=True)\nxbow2 = wei @ x.reshape(B, T, C) @ (B, T, C)  # (B, T, C)\n\ntorch.tril(torch.ones(3, 3))\n```",
    "frame_3209.jpg": "```python\ntorch.manual_seed(1337)\nB,T,C = 4,8,2  # batch, time, channels\nx = torch.randn(B,T,C)\nx.shape\n\n# We want x[b,t] = mean_{i<=t} x[b,i]\nxbow = torch.zeros((B,T,C))\nfor b in range(B):\n    for t in range(T):\n        xprev = x[b,:t+1]  # (t,C)\n        xbow[b,t] = torch.mean(xprev, 0)\n\nwei = torch.tril(torch.ones(T, T))\nwei = wei / wei.sum(dim=1, keepdim=True)\nxbow2 = wei @ x @ (B, T, C)  # -----> (B, T, C)\ntorch.tril(torch.ones(3, 3))\ntorch.manual_seed(42)\n```",
    "frame_3215.jpg": "```python\ntorch.manual_seed(1337)\nB, T, C = 4, 8, 2  # batch, time, channels\nx = torch.randn(B, T, C)\nx.shape\n\ntorch.Size([4, 8, 2])\n\n# We want x[b,t] = mean_{i\u2264t} x[b,i]\nxbow = torch.zeros((B, T, C))\nfor b in range(B):\n    for t in range(T):\n        xprev = x[b, :t+1]  # (t, C)\n        xbow[b, t] = torch.mean(xprev, 0)\n\nwei = torch.tril(torch.ones(T, T))\nwei = wei / wei.sum(1, keepdim=True)\nxbow2 = wei @ x # (B, T) @ (B, T, C) ----> (B, T, C)\ntorch.allclose(xbow, xbow2)\n\ntorch.tril(torch.ones(3, 3))\ntensor([[1., 0., 0.],\n        [1., 1., 0.],\n        [1., 1., 1.]])\n```",
    "frame_3217.jpg": "```python\ntorch.manual_seed(1337)\nB, T, C = 4, 2, 8  # batch, time, channels\nx = torch.randn(B, T, C)\nx.shape\n\ntorch.Size([4, 2, 8])\n\n# We want x[b, t] = mean_{i<=t} x[b, i]\nxbow = torch.zeros((B, T, C))\nfor b in range(B):\n    for t in range(T):\n        xprev = x[b, :t+1]  # (t, C)\n        xbow[b, t] = torch.mean(xprev, 0)\n\nwei = torch.tril(torch.ones(T, T))\nwei = wei / wei.sum(1, keepdim=True)\nxbow2 = wei @ x  # (B, T) @ (B, T, C) ----> (B, T, C)\ntorch.allclose(xbow, xbow2)\n\ntorch.tril(torch.ones(3, 3))\ntensor([[1., 0., 0.],\n        [1., 1., 0.],\n        [1., 1., 1.]])\n```",
    "frame_3220.jpg": "```python\n# consider the following toy example:\ntorch.manual_seed(1337)\nB,T,C = 4,8,2  # batch, time, channels\nx = torch.randn(B,T,C)\nx.shape\n\ntorch.Size([4, 8, 2])\n\n# We want x[b,t] = mean_{i<=t} x[b,i]\nxwob = torch.zeros((B,T,C))\nfor b in range(B):\n    for t in range(T):\n        xprev = x[b,:t+1]  # (t,C)\n        xwob[b,t] = torch.mean(xprev, 0)\n\nwei = torch.tril(torch.ones(T, T))\nwei = wei / wei.sum(keepdim=True)\nxw2 = wei @ x.view(B, T, C) @ (B, T, C)  # -----> (B, T, C)\ntorch.allclose(xwob, xw2)\n\ntorch.tril(torch.ones(3, 3))\ntensor([[1., 0., 0.],\n        [1., 1., 0.],\n        [1., 1., 1.]])\n    \ntorch.manual_seed(42)\n```",
    "frame_3225.jpg": "```python\ntorch.manual_seed(1337)\nB, T, C = 4, 8, 2  # batch, time, channels\nx = torch.randn(B, T, C)\nx.shape\n\ntorch.Size([4, 8, 2])\n\n# We want x[b, t] = mean_{i<=t} x[b, i]\nxbow = torch.zeros((B, T, C))\nfor b in range(B):\n    for t in range(T):\n        xprev = xbow[b, t-1]  # (t, C)\n        xbow[b, t] = torch.mean(xprev, 0)\n\nwei = torch.tril(torch.ones(T, T))\nwei = wei / wei.sum(1, keepdim=True)\nxbow2 = wei @ x @ (B, T, C)  # -----> (B, T, C)\ntorch.allclose(xbow, xbow2)\n\ntorch.tril(torch.ones(3, 3))\ntensor([[1., 0., 0.],\n        [1., 1., 0.],\n        [1., 1., 1.]])\n```\n",
    "frame_3227.jpg": "Code NA",
    "frame_3231.jpg": "```python\nwei = torch.tril(torch.ones(T, T))\nwei = wei / wei.sum(1, keepdim=True)\nxbow2 = wei @ x  # (B, T, C) ----> (B, T, C)\ntorch.allclose(xbow, xbow2)\n```",
    "frame_3232.jpg": "```python\nfor t in range(T):\n    xprev = xbow[b, t+1]  # (t, C)\n    xbow[t] = torch.mean(xprev, 0)\n\nwei = torch.tril(torch.ones(T, T))\nwei = wei / wei.sum(1, keepdim=True)\nxbow2 = wei @ x @ (B, T, C)  # ---> (B, T, C)\ntorch.allclose(xbow, xbow2)\n```\n",
    "frame_3233.jpg": "Code NA",
    "frame_3234.jpg": "```python\ntorch.tril(torch.ones(3, 3))\ntorch.manual_seed(42)\na = torch.tril(torch.ones(3, 3))\na = a / torch.sum(a, 1, keepdim=True)\nb = torch.randint(0, 10, (3, 2)).float()\nc = a @ b\nprint('a=')\nprint(a)\nprint('-')\nprint('b=')\nprint(b)\nprint('-')\nprint('c=')\nprint(c)\n```",
    "frame_3235.jpg": "```python\n# We want x[b,t] = mean_{i<=t} x[b,i]\nxbow = torch.zeros((B,T,C))\nfor b in range(B):\n    for t in range(T):\n        xprev = x[b,:t+1]  # (t,C)\n        xbow[b,t] = torch.mean(xprev, 0)\n\nwei = torch.tril(torch.ones(T, T))\nwei = wei / wei.sum(keepdim=True)\nxbow2 = wei @ x  # (B, T, C) @ (B, T, C) -----> (B, T, C)\ntorch.allclose(xbow, xbow2)\n\ntorch.tril(torch.ones(3, 3))\n\na = torch.tril(torch.ones(3, 3))\na = a / torch.sum(a, 1, keepdim=True)\nb = torch.randint(0, 10, (3, 2)).float()\nc = a @ b\nprint('a=')\nprint(a)\nprint('b=')\nprint(b)\n```",
    "frame_3246.jpg": "```python\nxbow = torch.zeros((B, T, C))\nfor b in range(B):\n    for t in range(T):\n        xprev = x[b, t:t+1]  # (t, C)\n        xbow[b, t] = torch.mean(xprev, 0)\n\nwei = torch.tril(torch.ones(T, T))\nwei = wei / wei.sum(1, keepdim=True)\nxbow2 = x @ (B, T, C) @ (B, T, C)  # ---> (B, T, C)\ntorch.allclose(xbow, xbow2)\n\n(xbow[0][0], xbow2[0][0])  # Checking values\n```",
    "frame_3247.jpg": "```python\ntorch.manual_seed(1337)\nB, T, C = 4, 8, 2  # batch_time, time, channels\nx = torch.randn(B, T, C)\nx.shape\n\n# We want x[b, t] = mean_{i<=t} x[b, i]\nxbow = torch.zeros((B, T, C))\nfor b in range(B):\n    for t in range(T):\n        xprev = x[b, :t + 1]  # (t, C)\n        xbow[b, t] = torch.mean(xprev, 0)\n\nwei = torch.tril(torch.ones(T, T))\nwei = wei / wei.sum(keepdim=True)\nxbow2 = wei @ x.view(B, T, C)  # (B, T, C) ---> (B, T, C)\ntorch.allclose(xbow, xbow2)\n\ntorch.tril(torch.ones(3, 3))\ntensor([[1., 0., 0.],\n        [1., 1., 0.],\n        [1., 1., 1.]])\n        \ntorch.manual_seed(42)\na = a / torch.sum(a, keepdim=True)\nb = torch.randint(0, 3, (2,)).float()\n```",
    "frame_3253.jpg": "```python\ntorch.manual_seed(1337)\nB, T, C = 4, 8, 2  # batch, time, channels\nx = torch.randn(B, T, C)\nx.shape\n\n# We want x[b,t] = mean_{i<=t} x[b,i]\nxbow = torch.zeros((B, T, C))\nfor b in range(B):\n    for t in range(T):\n        xprev = x[b, :t+1]  # (t, C)\n        xbow[b, t] = torch.mean(xprev, 0)\n\nwei = torch.tril(torch.ones(T, T))\nwei = wei / wei.sum(1, keepdim=True)\nwei2 = wei @ x  # (B, T, C) @ (B, T, C) ----> (B, T, C)\ntorch.allclose(xbow, wei2)\n\ntorch.tril(torch.ones(3, 3))\ntensor([[1., 0., 0.],\n        [1., 1., 0.],\n        [1., 1., 1.]])\n```",
    "frame_3256.jpg": "```python\n# consider the following toy example:\ntorch.manual_seed(1337)\nB,T,C = 4,8,2  # batch, time, channels\nx = torch.randn(B,T,C)\nx.shape\n\ntorch.Size([4, 8, 2])\n\n# We want x[b,t] = mean_{i<=t} x[b,i]\nxbow = torch.zeros((B,T,C))\nfor b in range(B):\n    for t in range(T):\n        xprev = x[b,:t+1]  # (t,C)\n        xbow[b,t] = torch.mean(xprev, 0)\n\nwei = torch.tril(torch.ones(T, T))\nwei = wei / wei.sum(1, keepdim=True)\nxbow2 = wei @ x # (B, T, C) @ (B, T, C) ----> (B, T, C)\ntorch.allclose(xbow, xbow2)\n\ntorch.tril(torch.ones(3, 3))\ntensor([[1., 0., 0.],\n        [1., 1., 0.],\n        [1., 1., 1.]])\n```",
    "frame_3264.jpg": "```python\ntorch.manual_seed(1337)\nB,T,C = 4,8,2  # batch, time, channels\nx = torch.randn(B,T,C)\nx.shape\n\ntorch.Size([4, 8, 2])\n\n# We want x[b, t] = mean_{i<=t} x[b, i]\nxbow = torch.zeros((B,T,C))\nfor b in range(B):\n    for t in range(T):\n        xprev = x[b, :t+1]  # (t,C)\n        xbow[b, t] = torch.mean(xprev, 0)\n\nwei = torch.tril(torch.ones(T, T)),\nwei = wei / wei.sum(1, keepdim=True)\nxbow2 = wei @ x @ (B, T, C)  # -----> (B, T, C)\ntorch.allclose(xbow, xbow2)\n\ntorch.tril(torch.ones(3, 3))\n```",
    "frame_3266.jpg": "```\ntorch.manual_seed(1337)\nB, T, C = 4, 8, 2  # batch, time, channels\nx = torch.randn(B, T, C)\nx.shape\n\ntorch.Size([4, 8, 2])\n\n# We want x[b, t] = mean_{i<=t} x[b, i]\nxbow = torch.zeros((B, T, C))\nfor b in range(B):\n    for t in range(T):\n        xprev = x[b, :t + 1]  # (t, C)\n        xbow[b, t] = torch.mean(xprev, 0)\n\nwei = torch.tril(torch.ones(T, T))\nwei = wei / wei.sum(1, keepdim=True)\nxbow2 = wei @ x @ (B, T, C)  # -----> (B, T, C)\ntorch.allclose(xbow, xbow2)\n\ntorch.tril(torch.ones(3, 3))\n```",
    "frame_3278.jpg": "```python\ntorch.manual_seed(1337)\nB, T, C = 4, 8, 2  # batch, time, channels\nx = torch.randn(B, T, C)\nx.shape\n\n# We want x[b, t] = mean_{i<=t} x[b, i]\nxbow = torch.zeros((B, T, C))\nfor b in range(B):\n    for t in range(T):\n        xprev = x[b, :t+1]  # (t, C)\n        xbow[b, t] = torch.mean(xprev, 0)\n\nwei = torch.tril(torch.ones(T, T))\nwei = wei / wei.sum(1, keepdim=True)\nxbow2 = wei @ x  # (B, T, C) @ (B, T, C) ----> (B, T, C)\ntorch.allclose(xbow, xbow2)\n\ntorch.tril(torch.ones(3, 3))\n```",
    "frame_3283.jpg": "```python\ntorch.manual_seed(42)\na = torch.tril(torch.ones(3, 3))\na = a / torch.sum(a, 1, keepdim=True)\nb = torch.randint(0, 10, (3, 2)).float()\nc = a @ b\nprint('a=')\nprint(a)\nprint('---')\nprint('b=')\nprint(b)\nprint('c=')\nprint(c)\n```",
    "frame_3291.jpg": "Here is the extracted developer code from the image:\n\n```python\nfor b in range(B):\n    for t in range(T):\n        xprev = x[b, t : t + 1]  # (b, c)\n        xbow[b] = torch.mean(xprev, 0)\n\n# version 2\nwei = torch.tril(torch.ones(T, T))\nwei = wei / wei.sum(1, keepdim=True)\nxbow2 = wei @ x  # (B, T, C) @ (B, T, C) ----> (B, T, C)\ntorch.allclose(xbow, xbow2)\n\n# version 3: use Softmax\ntril = torch.tril(torch.ones(T, T))\nwei = torch.zeros((T, T))\nwei = wei.masked_fill(tril == 0, float('-inf'))\nwei = F.softmax(wei, dim=-1)\nxbow3 = wei @ x\ntorch.allclose(xbow, xbow3)\n\ntorch.manual_seed(42)\na = torch.tril(torch.ones(3, 3))\na = a / torch.sum(a, 1, keepdim=True)\nb = torch.randint(0, 10, (3, 2)).float()\nc = a @ b\nprint('a=')\nprint(a)\nprint('---')\n```\n\nIf you need further assistance, feel free to ask!",
    "frame_3294.jpg": "```python\nfor b in range(B):\n    for t in range(T):\n        xprev = x[b, t + 1]  # (t, C)\n        xbow[b, t] = torch.mean(xprev, 0)\n\n# version 2\nwei = torch.tril(torch.ones(T, T))\nwei = wei / wei.sum(1, keepdim=True)\nxbow2 = wei @ x @ (B, T, C)  # ---> (B, T, C)\ntorch.allclose(xbow, xbow2)\n\n# version 3: use Softmax\ntril = torch.tril(torch.ones(T, T))\nwei = torch.zeros((T, T))\nwei = wei.masked_fill(tril == 0, float('-inf'))\nxbow3 = wei @ x\ntorch.allclose(xbow, xbow3)\n\ntorch.manual_seed(42)\na = torch.tril(torch.ones(3, 3))\na = a / torch.sum(a, 1, keepdim=True)\nb = torch.randint(0, 10, (3, 2)).float()\nc = a @ b\n```",
    "frame_3301.jpg": "```python\n# version 2\nwei = torch.tril(torch.ones(T, T))\nwei = wei / wei.sum(1, keepdim=True)\nxbow2 = wei @ x # (B, T, C) ----> (B, T, C)\ntorch.allclose(xbow, xbow2)\n\nTrue\n\ntril = torch.tril(torch.ones(T, T))\nwei = torch.zeros((T,T))\nwei = wei.masked_fill(tril == 0, float('-inf'))\nwei = F.softmax(wei, dim=-1)\nxbow3 = wei @ x\ntorch.allclose(xbow, xbow3)\n\nTrue\n\ntorch.manual_seed(42)\na = torch.tril(torch.ones(3, 3))\n```",
    "frame_3305.jpg": "Here's the extracted developer code from the image:\n\n```python\nwei = torch.tril(torch.ones(T, T))\nwei = wei / wei.sum(1, keepdim=True)\nxbow2 = wei @ x  # @ (B, T, C) -----> (B, T, C)\ntorch.allclose(xbow, xbow2)\n\ntril = torch.tril(torch.ones(T, T))\nwei = torch.zeros((T, T))\nwei = wei.masked_fill(tril == 0, float('-inf'))\nwei = F.softmax(wei, dim=-1)\nxbow3 = wei @ x\ntorch.allclose(xbow, xbow3)\n```\n\nLet me know if you need anything else!",
    "frame_3308.jpg": "```python\n# version z\nwei = torch.tril(torch.ones(T, T))\nwei = wei / wei.sum(1, keepdim=True)\nxbow2 = wei @ x  # (B, T, C) -----> (B, T, C)\ntorch.allclose(xbow, xbow2)\n\nTrue\n\ntril\ntensor([[1., 0., 0., 0., 0., 0.],\n        [1., 1., 0., 0., 0., 0.],\n        [1., 1., 1., 0., 0., 0.],\n        [1., 1., 1., 1., 0., 0.],\n        [1., 1., 1., 1., 1., 0.],\n        [1., 1., 1., 1., 1., 1.]])\n\nwei = torch.zeros((T, T))\nwei\ntensor([[0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0.]])\n\n# version 3: use Softmax\ntril = torch.tril(torch.ones(T, T))\nwei = torch.zeros((T, T))\n```",
    "frame_3310.jpg": "```python\ntril = torch.tril(torch.ones(T, T))\nwei = torch.zeros((T, T))\nwei = wei.masked_fill(tril == 0, float('-inf'))\nwei = F.softmax(wei, dim=-1)\nxbow3 = wei @ x\ntorch.allclose(xbow, xbow3)\n```",
    "frame_3314.jpg": "```python\ntril = torch.tril(torch.ones(T, T))\nwei = torch.zeros((T, T))\nwei = wei.masked_fill(tril == 0, float('-inf'))\nwei\n```\n",
    "frame_3330.jpg": "```\nwei = torch.zeros((T,T))\nwei = wei.masked_fill(tril == 0, float('-inf'))\nwei\ntensor([[0., -inf, -inf, -inf, -inf, -inf],\n        [0., -inf, -inf, -inf, -inf, -inf],\n        [0., 0., -inf, -inf, -inf, -inf],\n        [0., 0., 0., -inf, -inf, -inf],\n        [0., 0., 0., 0., -inf, -inf],\n        [0., 0., 0., 0., 0., -inf]])\n# version 3: use Softmax\ntril = torch.tril(torch.ones(T, T))\nwei = torch.zeros((T,T))\nwei = wei.masked_fill(tril == 0, float('-inf'))\nwei = F.softmax(wei, dim=-1)\nxbow3 = wei @ x\ntorch.allclose(xbow, xbow3)\n```",
    "frame_3353.jpg": "```python\nwei = torch.zeros((T,T))\nwei = wei.masked_fill(tril == 0, float('-inf'))\nwei = F.softmax(wei, dim=-1)\n```\n",
    "frame_3357.jpg": "```python\nwei = torch.zeros((T, T))\nwei = wei.masked_fill(tril == 0, float('-inf'))\nwei = F.softmax(wei, dim=-1)\n```\n",
    "frame_3360.jpg": "Here's the extracted code from the image:\n\n```python\nwei = torch.zeros((T,T))\nwei = wei.masked_fill(tril == 0, float('-inf'))\n# wei = F.softmax(wei, dim=-1)\nwei\n```\n\n```python\n# version 3: use Softmax\ntril = torch.tril(torch.ones((T, T)))\nwei = torch.zeros((T,T))\nwei = wei.masked_fill(tril == 0, float('-inf'))\nwei\nxbow3 = wei @ x\ntorch.allclose(xbow, xbow3)\n```",
    "frame_3361.jpg": "```python\nwei = torch.zeros((T,T))\nwei = wei.masked_fill(tril == 0, float('-inf'))\n# wei = F.softmax(wei, dim=-1)\nwei\n```\n",
    "frame_3393.jpg": "```python\n# version 3: use Softmax\ntril = torch.tril(torch.ones(T, T))\nwei = torch.zeros((T,T))\nwei = wei.masked_fill(tril == 0, float('-inf'))\nwei = F.softmax(wei, dim=-1)\nxbow3 = wei @ x\ntorch.allclose(xbow, xbow3)\n\ntorch.manual_seed(42)\na = torch.tril(torch.ones(3, 3))\na = a / torch.sum(a, keepdim=True)\nb = torch.randint(0, 10, (3, 2)).float()\nc = a @ b\nprint('a=')\nprint(a)\nprint('b=')\nprint(b)\n```",
    "frame_3395.jpg": "```python\n# version 3: use Softmax\ntril = torch.tril(torch.ones(T, T))\nwei = torch.zeros((T, T))\nwei = wei.masked_fill(tril == 0, float('-inf'))\nwei = F.softmax(wei, dim=-1)\nxbow3 = wei @ x\ntorch.allclose(xbow, xbow3)\n\ntorch.manual_seed(42)\na = torch.tril(torch.ones(3, 3))\na = a / torch.sum(a, 1, keepdim=True)\nb = torch.randint(0, 10, (3, 2)).float()\nc = a @ b\nprint('a=', a)\nprint(a)\n```",
    "frame_3406.jpg": "```python\n# version 3: use Softmax\ntri1 = torch.tril(torch.ones(T, T))\nwei = torch.zeros((T,T))\nwei = wei.masked_fill(tri1 == 0, float('-inf'))\nwei = F.softmax(wei, dim=-1)\nxbow3 = wei @ x\ntorch.allclose(xbow, xbow3)\n\n# Other code\ntorch.manual_seed(42)\na = torch.tril(torch.ones(3, 3))\na = a / torch.sum(a, 1, keepdim=True)\nb = torch.randint(0, 10, (3, 2)).float()\nc = a @ b\nprint('a=')\nprint(a)\nprint('b=')\nprint(b)\nprint('c=')\nprint(c)\n```",
    "frame_3415.jpg": "```python\n# version 3: use Softmax\ntri1 = torch.tril(torch.ones(T, T))\nwei = torch.zeros((T, T))\nwei = wei.masked_fill(tri1 == 0, float('-inf'))\nwei = F.softmax(wei, dim=-1)\nxbow3 = wei @ x\ntorch.allclose(xbow, xbow3)\n\ntorch.manual_seed(42)\na = torch.tril(torch.ones(3, 3))\na = a / torch.sum(a, 1, keepdim=True)\nb = torch.randint(0, 10, (3, 2)).float()\nc = a @ b\nprint(a)\nprint('a=')\nprint(a)\n```\n",
    "frame_3420.jpg": "```python\n# version 3: use Softmax\ntril = torch.tril(torch.ones(T, T))\nwei = torch.zeros((T,T))\nwei = wei.masked_fill(tril == 0, float('-inf'))\nwei = F.softmax(wei, dim=-1)\nxbow3 = wei @ x\ntorch.allclose(xbow, xbow3)\n\ntorch.manual_seed(42)\na = torch.tril(torch.ones(3, 3))\na = a / torch.sum(a, 1, keepdim=True)\nb = torch.randint(0, 10, (3, 2)).float()\nc = a @ b\nprint(a)\nprint('a=', a)\n```",
    "frame_3422.jpg": "```python\n# version 3: use Softmax\ntri1 = torch.tril(torch.ones(T, T))\nwei = torch.zeros((T, T))\nwei = wei.masked_fill(tri1 == 0, float('-inf'))\nwei1 = F.softmax(wei, dim=-1)\nxbow3 = wei1 @ x\ntorch.allclose(xbow, xbow3)\n\n# Additional code\ntorch.manual_seed(42)\na = torch.tril(torch.ones(3, 3))\na = a / torch.sum(a, 1, keepdim=True)\nb = torch.randint(0, 10, (3, 2)).float()\nprint(a)\nprint('=') \nprint(b)\n```",
    "frame_3423.jpg": "Here\u2019s the extracted code from the image:\n\n```python\n# version 3: use Softmax\ntril = torch.tril(torch.ones(T, T))\nwei = torch.zeros((T, T))\nwei = wei.masked_fill(tril == 0, float('-inf'))\nwei = F.softmax(wei, dim=-1)\nxbow3 = wei @ x\ntorch.allclose(xbow, xbow3)\n\ntorch.manual_seed(42)\na = torch.tril(torch.ones(3, 3))\na = a / torch.sum(a, 1, keepdim=True)\nb = torch.randint(0, 10, (3, 2)).float()\nc = a @ b\nprint(a)\nprint('a=') \n```\n\nIf you need more details or additional commands, let me know!",
    "frame_3430.jpg": "```python\ndef softmax(input: Tensor, dim: Optional[int] = None, _stacklevel: int = 3, dtype: Optional[DTYPE] = None) -> Tensor:\n    tril = torch.tril(torch.ones(3, 3))\n    wei = torch.dtype\n    wei = F.softmax(wei, dim=-1)\n    xbow3 = wei @ x\n    torch.allclose(xbow, xbow3)\n\ntorch.manual_seed(42)\na = torch.tril(torch.ones(3, 3))\na = a / torch.sum(a, 1, keepdim=True)\nb = torch.randint(0, 10, (3, 2)).float()\nc = a @ b\nprint(a)\n```",
    "frame_3431.jpg": "```python\n# version 3: use Softmax\ntril = torch.tril(torch.ones(T, T))\nwei = torch.zeros((T, T))\nwei = wei.masked_fill(tril == 0, float('-inf'))\nwei = F.softmax(wei, dim=-1)\nxbow3 = wei @ x\ntorch.allclose(xbow, xbow3)\n\ntorch.manual_seed(42)\na = torch.tril(torch.ones(3, 3))\na = a / torch.sum(a, keepdim=True)\nb = torch.randint(0, 10, (3, 2)).float()\nc = a @ b\nprint(a)\nprint('a=')\nprint(a)\n```",
    "frame_3442.jpg": "```python\n# version 3: use Softmax\ntril = torch.tril(torch.ones(T, T))\nwei = torch.zeros((T,T))\nwei = wei.masked_fill(tril == 0, float('-inf'))\nwei = F.softmax(wei, dim=-1)\nxbow3 = wei @ x\ntorch.allclose(xbow, xbow3)\n\ntorch.manual_seed(42)\na = torch.tril(torch.ones(3, 3))\nb = a / torch.sum(a, 1, keepdim=True)\nc = torch.randint(0, 10, (3, 2)).float()\nc = a @ b\nprint(a)\nprint('a=')\nprint(a)\n```",
    "frame_3444.jpg": "```python\n# version 3: use Softmax\ntril = torch.tril(torch.ones(T, T))\nwei = torch.zeros((T, T))\nwei = wei.masked_fill(tril == 0, float('-inf'))\nwei = F.softmax(wei, dim=-1)\nxbow3 = wei @ x\ntorch.allclose(xbow, xbow3)\n\n# Additional code\ntorch.manual_seed(42)\na = torch.tril(torch.ones(3, 3))\nb = torch.randint(0, 10, (3, 2)).float()\nc = a @ b\nprint(a)\nprint('a=', a)\nprint(b)\n```",
    "frame_3453.jpg": "```python\ntril = torch.tril(s(T, T))\nwei = torch.zeros((T, T))\nwei = wei.masked_fill(tril == 0, float('-inf'))\nwei = F.softmax(wei, dim=-1)\nxbow3 = wei @ x\ntorch.allclose(xbow, xbow3)\n\n# Additional code \ntorch.manual_seed(42)\na = torch.tril(torch.ones(3, 3))\na = a / torch.sum(a, 1, keepdim=True)\na = torch.randint(0, 10, (3, 2)).float()\n```",
    "frame_3457.jpg": "```python\ntril = torch.tril(torch.ones((T,T)))\nwei = torch.zeros((T,T))\nwei = wei.masked_fill(tril == 0, float('-inf'))\nwei = F.softmax(wei, dim=-1)\nxbow3 = wei @ x\ntorch.allclose(xbow, xbow3)\n\n# Random seed and tensor operations\ntorch.manual_seed(42)\na = torch.tril(torch.ones(3, 3))\na = a / torch.sum(a, 1, keepdim=True)\na = torch.randint(0, 10, (3, 2)).float()\n```",
    "frame_3463.jpg": "```\ntril = torch.tril(torch.tensor([[0., 0., 0., 0., 0., 0.],\n                                 [1., 0., 0., 0., 0., 0.],\n                                 [1., 1., 0., 0., 0., 0.],\n                                 [1., 1., 1., 0., 0., 0.],\n                                 [1., 1., 1., 1., 0., 0.],\n                                 [1., 1., 1., 1., 1., 1.]]))\nwei = torch.zeros((T, T))\nwei = wei.masked_fill(tril == 0, float('-inf'))\nwei = F.softmax(wei, dim=-1)\nxbow3 = wei @ x\ntorch.allclose(xbow, xbow3)\n```",
    "frame_3478.jpg": "```python\n# version 3: use Softmax\ntril = torch.tril(torch.ones(T, T))\nwei = torch.zeros((T, T))\nwei = wei.masked_fill((parameter) dim: int | None\nwei = F.softmax(wei, dim=-1)\nxbow3 = wei @ x\ntorch.allclose(xbow, xbow3)\n\n# Another segment\ntorch.manual_seed(42)\na = torch.tril(torch.ones(3, 3))\na = a / torch.sum(a, keepdim=True)\nb = torch.randint(0, 10, (3, 2)).float()\nc = a @ b\nprint(a)\nprint(a='a')\n```",
    "frame_3484.jpg": "Here is the extracted developer code from the image:\n\n```python\n# version 2\nwei = torch.tril(torch.ones(T, T))\nwei = wei / wei.sum(keepdim=True)\nxbow2 = wei @ x @ (B, T, C) @ (B, T, C)\ntorch.allclose(xbow, xbow2)\n\n# version 3: use Softmax\ntril = torch.tril(torch.ones(T, T))\nwei = torch.zeros((T,T))\nwei = wei.masked_fill(tril == 0, float('-inf'))\nwei = F.softmax(wei, dim=-1)\nxbow3 = wei @ x\ntorch.allclose(xbow, xbow3)\n```",
    "frame_3488.jpg": "```python\n# version 3: use Softmax\ntril = torch.tril(torch.ones(T, T))\nwei = torch.zeros((T, T))\nwei = wei.masked_fill(tril == 0, float('-inf'))\nwei = F.softmax(wei, dim=-1)\nxbow3 = wei @ x\ntorch.allclose(xbow, xbow3)\n\ntorch.manual_seed(42)\na = torch.tril(torch.ones(3, 3))\na = torch.sum(a, 1, keepdim=True)\nb = torch.randint(0, 10, (3, 2)).float()\n```",
    "frame_3502.jpg": "```python\nxbow2 = wei @ x @ (B, T, C)  # (B, T, C)\ntorch.allclose(xbow, xbow2)\n\ntril = torch.tril(torch.ones(T, T))\nwei = torch.zeros((T, T))\nwei = wei.masked_fill(tril == 0, float('-inf'))\nwei = F.softmax(wei, dim=-1)\nxbow3 = wei @ x\ntorch.allclose(xbow, xbow3)\n\ntorch.manual_seed(42)\na = torch.tril(torch.ones(3, 3))\na = torch.sum(a, 1, keepdim=True)\nc = a @ b\nprint('a=')\n```",
    "frame_3508.jpg": "Here's the extracted developer code from the image:\n\n```python\n# super simple bigram model\nclass BigramLanguageModel(nn.Module):\n\n    def __init__(self, vocab_size):\n        super().__init__()\n        # each token directly reads off the logits for the next token from a lookup table\n        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n\n    def forward(self, idx, targets=None):\n        # idx and targets are both (B,T) tensor of integers\n        logits = self.token_embedding_table(idx)  # (B,T,C)\n        \n        if targets is None:\n            loss = None\n        else:\n            B, T, C = logits.shape\n            logits = logits.view(B*T, C)\n            targets = targets.view(B*T)\n            loss = F.cross_entropy(logits, targets)\n\n        return logits, loss\n\n    def generate(self, idx, max_new_tokens):\n        # idx is (B,T) array of indices in the current context\n```\n\nIf you need anything else, feel free to ask!",
    "frame_3515.jpg": "```python\n# super simple bigram model\nclass BigramLanguageModel(nn.Module):\n    def __init__(self, vocab_size):\n        super().__init__()\n        # each token directly reads off the logits for the next token from a lookup table\n        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n\n    def forward(self, idx, targets=None):\n        # idx and targets are both (B,T) tensor of integers\n        logits = self.token_embedding_table(idx) # (B,T,C)\n\n        if targets is None:\n            loss = None\n        else:\n            B, T, C = logits.shape\n            logits = logits.view(B*T, C)\n            targets = targets.view(B*T)\n            loss = F.cross_entropy(logits, targets)\n\n        return logits, loss\n\n    def generate(self, idx, max_new_tokens):\n        # idx is (B,T) array of indices in the current context\n        for _ in range(max_new_tokens):\n            pass\n```\n",
    "frame_3518.jpg": "```python\ndef generate(self, idx, max_new_tokens):\n    # idx is (B, T) array of indices in the current context\n    for _ in range(max_new_tokens):\n        # get the predictions\n        logits = self(idx)\n        # only on the last time step\n        logits = logits[:, -1, :]  # becomes (B, C)\n        # apply softmax to get probabilities\n        probs = F.softmax(logits, dim=-1)  # (B, C)\n        # sample from the distribution\n        idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n        # append sampled index to the running sequence\n        idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n    return idx\n\nmodel = BigramLanguageModel(vocab_size)\nmodel = model.to(device)\n\n# create a PyTorch optimizer\noptimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n\nfor iter in range(max_iters):\n    # every once in a while evaluate the loss on train and val sets\n    if iter % eval_interval == 0:\n        losses = estimate_loss()\n        print(f\"step {iter}: train loss {losses['train1:.4f']}, val loss {losses['val']:.4f}\")\n\n# sample a batch of data\n```",
    "frame_3521.jpg": "```python\nclass BigramLanguageModel(nn.Module):\n\n    def __init__(self):\n        super().__init__()\n        # each token directly reads off the logits for the next token from a lookup table\n        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n\n    def forward(self, idx, targets=None):\n        # idx and targets are both (B, T) tensor of integers\n        logits = self.token_embedding_table(idx)  # (B,T,C)\n\n        if targets is None:\n            loss = None\n        else:\n            B, T, C = logits.shape\n            logits = logits.view(B * T, C)\n            targets = targets.view(B * T)\n            loss = F.cross_entropy(logits, targets)\n\n        return logits, loss\n\n    def generate(self, idx, max_new_tokens):\n        # idx is (B, T) array of indices in the current context\n        for _ in range(max_new_tokens):\n            # get the predictions\n            logits, loss = self(idx)\n            logits = logits[:, -1, :]  # becomes (B, C)\n```\n",
    "frame_3524.jpg": "```python\nclass BigramLanguageModel(nn.Module):\n\n    def __init__(self):\n        super().__init__()\n        # each token directly reads off the logits for the next token from a lookup table\n        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n\n    def forward(self, idx, targets=None):\n        # idx and targets are both (B,T) tensor of integers\n        logits = self.token_embedding_table(idx)  # (B,T,C)\n\n        if targets is None:\n            loss = None\n        else:\n            B, T, C = logits.shape\n            logits = logits.view(B*T, C)\n            targets = targets.view(B*T)\n            loss = F.cross_entropy(logits, targets)\n\n        return logits, loss\n\n    def generate(self, idx, max_new_tokens):\n        # if idx is (B,T) array of indices in the current context\n```\n",
    "frame_3527.jpg": "```python\nclass BigramLanguageModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        # each token directly reads off the logits for the next token from a lookup table\n        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n\n    def forward(self, idx, targets=None):\n        # idx and targets are both (B,T) tensor of integers\n        logits = self.token_embedding_table(idx)  # (B,T,C)\n\n        if targets is None:\n            loss = None\n        else:\n            B, T, C = logits.shape\n            logits = logits.view(B*T, C)\n            targets = targets.view(B*T)\n            loss = F.cross_entropy(logits, targets)\n```",
    "frame_3547.jpg": "```python\nimport torch\nfrom torch.nn import functional as F\n\n# hyperparameters\nbatch_size = 32  # how many independent sequences will we process in parallel?\nblock_size = 8   # what is the maximum context length for predictions?\nmax_iters = 3000\neval_interval = 300\nlearning_rate = 1e-2\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\neval_iters = 200\n\ntorch.manual_seed(1337)\n\n# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\nwith open('input.txt', 'r', encoding='utf-8') as f:\n    text = f.read()\n\n# here are all the unique characters that occur in this text\nchars = sorted(list(set(text)))\nvocab_size = len(chars)\n# create a mapping from characters to integers\nstoi = {ch: i for i, ch in enumerate(chars)}\nitos = {i: ch for i, ch in enumerate(chars)}\nencode = lambda s: [stoi[c] for c in s]  # encoder: take a string, output a list of integers\ndecode = lambda l: ''.join([itos[i] for i in l])  # decoder: take a list of integers, output a string\n```",
    "frame_3587.jpg": "```python\n# super simple bigram model\nclass BigramLanguageModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        # each token directly reads off the logits for the next token from a lookup table\n        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n        self.lm_head = nn.Linear(n_embd, vocab_size)\n\n    def forward(self, idx, targets=None):\n        # idx and targets are both (B,T) tensor of integers\n        tok_emb = self.token_embedding_table(idx)  # (B,T,C)\n        logits = self.lm_head(tok_emb)  # (B,T,C)\n\n        if targets is None:\n            loss = None\n        else:\n            B, T, C = logits.shape\n            logits = logits.view(B*T, C)\n            targets = targets.view(B*T)\n            loss = F.cross_entropy(logits, targets)\n\n        return logits, loss\n\n    def generate(self, idx, max_new_tokens):\n        # idx is (B,T) array of indices in the current context\n```\n",
    "frame_3612.jpg": "```python\n# super simple bigram model\nclass BigramLanguageModel(nn.Module):\n\n    def __init__(self):\n        super().__init__()\n        # each token directly reads off the logits for the next token from a lookup table\n        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n        self.lm_head = nn.Linear(n_embd, vocab_size)\n\n    def forward(self, idx, targets=None):\n        # idx and targets are both (B,T) tensor of integers\n        tok_emb = self.token_embedding_table(idx)  # (B,T,C)\n        logits = self.lm_head(tok_emb)  # (B,T,vocab_size)\n\n        if targets is None:\n            loss = None\n        else:\n            B, T, C = logits.shape\n            logits = logits.view(B*T, C)\n            targets = targets.view(B*T)\n            loss = F.cross_entropy(logits, targets)\n\n        return logits, loss\n\n    def generate(self, idx, max_new_tokens):\n        # idx is (B,T) array of indices in the current context\n```\n",
    "frame_3654.jpg": "```python\n# super simple bigram model\nclass BigramLanguageModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        # each token directly reads off the logits for the next token from a lookup table\n        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n        self.lm_head = nn.Linear(n_embd, vocab_size)\n\n    def forward(self, idx, targets=None):\n        B, T = idx.shape\n        # idx is (B,T) tensor of integers\n        # idx and targets are both (B,T) tensor of integers\n        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n        logits = self.lm_head(tok_emb) # (B,T,vocab_size)\n\n        if targets is None:\n            loss = None\n        else:\n            B, T, C = logits.shape\n            logits = logits.view(B*T, C)\n            targets = targets.view(B*T)\n            loss = F.cross_entropy(logits, targets)\n\n        return logits, loss\n```",
    "frame_3665.jpg": "```python\n# super simple bigram model\nclass BigramLanguageModel(nn.Module):\n\n    def __init__(self):\n        super().__init__()\n        # each token directly reads off the logit\n        self.token_embedding_table = nn.Embedding(vocab_size, embedding_dim)\n        self.position_embedding_table = nn.Embedding(max_length, embedding_dim)\n        self.lm_head = nn.Linear(embedding_dim, vocab_size)\n\n    def forward(self, idx, targets=None):\n        B, T = idx.shape\n        # idx and targets are both (B, T) tensor\n        tok_emb = self.token_embedding_table(idx)\n        pos_emb = self.position_embedding_table(torch.arange(T, device=device))  # (T, C)\n        logits = self.lm_head(tok_emb)\n\n        if targets is None:\n            loss = None\n        else:\n            B, T, C = logits.shape\n            logits = logits.view(B*T, C)\n            targets = targets.view(B*T)\n            loss = F.cross_entropy(logits, targets)\n\n        return logits, loss\n```",
    "frame_3706.jpg": "```python\n# super simple bigram model\nclass BigramLanguageModel(nn.Module):\n\n    def __init__(self):\n        super().__init__()\n        # each token directly reads off the logits for the next token from a lookup table\n        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n        self.lm_head = nn.Linear(n_embd, vocab_size)\n\n    def forward(self, idx, targets=None):\n        B, T = idx.shape\n        # idx and targets are both (B,T) tensor of integers\n        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n        x = tok_emb + pos_emb # (B,T,C)\n        logits = self.lm_head(x) # (B,T,vocab_size)\n\n        if targets is None:\n            loss = None\n        else:\n            B, T, C = logits.shape\n            logits = logits.view(B, T, C)\n            targets = targets.view(B, T)\n            loss = F.cross_entropy(logits, targets)\n```",
    "frame_3717.jpg": "Code NA",
    "frame_3726.jpg": "```python\n# version 4: self-attention!\ntorch.manual_seed(1337)\nB, T, C = 4, 8, 32  # batch_size, time_channels\nx = torch.randn(B, T, C)\n\ntril = torch.tril(torch.ones(T, T))\nwei = torch.zeros((T, T))\nwei = wei.masked_fill(tril == 0, float('-inf'))\nwei = F.softmax(wei, dim=-1)\nout = wei @ x\n\nout.shape\ntorch.Size([4, 8, 32])\n\n# [44] tril\ntensor([[1., 0., 0., 0., 0., 0., 0., 0.],\n        [1., 1., 0., 0., 0., 0., 0., 0.],\n        [1., 1., 1., 0., 0., 0., 0., 0.],\n        [1., 1., 1., 1., 0., 0., 0., 0.],\n        [1., 1., 1., 1., 1., 0., 0., 0.],\n        [1., 1., 1., 1., 1., 1., 0., 0.],\n        [1., 1., 1., 1., 1., 1., 1., 0.],\n        [1., 1., 1., 1., 1., 1., 1., 1.]])\n\n# [45] wei\ntensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.3333, 0.6667, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n        [0.0000, 0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]])\n```",
    "frame_3735.jpg": "```python\n# version 4: self-attention!\ntorch.manual_seed(1337)\nB, T, C = 4, 8, 32  # batch, time, channels\nx = torch.randn(B, T, C)\n\ntril = torch.tril(torch.ones(T, T))\nwei = torch.zeros((T, T))\nwei = wei.masked_fill(tril == 0, float('-inf'))\nwei = F.softmax(wei, dim=-1)\nout = wei @ x\n\nout.shape\ntorch.Size([4, 8, 32])\n```",
    "frame_3766.jpg": "```python\n# version 4: self-attention!\ntorch.manual_seed(1337)\nB, T, C = 4, 8, 32  # batch, time, channels\nx = torch.randn(B, T, C)\n\ntril = torch.tril(torch.ones(T, T))\nwei = torch.zeros((T, T))\nwei = wei.masked_fill(tril == 0, float('-inf'))\nwei = F.softmax(wei, dim=-1)\nout = wei @ x \n\nout.shape\ntorch.Size([4, 8, 32])\n```",
    "frame_3810.jpg": "```python\nB, T, C = 4, 8, 32  # batch, time, channels\nx = torch.randn(B, T, C)\ntril = torch.tril(torch.ones(T, T))\nwei = torch.zeros((T, T))\nwei = wei.masked_fill(tril == 0, float('-inf'))\nwei = F.softmax(wei, dim=-1)\nout = wei @ x\n\nout.shape\ntorch.Size([4, 8, 32])\n```",
    "frame_3823.jpg": "```python\nB, T, C = 4, 8, 32  # batch, time, channels\nx = torch.randn(B, T, C)\n\ntril = torch.tril(torch.ones(T, T))\nwei = torch.zeros((T, T))\nwei = wei.masked_fill(tril == 0, float('-inf'))\nwei = F.softmax(wei, dim=-1)\nout = wei @ x\n\nout.shape\ntorch.Size([4, 8, 32])\n```",
    "frame_3836.jpg": "```python\nB, T, C = 4, 8, 32  # batch, time, channels\nx = torch.randn(B, T, C)\n\ntril = torch.tril(torch.ones(T, T))\nwei = torch.zeros((T, T))\nwei = wei.masked_fill(tril == 0, float('-inf'))\nwei = F.softmax(wei, dim=-1)\nout = wei @ x\n\nout.shape\ntorch.Size([4, 8, 32])\n```\n",
    "frame_3840.jpg": "Here's the extracted developer code from the image:\n\n```python\ntorch.manual_seed(1337)\nB,T,C = 4,8,32 # batch, time, channels\nx = torch.rand(B,T,C)\n\ntril = torch.tril(torch.ones(T, T))\nwei = torch.zeros((T,T))\nwei = wei.masked_fill(tril == 0, float('-inf'))\nwei = F.softmax(wei, dim=-1)\nout = wei @ x\n\nout.shape\ntorch.Size([4, 8, 32])\n```",
    "frame_3847.jpg": "```python\n# version 4: self-attention!\ntorch.manual_seed(1337)\nB, T, C = 4, 8, 32  # batch, time, channels\nx = torch.rand(B, T, C)\n\ntri = torch.tril(torch.ones(T, T))\nwei = torch.zeros((T, T))\nwei = wei.masked_fill(tri == 0, float('-inf'))\nwei = F.softmax(wei, dim=-1)\nout = wei @ x\n\nout.shape\n# torch.Size([4, 8, 32])\n```",
    "frame_3864.jpg": "```python\n# version 4: self-attention\ntorch.manual_seed(1337)\nB, T, C = 4, 8, 32  # batch, time, channels\nx = torch.randn(B, T, C)\n\ntril = torch.tril(torch.ones(T, T))\nwei = torch.zeros((T, T))\nwei = wei.masked_fill(tril == 0, float('-inf'))\nwei = F.softmax(wei, dim=-1)\nout = wei @ x\n\nout.shape\n# torch.Size([4, 8, 32])\n```",
    "frame_3880.jpg": "```python\n# Version 4: self-attention!\ntorch.manual_seed(1337)\nB, T, C = 4, 8, 32  # batch, time, channels\nx = torch.randn(B, T, C)\n\ntri = torch.tril(torch.ones(T, T))\nwei = torch.zeros((T, T))\nwei = wei.masked_fill(tri == 0, float('-inf'))\nwei = F.softmax(wei, dim=-1)\n\nout = wei @ x\n\nout.shape\n```",
    "frame_3889.jpg": "```python\ntorch.manual_seed(1337)\nB, T, C = 4, 8, 32  # batch, time, channels\nx = torch.randn(B, T, C)\n\ntril = torch.tril(torch.ones(T, T))\nwei = torch.zeros((T, T))\nwei = wei.masked_fill(tril == 0, float('-inf'))\nwei = F.softmax(wei, dim=-1)\nout = wei @ x\n\nout.shape\ntorch.Size([4, 8, 32])\n```",
    "frame_3898.jpg": "Here is the extracted developer code from the image:\n\n```python\n# version 4: self-attention!\ntorch.manual_seed(1337)\nB,T,C = 4,8,32  # batch, time, channels\nx = torch.randn(B,T,C)\n\ntril = torch.tril(torch.ones(T,T))\nwei = torch.zeros((T,T))\nwei = wei.masked_fill(tril == 0, float('-inf'))\nwei = F.softmax(wei, dim=-1)\nout = wei @ x\n\nout.shape\n```\n\nLet me know if you need anything else!",
    "frame_3900.jpg": "```python\n# version 4: self-attention!\ntorch.manual_seed(1337)\nB,T,C = 4,8,32  # batch, time, channels\nx = torch.randn(B,T,C)\n\ntril = torch.tril(torch.ones(T, T))\nwei = torch.zeros((T,T))\nwei = wei.masked_fill(tril == 0, float('-inf'))\nwei = F.softmax(wei, dim=-1)\nout = wei @ x\n\nout.shape\n\ntorch.Size([4, 8, 32])\n```",
    "frame_3907.jpg": "```python\n# version 4: self-attention!\ntorch.manual_seed(1337)\nB, T, C = 4, 8, 32  # batch, time, channels\nx = torch.randn(B, T, C)\n\n# let's see a single Head perform self-attention\nhead_size = 16\nkey = nn.Linear(C, head_size, bias=False)\nquery = nn.Linear(C, head_size, bias=False)\n\ntril = torch.tril(torch.ones(T, T))\nwei = torch.zeros((T, T))\nwei = wei.masked_fill(tril == 0, float('-inf'))\nwei = F.softmax(wei, dim=-1)\nout = wei @ x\n\nout.shape\n```\n",
    "frame_3924.jpg": "```python\n# version 4: self-attention!\ntorch.manual_seed(1337)\nB,T,C = 4,8,32 # batch, time, channels\nx = torch.randn(B,T,C)\n\n# let's see a single Head perform self-attention\nhead_size = 16\nkey = nn.Linear(C, head_size, bias=False)\nquery = nn.Linear(C, head_size, bias=False)\n\ntril = torch.tril(torch.ones(T, T))\nwei = torch.zeros(T, T)\nwei = wei.masked_fill(tril == 0, float('-inf'))\nwei = F.softmax(wei, dim=-1)\nout = wei @ x\n\nout.shape\ntorch.Size([4, 8, 32])\n```\n",
    "frame_3946.jpg": "```python\n# version 4: self-attention!\ntorch.manual_seed(1337)\nB, T, C = 4, 8, 32  # batch, time, channels\nx = torch.randn(B, T, C)\n\n# let's see a single Head perform self-attention\nhead_size = 16\nkey = nn.Linear(C, head_size, bias=False)\nquery = nn.Linear(C, head_size, bias=False)\nk = key(x)  # (B, T, 16)\nq = query(x)  # (B, T, 16)\n\ntril = torch.tril(torch.ones(T, T))\nwei = torch.zeros((T, T))\nwei = wei.masked_fill(tril == 0, float('-inf'))\nwei = F.softmax(wei, dim=-1)\nout = wei @ x\n\nout.shape\n```",
    "frame_3959.jpg": "```python\n# version 4: self-attention!\ntorch.manual_seed(1337)\nB, T, C = 4, 48, 32  # batch, time, channels\nx = torch.randn(B, T, C)\n\n# let's see a single Head perform self-attention\nhead_size = 16\nkey = nn.Linear(C, head_size, bias=False)\nquery = nn.Linear(C, head_size, bias=False)\nk = key(x)  # (B, T, 16)\nq = query(x)  # (B, T, 16)\n\ntri1 = torch.tril(torch.ones(T, T))\nwei = torch.zeros(T, T)\nwei = wei.masked_fill(tri1 == 0, float('-inf'))\nwei = F.softmax(wei, dim=-1)\nout = wei @ x\n\nout.shape\ntorch.Size([4, 8, 32])\n```",
    "frame_3973.jpg": "```python\n# version 4: self-attention!\ntorch.manual_seed(1337)\nB, T, C = 4, 8, 32 # batch, time, channels\nx = torch.randn(B, T, C)\n\n# let's see a single Head perform self-attention\nhead_size = 16\nkey = nn.Linear(C, head_size, bias=False)\nquery = nn.Linear(C, head_size, bias=False)\nk = key(x) # (B, T, 16)\nq = query(x) # (B, T, 16)\nwei = ...\n\ntri = torch.tril(torch.ones(T, T))\nwei = torch.zeros((T, T))\nwei = wei.masked_fill(tri == 0, float('-inf'))\nwei = F.softmax(wei, dim=-1)\nout = wei @ x\n\nout.shape\n```",
    "frame_3978.jpg": "```python\n# version 4: self-attention!\ntorch.manual_seed(1337)\nB,T,C = 4,8,32  # batch, time, channels\nx = torch.randn(B,T,C)\n\n# let's see a single Head perform self-attention\nhead_size = 16\nkey = nn.Linear(C, head_size, bias=False)\nquery = nn.Linear(C, head_size, bias=False)\nk = key(x)  # (B, T, 16)\nq = query(x)  # (B, T, 16)\nwei = q @ k\n\ntril = torch.tril(torch.ones(T, T))\nwei = torch.zeros((T,T))\nwei = wei.masked_fill(tril == 0, float('-inf'))\nwei = F.softmax(wei, dim=-1)\nout = wei @ x\n\nout.shape\ntorch.Size([4, 8, 32])\n```",
    "frame_4022.jpg": "```python\n# version 4: self-attention!\ntorch.manual_seed(1337)\nB,T,C = 4,8,32 # batch, time, channels\nx = torch.randn(B,T,C)\n\n# let's see a single Head perform self-attention\nhead_size = 16\nkey = nn.Linear(C, head_size, bias=False)\nquery = nn.Linear(C, head_size, bias=False)\nk = key(x) # (B, T, 16)\nq = query(x) # (B, T, 16)\nwei = q @ k.transpose(-2, -1) # (B, T, 16) @ (B, 16, T) ----> (B, T, T)\n\ntril = torch.tril(torch.ones(T, T))\nwei = torch.zeros((T,T))\nwei = wei.masked_fill(tril == 0, float('-inf'))\nwei = F.softmax(wei, dim=-1)\nout = wei @ x\n\nout.shape\n```",
    "frame_4043.jpg": "```python\ntorch.manual_seed(1337)\nB, T, C = 4, 8, 32  # batch, time, channels\nx = torch.randn(B, T, C)\n\n# let's see a single Head perform self-attention\nhead_size = 16\nkey = nn.Linear(C, head_size, bias=False)\nquery = nn.Linear(C, head_size, bias=False)\nk = key(x)  # (B, T, 16)\nq = query(x) # (B, T, 16)\nwei = q @ k.transpose(-1, -2)  # (B, T, 16) @ (B, 16, T) ---> (B, T, T)\n\ntri1 = torch.tril(torch.ones(T, T))\n# wei = torch.zeros((T, T))\nwei = wei.masked_fill(tri1 == 0, float('-inf'))\nwei = F.softmax(wei, dim=-1)\n\nout = wei @ x\n\nout.shape\n\ntorch.Size([4, 8, 32])\n```\n\nOutput tensor:\n```\nwei\ntensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])\n```",
    "frame_4046.jpg": "```python\nx = torch.randn(B, T, C)\n# let's see a single Head perform self-attention\nhead_size = 16\nkey = nn.Linear(C, head_size, bias=False)\nquery = nn.Linear(C, head_size, bias=False)\nk = key(x)  # (B, T, 16)\nq = query(x) # (B, T, 16)\nq = q @ k.transpose(-2, -1) # (B, 16, T) @ (B, T, T) ---> (B, T, T)\n\ntri1 = torch.tril(torch.ones(T, T))\n# wei = torch.zeros((T,T))\nwei = wei.masked_fill(tri1 == 0, float('-inf'))\nwei = F.softmax(wei, dim=-1)\nout = wei @ x\n\nout.shape\ntorch.Size([4, 8, 32])\n```",
    "frame_4048.jpg": "Code NA",
    "frame_4049.jpg": "The image contains the following code:\n\n```python\ntensor([[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n          [0.1574, 0.6266, 0.1889, 0.0469, 0.2155, 0.1691],\n          ...\n          [0.0210, 0.0843, 0.0555, 0.0000]]],\n        ...\n          [[0.0522, 0.0517, 0.0961, 0.0333, 0.1409, 0.1414]]])\ngrad_fn=<SoftmaxBackward>\n```\n\nIf you need any further help or details, let me know!",
    "frame_4050.jpg": "```python\nout.shape\ntorch.Size([4, 8, 32])\nwei = tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n              [0.1574, 0.8426, 0.2266, 0.6266, 0.5792, 0.1187, 0.1889, 0.1131, 0.0469, 0.0276, 0.7999, 0.0000],\n              [0.0215, 0.0009, 0.6812, 0.0019, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n              ...\n              [0.3650, 0.0474, 0.8767, 0.2019, 0.0000, 0.0000, 0.0000, 0.0000]])\n\n# Note: The full tensor data is not fully captured.\n```\n",
    "frame_4051.jpg": "```python\nwei = torch.zeros((T, T))\nwei = wei.masked_fill(tril == 0, float('-inf'))\nwei = F.softmax(wei, dim=-1)\nout = wei @ x\n\nout.shape\ntorch.Size([4, 8, 32])\n```",
    "frame_4054.jpg": "```python\ntril = torch.tril(torch.ones(T, T))\n#wei = torch.zeros((T,T))\nwei = wei.masked_fill(tril == 0, float('-inf'))\nwei = F.softmax(wei, dim=-1)\nout.shape\n\ntorch.Size([4, 8, 32])\n\nwei\ntensor([[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0001],\n          [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n          ...\n          [0.0210, 0.0843, 0.0555, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n         ...\n         [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0001],\n          ...\n          [0.8359, 0.0416, 0.0525, 0.0119, 0.0000, 0.0000, 0.0000, 0.0000]]])\n```",
    "frame_4062.jpg": "```python\nwei = torch.zeros((T, T))\nwei = wei.masked_fill(tril == 0, float('-inf'))\nwei = F.softmax(wei, dim=-1)\nout = wei @ x\nout.shape\ntorch.Size([4, 8, 32])\n```\n\n```python\nwei = tensor([[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n                [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n                [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n                ...\n                [0.0210, 0.2403, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n              ...\n              [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n                [0.1687, 0.8313, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n                ...\n                [0.0355, 0.0215, 0.0580, 0.2119, 0.0000, 0.0000, 0.0000, 0.0000]]])\n```",
    "frame_4063.jpg": "```python\nq = query(x)  # (B, T, 16)\nwei = q @ k.transpose(-2, -1)  # (B, 16, T) @ (B, T, T) ----> (B, T, T)\n# wei = torch.zeros((T, T))\nwei = wei.masked_fill(tril == 0, float('-inf'))\nwei = F.softmax(wei, dim=-1)\nout = wei @ x\n\nout.shape\ntorch.Size([4, 8, 32])\n```",
    "frame_4064.jpg": "```python\n# let's see a single Head perform self-attention\nhead_size = 16\n\nkey = nn.Linear(C, head_size, bias=False)\nquery = nn.Linear(head_size, bias=False)\nk = key(x)\nq = query(x)  # (B, T, 16)\nwei = q @ k.transpose(-2, -1)  # (B, T, 16) @ (B, 16, T) ---> (B, T, T)\n\ntri1 = torch.tril(torch.ones(T, T))\n# wei = torch.zeros((T, T))\nwei = wei.masked_fill(tri1 == 0, float('-inf'))\nwei = F.softmax(wei, dim=-1)\nout = wei @ x\n\nout.shape\ntorch.Size([4, 8, 32])\n```",
    "frame_4067.jpg": "Code NA",
    "frame_4068.jpg": "```python\nout.shape\ntorch.Size([4, 8, 32])\nwei[0]\ntensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n[0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n[0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000],\n[0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000],\n[0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000],\n[0.0176, 0.2690, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000],\n[0.1691, 0.4060, 0.0438, 0.0416, 1.0488, 0.2012, 0.0329, 0.0000],\n[0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]])\ngrad_fn=<SelectBackward>\n```",
    "frame_4069.jpg": "```python\nkey = nn.Linear(C, head_size, bias=False)\nquery = nn.Linear(C, head_size, bias=False)\nk = key(x)  # (B, T, 16)\nq = query(x)  # (B, T, 16)\nwei = q @ k.transpose(-2, -1)  # (B, T, T)\ntrl = torch.tril(torch.ones(T, T))\n# wei = torch.zeros((T, T))\nwei = wei.masked_fill(trl == 0, float('-inf'))\nwei = F.softmax(wei, dim=-1)\nout = wei @ x\n\nout.shape\ntorch.Size([4, 8, 32])\nwei[0]\ntensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.5792, 0.1710, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.2104, 0.1052, 0.0495, 0.2779, 0.7999, 0.0000, 0.0000, 0.0000],\n        [0.0425, 0.0089, 0.6812, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.0210, 0.0043, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]])\n```\n",
    "frame_4078.jpg": "```python\nB, T, C = 4, 8, 32  # batch, time, channels\nx = torch.randn(B, T, C)\n\n# let's see a single Head perform self-attention\nhead_size = 16\nkey = nn.Linear(C, head_size, bias=False)\nquery = nn.Linear(C, head_size, bias=False)\nk = key(x)  # (B, T, 16)\nq = query(x)  # (B, T, 16)\nwei = q @ k.transpose(-2, -1)  # (B, T, 16) @ (B, 16, T) ----> (B, T, T)\n\ntri1 = torch.tril(torch.ones(T, T))\n# wei = torch.zeros((T, T))\nwei = wei.masked_fill(tri1 == 0, float('-inf'))\nwei = F.softmax(wei, dim=-1)\nout = wei @ x\n\nout.shape\n\ntorch.Size([4, 8, 32])\n\nwei[0]\ntensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.1574, 0.0180, 0.6266, 0.2267, 0.0183, 0.0000],\n        [0.5792, 0.1106, 0.1131, 0.0000, 0.0000, 0.0000],\n        [0.1691, 0.4066, 0.0380, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000, 0.0000],\n        [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.239]])\n```",
    "frame_4090.jpg": "```python\nB,T,C = 4,8,32 # batch, time, channels\nx = torch.randn(B,T,C)\n\n# let's see a single Head perform self-attention\nhead_size = 16\nkey = nn.Linear(C, head_size, bias=False)\nquery = nn.Linear(C, head_size, bias=False)\nk = key(x) # (B, T, 16)\nq = query(x) # (B, T, 16)\nwei = q @ k.transpose(-2, -1) # (B, T, 16) @ (B, 16, T) ---> (B, T, T)\n\ntril = torch.tril(torch.ones(T, T))\n# wei = torch.zeros((T, T))\nwei = wei.masked_fill(tril == 0, float('-inf'))\nout = wei @ x\n\nout.shape\n\ntorch.Size([4, 8, 32])\nwei[0]\ntensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.1574, 0.0000, 0.6266, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.2088, 0.1466, 0.6266, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.5792, 0.1012, 0.1889, 0.6812, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.6919, 0.1052, 0.1495, 0.6812, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.1691, 0.0463, 0.0510, 0.7179, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391],\n        [0.0000]])\n```",
    "frame_4102.jpg": "```python\nB, T, C = 4, 8, 32  # batch, time, channels\nx = torch.randn(B, T, C)\n\n# let's see a single Head perform self-attention\nhead_size = 16\nkey = nn.Linear(C, head_size, bias=False)\nquery = nn.Linear(C, head_size, bias=False)\nk = key(x)  # (B, T, 16)\nq = query(x)  # (B, T, 16)\nwei = q @ k.transpose(-2, -1)  # (B, T, 16) @ (B, 16, T) ----> (B, T, T)\n\ntri1 = torch.tril(torch.ones(T, T))\n# wei = torch.zeros((T, T))\nwei = wei.masked_fill(tri1 == 0, float('-inf'))\nout = wei.softmax(dim=-1) @ x\n\nout.shape\n\ntorch.Size([4, 8, 32])\n\nwei[0]\ntensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.1574, 0.8446, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.2088, 0.6126, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.5792, 0.1020, 0.1889, 0.2110, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.4873, 0.0672, 0.0276, 0.0790, 0.6812, 0.0000, 0.0000, 0.0000],\n        [0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000, 0.0000, 0.0000],\n        [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]])\n```",
    "frame_4106.jpg": "```python\nB, T, C = 4, 8, 32  # batch, time, channels\nx = torch.randn(B, T, C)\n\n# let's see a single Head perform self-attention\nhead_size = 16\nkey = nn.Linear(C, head_size, bias=False)\nquery = nn.Linear(C, head_size, bias=False)\nk = key(x)  # (B, T, 16)\nq = query(x)  # (B, T, 16)\nwei = q @ k.transpose(-2, -1)  # (B, T, T)\n\ntri1 = torch.tril(torch.ones(T, T))\n# wei = torch.zeros((T, T))\nwei = wei.masked_fill(tri1 == 0, float('-inf'))\nwei = F.softmax(wei, dim=-1)\nout = wei @ x\n\nout.shape\n\ntorch.Size([4, 8, 32])\n\nwei[0]\ntensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.1574, 0.0412, 0.6266, 0.2188, 0.0329, 0.0000, 0.0000, 0.0000],\n        [0.2088, 0.1674, 0.2646, 0.1891, 0.0111, 0.0000, 0.0000, 0.0000],\n        [0.5792, 0.1102, 0.1050, 0.0250, 0.0276, 0.0793, 0.0000, 0.0000],\n        [0.0436, 0.0191, 0.6812, 0.0190, 0.0385, 0.0416, 0.1048, 0.2012],\n        [0.0329, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]])\n```",
    "frame_4113.jpg": "```python\nB, T, C = 4, 8, 32  # batch, time, channels\nx = torch.randn(B, T, C)\n\n# let's see a single Head perform self-attention\nhead_size = 16\nkey = nn.Linear(C, head_size, bias=False)\nquery = nn.Linear(C, head_size, bias=False)\nk = key(x)  # (B, T, 16)\nq = query(x)  # (B, T, 16)\nwei = q @ k.transpose(-2, -1)  # (B, T, 16) @ (B, 16, T) ----> (B, T, T)\n\ntri1 = torch.tril(torch.ones(T, T))\n# wei = torch.zeros((T, T))\nwei = wei.masked_fill(tril == 0, float('-inf'))\nwei = F.softmax(wei, dim=-1)\nout = wei @ x\n\nout.shape\n\ntorch.Size([4, 8, 32])\nwei[0]\ntensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.1574, 0.0486, 0.6266, 0.2766, 0.0272, 0.0000, 0.0000, 0.0000],\n        [0.2088, 0.1464, 0.6266, 0.1889, 0.0111, 0.0000, 0.0000, 0.0000],\n        [0.5792, 0.1025, 0.1890, 0.1412, 0.0263, 0.0000, 0.0000, 0.0000],\n        [0.0376, 0.2770, 0.1052, 0.0419, 0.0058, 0.0000, 0.0000, 0.0000],\n        [0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000, 0.0000, 0.0000],\n        [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]])\n```",
    "frame_4116.jpg": "```\nB,T,C = 4,8,32  # batch, time, channels\nx = torch.randn(B, T, C)\n\n# let's see a single Head perform self-attention\nhead_size = 16\nkey = nn.Linear(C, head_size, bias=False)\nquery = nn.Linear(C, head_size, bias=False)\nk = key(x)  # (B, T, 16)\nq = query(x)  # (B, T, 16)\n\nwei = q @ k.transpose(-2, -1)  # (B, T, T)\n\ntri = torch.tril(torch.ones(T, T))\n# wei = torch.zeros((T, T))\nwei = wei.masked_fill(tri == 0, float('-inf'))\nout = wei @ x\n\nout.shape\n\ntorch.Size([4, 8, 32])\n\nwei[0]\ntensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.1574, 0.0486, 0.6266, 0.2642, 0.1899, 0.1414, 0.1899, 0.1136],\n        [0.5792, 0.1052, 0.6266, 0.6266, 0.2779, 0.2963, 0.2311, 0.1010],\n        ...\n        [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]])\n```",
    "frame_4129.jpg": "```python\nB, T, C = 4, 8, 32  # batch, time, channels\nx = torch.randn(B, T, C)\n\n# let's see a single Head perform self-attention\nhead_size = 16\nkey = nn.Linear(C, head_size, bias=False)\nquery = nn.Linear(C, head_size, bias=False)\nk = key(x)  # (B, T, 16)\nq = query(x)  # (B, T, 16)\n\nwei = q @ k.transpose(-2, -1)  # (B, T, 16) @ (B, 16, T) ----> (B, T, T)\n\ntril = torch.tril(torch.ones(T, T))\n# wei = torch.zeros((T, T))\nwei = wei.masked_fill(tril == 0, float('-inf'))\nwei = F.softmax(wei, dim=-1)\nout = wei @ x\n\nout.shape\ntorch.Size([4, 8, 32])\n\nwei[0]\ntensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.1574, 0.0000, 0.6266, 0.2260, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.2088, 0.1466, 0.2196, 0.1889, 0.0113, 0.0000, 0.0000, 0.0000],\n        [0.5792, 0.1872, 0.0276, 0.7961, 0.1816, 0.0000, 0.0000, 0.0000],\n        [0.0151, 0.0000, 0.6812, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000, 0.0000, 0.0000],\n        [0.0210, 0.0834, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]])\n```",
    "frame_4135.jpg": "```python\nB, T, C = 4, 8, 32  # batch, time, channels\nx = torch.randn(B, T, C)\n\n# let's see a single Head perform self-attention\nhead_size = 16\nkey = nn.Linear(C, head_size, bias=False)\nquery = nn.Linear(C, head_size, bias=False)\nk = key(x)  # (B, T, 16)\nq = query(x)  # (B, T, 16)\nwei = q @ k.transpose(-2, -1)  # (B, T, T)\n\ntri_l = torch.tril(torch.ones(T, T))\n# wei = torch.zeros((T, T))\nwei = wei.masked_fill(tri_l == 0, float('-inf'))\nwei = F.softmax(wei, dim=-1)\nout = wei @ x\n\nout.shape\n\ntorch.Size([4, 8, 32])\n\nwei[0]\ntensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.2088, 0.1466, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.5792, 0.1819, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.0978, 0.2776, 0.7095, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.0000, 0.0000, 0.0000, 0.6812, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000, 0.0000, 0.0000],\n        [0.0210, 0.0834, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]])\n```",
    "frame_4145.jpg": "```python\nB,T,C = 4,8,32  # batch, time, channels\nx = torch.randn(B,T,C)\n\n# let's see a single Head perform self-attention\nhead_size = 16\nkey = nn.Linear(C, head_size, bias=False)\nquery = nn.Linear(C, head_size, bias=False)\nk = key(x)  # (B, T, 16)\nq = query(x)  # (B, T, 16)\nwei = q @ k.transpose(-1, -2)  # (B, T, 16) @ (B, 16, T) ----> (B, T, T)\n\ntri1 = torch.tril(torch.ones(T, T))\n# wei = torch.zeros((T, T))\n# wei = wei.masked_fill(tril == 0, float('-inf'))\n# wei = F.softmax(wei, dim=-1)\nout = wei @ x\n\nout.shape\n\ntorch.Size([4, 8, 32])\nwei[0]\n```\n",
    "frame_4164.jpg": "```python\ntorch.manual_seed(1337)\nB, T, C = 4, 8, 32  # batch, time, channels\nx = torch.randn(B, T, C)\n\n# let's see a single Head perform self-attention\nhead_size = 16\nkey = nn.Linear(C, head_size, bias=False)\nquery = nn.Linear(C, head_size, bias=False)\nk = key(x)  # (B, T, 16)\nwei = q @ k.transpose(-2, -1)  # (B, T, 16) @ (B, 16, T) ----> (B, T, T)\n\ntri1 = torch.tril(torch.ones(T, T))\nwei = torch.zeros((T, T))\n# wei = wei.masked_fill(tri1 == 0, float('-inf'))\n# wei = F.softmax(wei, dim=-1)\nout = wei @ x\n\nout.shape\ntorch.Size([4, 8, 32])\n```",
    "frame_4174.jpg": "Here's the developer code extracted from the image:\n\n```python\nx = torch.randn(B, T, C)\n# let's see a single Head perform self-attention\nhead_size = 16\nkey = nn.Linear(C, head_size, bias=False)\nquery = nn.Linear(C, head_size, bias=False)\nk = key(x)  # (B, T, 16)\nwei = q @ k.transpose(-2, -1)  # (B, T, 16) @ (B, 16, T) ----> (B, T, T)\n\ntri = torch.tril(torch.ones(T, T))\n# wei = wei.zeros((T, T))\nwei = wei.masked_fill(tril == 0, float('-inf'))\nwei = F.softmax(wei, dim=-1)\nout = wei @ x\n\nout.shape\n\ntorch.Size([4, 8, 32])\nwei[0]\ntensor([[-1.7629,  -inf,  -inf,  -inf,  -inf,  -inf,  -inf,  -inf],\n        [-3.3334, -1.6556,  -inf,  -inf,  -inf,  -inf,  -inf,  -inf],\n        [-1.0226, -1.2606,  0.0762,  -inf,  -inf,  -inf,  -inf,  -inf],\n        [-1.7836, -0.4104,  0.3368, -0.8496,  -inf,  -inf,  -inf,  -inf],\n        [-1.2566,  0.4127, -1.3204,  2.0363, -2.5229,  -inf,  -inf,  -inf],\n        [1.8076,  1.9652,  0.2612,  0.3158,  0.6091,  1.2616, -0.5484,  -inf],\n        [-1.8044, -0.4126, -0.8306,  0.5899, -0.7987, -0.5856,  0.6433, 0.6303]])\n```\n\nIf you need further assistance, feel free to ask!",
    "frame_4179.jpg": "```python\ntorch.manual_seed(1337)\nB, T, C = 4, 32  # batch, time, channels\nx = torch.randn(B, T, C)\n\n# let's see a single Head perform self-attention\nhead_size = 16\nkey = nn.Linear(C, head_size, bias=False)\nquery = nn.Linear(C, head_size, bias=False)\nk = key(x)\nq = query(x)\nwei = q @ k.transpose(-2, -1)  # (B, T, 16) @ (B, 16, T) ---> (B, T, T)\n\ntri = torch.tril(torch.ones(T, T))\n# wei = torch.zeros((T, T))\nwei = wei.masked_fill(tri == 0, float('-inf'))\n# wei = F.softmax(wei, dim=-1)\nout = wei @ x\n\nout.shape\n```\n\n```plaintext\ntorch.Size([4, 8, 32])\n``` \n\n```plaintext\nwei[0]\ntensor([[-1.7629,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n        [-3.3334, -1.6556, -1.6971, -1.6223,    -inf,    -inf,    -inf,    -inf],\n        [-1.0226, -1.2606,  0.0762, -0.3380, -0.7830,    -inf,    -inf,    -inf],\n        ...\n        [ 0.5899, -0.7987, -0.5856,  0.6433,  0.6303]])\n```",
    "frame_4182.jpg": "```python\ntorch.manual_seed(1337)\nB, T, C = 4, 32, 8  # batch, time, channels\nx = torch.rand(B, T, C)\n\n# let's see a single Head perform self-attention\nhead_size = 16\nkey = nn.Linear(C, head_size, bias=False)\nquery = nn.Linear(head_size, bias=False)\nk = key(x)\nq = query(x)\n\nwei = q @ k.transpose(-1)  # (B, T, 16) @ (B, 16, T) ----> (B, T, T)\n\ntri1 = torch.tril(torch.ones(T, T))\n# wei = torch.zeros((T, T))\nwei = wei.masked_fill(tri1 == 0, float('-inf'))\nwei = F.softmax(wei, dim=-1)\nout = wei @ x\n\nout.shape\n\ntorch.Size([4, 8, 32])\n```",
    "frame_4189.jpg": "```python\ntorch.manual_seed(1337)\nB, T, C = 4, 8, 32  # batch, time, channels\nx = torch.randn(B, T, C)\n\n# let's see a single Head perform self-attention\nhead_size = 16\nkey = nn.Linear(C, head_size, bias=False)\nquery = nn.Linear(C, head_size, bias=False)\nk = key(x)\nq = query(x)\nwei = q @ k.transpose(-2, -1)  # (B, T, 16) @ (B, 16, T) ----> (B, T, T)\n\ntri = torch.tril(torch.ones(T, T))\n# wei = torch.zeros((T, T))\nwei = wei.masked_fill(tri == 0, float('-inf'))\nwei = F.softmax(wei, dim=-1)\nout = wei @ x\n\nout.shape\n\ntorch.Size([4, 8, 32])\nwei[0]\ntensor([[ -1.7629,     -inf,     -inf,     -inf,     -inf,     -inf,     -inf,     -inf],\n        [ -3.3343,  -1.6556,  0.0762,     -inf,     -inf,     -inf,     -inf,     -inf],\n        [ -1.0226,  -1.2606,  0.0762,     -inf,     -inf,     -inf,     -inf,     -inf],\n        [  0.7834,  -0.0110, -0.3368,  -0.8496,     -inf,     -inf,     -inf,     -inf],\n        [ -1.2566,  -1.7870, -1.3204,  2.0363,     -inf,     -inf,     -inf,     -inf],\n        [  1.0876,   0.4216,  0.2010, -2.9011,  -2.5229,     -inf,     -inf,     -inf],\n        [ -1.8044,  -2.4162, -0.8306,  0.5897, -0.7987,  0.5856,  0.6433,  0.6301]])\n```",
    "frame_4191.jpg": "```python\nx = torch.randn(B,T,C)\n\n# let's see a single Head perform self-attention\nhead_size = 16\nkey = nn.Linear(C, head_size, bias=False)\nquery = nn.Linear(C, head_size, bias=False)\nk = key(x)  # (B, T, 16)\nq = query(x) # (B, T, 16)\nwei = q @ k.transpose(-2, -1)  # (B, T, 16) @ (B, 16, T) ---> (B, T, T)\n\ntri = torch.tril(torch.ones(T, T))\n# wei = torch.zeros((T,T))\nwei = wei.masked_fill(tri == 0, float('-inf'))\nwei = F.softmax(wei, dim=-1)\nout = wei @ x\n\nout.shape\n\ntorch.Size([4, 8, 32])\nwei[0]\ntensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.2088, 0.6116, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.5792, 0.1121, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.0469, 0.0276, 0.7990, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.0624, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.2010, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]])\n```",
    "frame_4198.jpg": "```python\nB, T, C = 4, 8, 32  # batch, time, channels\nx = torch.randn(B, T, C)\n\n# let's see a single Head perform self-attention\nhead_size = 16\nkey = nn.Linear(C, head_size, bias=False)\nquery = nn.Linear(C, head_size, bias=False)\nk = key(x)  # (B, T, head_size)\nq = query(x)  # (B, T, head_size)\nwei = q @ k.transpose(-2, -1)  # (B, T, head_size) @ (B, head_size, T) --> (B, T, T)\n\ntri1 = torch.tril(torch.ones(T, T))\n# wei = torch.zeros((T, T))\nwei = wei.masked_fill(tri1 == 0, float('-inf'))\nwei = F.softmax(wei, dim=-1)\nout = wei @ x\n\nout.shape\n\ntorch.Size([4, 8, 32])\nwei[0]\n```",
    "frame_4205.jpg": "```python\ntorch.manual_seed(1337)\nB,T,C = 4,8,32 # batch, time, channels\nkey = nn.Linear(C, head_size, bias=False)\nquery = nn.Linear(C, head_size, bias=False)\nk = key(x)  # (B, T, 16)\nq = query(x)\nwei = q @ k.transpose(-2, -1)  # (B, T, 16) @ (B, T, 16) ---> (B, T, T)\ntril = torch.tril(torch.ones(T, T))\n# wei = torch.zeros((T,T))\nwei = wei.masked_fill(tril == 0, float('-inf'))\nwei = F.softmax(wei, dim=-1)\nout = wei @ x\nout.shape\n\ntorch.Size([4, 8, 32])\nwei[0]\ntensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.2088, 0.6166, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.5791, 0.1721, 0.1889, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.0496, 0.2776, 0.0416, 0.0000, 0.2012, 0.0329, 0.0000, 0.0000],\n        [0.0215, 0.0193, 0.6812, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.0123, 0.0410, 0.0416, 0.0000, 0.2012, 0.0329, 0.0000, 0.0000],\n        [0.0010, 0.0043, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]])\n```",
    "frame_4208.jpg": "```python\n# Version 4: self-attention!\ntorch.manual_seed(1337)\nB, T, C = 4, 32, 8 # batch, time, channels\nx = torch.randn(B, T, C)\n\n# let's see a single Head perform self-attention\nhead_size = 16\nkey = nn.Linear(C, head_size, bias=False)\nquery = nn.Linear(C, head_size, bias=False)\nk = key(x) # (B, T, 16)\nq = query(x) # (B, T, 16)\nwei = q @ k.transpose(-2, -1) # (B, T, 16) @ (B, 16, T) ---> (B, T, T)\n\ntril = torch.tril(torch.ones(T, T))\nwei = wei.masked_fill(tril == 0, float('-inf'))\nwei = F.softmax(wei, dim=-1)\nout = wei @ x\n\nout.shape\n``` \n\nThis includes the relevant developer code extracted from the image.",
    "frame_4231.jpg": "```python\n# Version 4: self-attention!\ntorch.manual_seed(1337)\nB, T, C = 8, 32, 8 # batch, time, channels\nx = torch.randn(B, T, C)\n\n# let's see a single Head perform self-attention\nhead_size = 16\nkey = nn.Linear(C, head_size, bias=False)\nquery = nn.Linear(C, head_size, bias=False)\nvalue = nn.Linear(C, head_size, bias=False)\nk = key(x) # (B, T, head_size)\nq = query(x) # (B, T, head_size)\nwei = q @ k.transpose(-1, -2) # (B, T, 16) @ (B, 16, T) ---> (B, T, T)\n\ntri = torch.tril(torch.ones(T, T))\n#wei = torch.zeros((T, T))\nwei = wei.masked_fill(tri == 0, float('-inf'))\nwei = F.softmax(wei, dim=-1)\n\nout = wei @ x\nout.shape\ntorch.Size([4, 8, 32])\n```",
    "frame_4249.jpg": "```python\n# Version 4: self-attention!\ntorch.manual_seed(1337)\nB,T,C = 4,8,32 # batch, time, channels\nx = torch.randn(B,T,C)\n\n# let's see a single Head perform self-attention\nhead_size = 16\nkey = nn.Linear(C, head_size, bias=False)\nquery = nn.Linear(C, head_size, bias=False)\nvalue = nn.Linear(C, head_size, bias=False)\nk = key(x) # (B, T, 16)\nq = query(x) # (B, T, 16)\nwei = q @ k.transpose(-2, -1) # (B, T, 16) @ (B, 16, T) ----> (B, T, T)\n\ntri = torch.tril(torch.ones(T, T))\nwei = torch.zeros((T,T))\nwei = wei.masked_fill(tri == 0, float('-inf'))\nwei = F.softmax(wei, dim=-1)\n\nv = value(x)\nout = wei @ v\n# out = wei @ x\n\nout.shape\n```\n",
    "frame_4251.jpg": "```python\nkey = nn.Linear(c, head_size, bias=False)\nquery = nn.Linear(c, head_size, bias=False)\nvalue = nn.Linear(c, head_size, bias=False)\nk = key(x)  # (B, T, 16)\nq = query(x)  # (B, T, 16)\nwei = q @ k.transpose(-2, -1)  # (B, T, 16) @ (B, 16, T) ---> (B, T, T)\n\ntril = torch.tril(torch.ones(T, T))\nwei = torch.zeros((T, T))\nwei = wei.masked_fill(tril == 0, float('-inf'))\nwei = F.softmax(wei, dim=-1)\n\nv = value(x)\nout = wei @ v\n# out = wei @ x\n\nout.shape\ntorch.Size([4, 8, 16])\n```",
    "frame_4254.jpg": "```python\n# let's see a single Head perform self-attention\nhead_size = 16\n\nkey = nn.Linear(C, head_size, bias=False)\nquery = nn.Linear(C, head_size, bias=False)\nvalue = nn.Linear(C, head_size, bias=False)\n\nk = key(x)  # (B, T, 16)\nq = query(x) \nq = q @ q.transpose(-2, -1)  # (B, T, 16) @ (B, 16, T) --> (B, T, T)\n\ntri1 = torch.tril(torch.ones(T, T))\n# wei = torch.zeros((T, T))\nwei = wei.masked_fill(tri1 == 0, float('-inf'))\nwei = F.softmax(wei, dim=-1)\n\nv = value(x)\nout = wei @ v\n# out = wei @ x\n\nout.shape\ntorch.Size([4, 8, 16])\n\nwei[0]\ntensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.2088, 0.6166, 0.1752, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.5792, 0.1710, 0.1504, 0.0993, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.6035, 0.2286, 0.1688, 0.0979, 0.0012, 0.0000, 0.0000, 0.0000],\n        [0.7345, 0.2001, 0.0638, 0.0016, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.8800, 0.1043, 0.0155, 0.0002, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.9105, 0.0695, 0.0200, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]])\n```\n",
    "frame_4267.jpg": "```\nB, T, C = 4, 8, 32  # batch, time, channels\nx = torch.randn(B, T, C)\n\n# let's see a single Head perform self-attention\nhead_size = 16\nkey = nn.Linear(C, head_size, bias=False)\nquery = nn.Linear(C, head_size, bias=False)\nvalue = nn.Linear(C, head_size, bias=False)\nk = key(x)\nq = query(x)\nwei = q @ k.transpose(-1, -2)  # (B, T, 16) @ (B, 16, T) ----> (B, T, T)\n\ntri1 = torch.tril(torch.ones(T, T))\n# wei = torch.zeros((T, T))\nwei = wei.masked_fill(tri1 == 0, float('-inf'))\nwei = F.softmax(wei, dim=-1)\n\nv = value(x)\nout = wei @ v\nout.shape\n```",
    "frame_4295.jpg": "```python\n# version 4: self-attention!\ntorch.manual_seed(1337)\nB,T,C = 4,8,32  # batch, time, channels\nx = torch.randn(B,T,C)\n\n# let's see a single Head perform self-attention\nhead_size = 16\nkey = nn.Linear(C, head_size, bias=False)\nquery = nn.Linear(C, head_size, bias=False)\nvalue = nn.Linear(C, head_size, bias=False)\nk = key(x)  # (B, T, 16)\nq = query(x)  # (B, T, 16)\nwei = q @ k.transpose(-2, -1)  # (B, 16, T) @ (B, T, 16) --> (B, 16, T)\n\ntri = torch.tril(torch.ones(T, T))\n#wei = torch.zeros((T,T))\nwei = wei.masked_fill(tri == 0, float('-inf'))\nwei = F.softmax(wei, dim=-1)\n\nv = value(x)\nout = wei @ v\n#out = wei @ x\n\nout.shape\n\ntorch.Size([4, 8, 16])\n```\n",
    "frame_4316.jpg": "```python\n# version 4: self-attention!\ntorch.manual_seed(1337)\nB, T, C = 4, 8, 32  # batch, time, channels\nx = torch.randn(B, T, C)\n\n# let's see a single Head perform self-attention\nhead_size = 16\nkey = nn.Linear(C, head_size, bias=False)\nquery = nn.Linear(C, head_size, bias=False)\nvalue = nn.Linear(C, head_size, bias=False)\nk = key(x)\nq = query(x)\nwei = q.k.transpose(-2, -1)  # (B, 16, T) @ (B, T, 16) ---> (B, T, T)\n\ntri1 = torch.tril(torch.ones(T, T))\n# wei = torch.zeros((T, T))\nwei = wei.masked_fill(wei == 0, float('-inf'))\nwei = F.softmax(wei, dim=-1)\n\nv = value(x)\nout = wei @ v\n# out = wei @ x\n\nout.shape\ntorch.Size([4, 8, 16])\n```",
    "frame_4324.jpg": "```python\n# version 4: self-attention!\ntorch.manual_seed(1337)\nB,T,C = 4,8,32 # batch, time, channels\nx = torch.randn(B,T,C)\n\n# let's see a single Head perform self-attention\nhead_size = 16\nkey = nn.Linear(C, head_size, bias=False)\nquery = nn.Linear(C, head_size, bias=False)\nvalue = nn.Linear(C, head_size, bias=False)\nk = key(x) # (B, T, 16)\nq = query(x) # (B, T, 16)\nwei = q @ k.transpose(-2, -1) # (B, 16) @ (B, 16, T) ----> (B, T, T)\ntri1 = torch.tril(torch.ones(T, T))\n#wei = torch.zeros((T,T))\nwei = wei.masked_fill(wei == 0, float('-inf'))\nwei = F.softmax(wei, dim=-1)\n\nv = value(x)\nout = wei @ v\n#out = wei @ x\n\nout.shape\ntorch.Size([4, 8, 16])\n```",
    "frame_4327.jpg": "```python\n# version 4: self-attention!\ntorch.manual_seed(1337)\nB, T, C = 4, 8, 32  # batch, time, channels\nx = torch.randn(B, T, C)\n\n# let's see a single Head perform self-attention\nhead_size = 16\nkey = nn.Linear(C, head_size, bias=False)\nquery = nn.Linear(C, head_size, bias=False)\nvalue = nn.Linear(C, head_size, bias=False)\nk = key(x)\nq = query(x)\nwei = q @ k.transpose(-2, -1)  # (B, T, 16) @ (B, 16, T) ---> (B, T, T)\nwei = torch.tril(torch.ones(T, T))\n# wei = torch.zeros((T, T))\nwei = wei.masked_fill(wei == 0, float('-inf'))\nwei = F.softmax(wei, dim=-1)\n\nv = value(x)\nout = wei @ v\n# out = wei @ x\n\nout.shape\ntorch.Size([4, 8, 16])\n```",
    "frame_4343.jpg": "```python\n# version 4: self-attention!\ntorch.manual_seed(1337)\nB, T, C = 4, 8, 32  # batch, time, channels\nx = torch.randn(B, T, C)\n\n# let's see a single Head perform self-attention\nhead_size = 16\nkey = nn.Linear(C, head_size, bias=False)\nquery = nn.Linear(C, head_size, bias=False)\nvalue = nn.Linear(C, head_size, bias=False)\nk = key(x)\nq = query(x)\nwei = q @ k.transpose(-1, -2)  # (B, T, 16) @ (B, 16, T) ----> (B, T, T)\n\ntri1 = torch.tril(torch.ones(T, T))\nwei = torch.zeros((T, T))\nwei = wei.masked_fill(tri1 == 0, float('-inf'))\nwei = F.softmax(wei, dim=-1)\n\nv = value(x)\nout = wei @ v\n# out = wei @ x\n\nout.shape\ntorch.Size([4, 8, 16])\n```",
    "frame_4348.jpg": "```python\n# version 4: self-attention!\ntorch.manual_seed(1337)\nB,T,C = 4,8,32 # batch, time, channels\nx = torch.randn(B,T,C)\n\n# let's see a single Head perform self-attention\nhead_size = 16\nkey = nn.Linear(C, head_size, bias=False)\nquery = nn.Linear(C, head_size, bias=False)\nvalue = nn.Linear(C, head_size, bias=False)\n\nk = key(x) # (B, T, 16)\nq = query(x) # (B, T, 16)\nwei = q @ k.transpose(-2, -1) # (B, T, 16) @ (B, 16, T) ----> (B, T, T)\nwei = torch.tril(torch.ones(T, T))\nwei = torch.zeros((T,T))\nwei = wei.masked_fill(wei == 0, float('-inf'))\nwei = F.softmax(wei, dim=-1)\n\nv = value(x)\nout = wei @ v\n# out = wei @ x\n\nout.shape\ntorch.Size([4, 8, 16])\n```",
    "frame_4352.jpg": "```python\n# version 4: self-attention!\ntorch.manual_seed(1337)\nB,T,C = 4,8,32  # batch, time, channels\nx = torch.randn(B,T,C)\n\n# let's see a single Head perform self-attention\nhead_size = 16\nkey = nn.Linear(C, head_size, bias=False)\nquery = nn.Linear(C, head_size, bias=False)\nvalue = nn.Linear(C, head_size, bias=False)\nk = key(x)\nq = query(x)\nwei = q @ k.transpose(-2, -1)  # (B, T, 16) @ (B, 16, T) ---> (B, T, T)\ntri1 = torch.tril(torch.ones(T, T))\n# wei = torch.zeros((T,T))\nwei = wei.masked_fill(wei == 0, float('-inf'))\nwei = F.softmax(wei, dim=-1)\n\nv = value(x)\nout = wei @ v\n# out = wei @ x\n\nout.shape\ntorch.Size([4, 8, 16])\n```",
    "frame_4355.jpg": "```python\n# version 4: self-attention!\ntorch.manual_seed(1337)\nB, T, C = 4, 8, 32 # batch, time, channels\nx = torch.randn(B, T, C)\n\n# let's see a single Head perform self-attention\nhead_size = 16\nkey = nn.Linear(C, head_size, bias=False)\nquery = nn.Linear(C, head_size, bias=False)\nvalue = nn.Linear(C, head_size, bias=False)\nk = key(x)\nq = query(x)\nwei = q @ k.transpose(-1, -2) # (B, T, 16) @ (B, 16, T) ---> (B, T, T)\n\ntri1 = torch.tril(torch.ones(T, T))\n# wei = torch.zeros((T, T))\nwei = wei.masked_fill(tri1 == 0, float('-inf'))\nwei = F.softmax(wei, dim=-1)\n\nv = value(x)\nout = wei @ v\n# out = wei @ x\n\nout.shape\ntorch.Size([4, 8, 16])\n```",
    "frame_4360.jpg": "```python\n# version 4: self-attention!\ntorch.manual_seed(1337)\nB, T, C = 4, 8, 32  # batch, time, channels\nx = torch.randn(B, T, C)\n\n# let's see a single Head perform self-attention\nhead_size = 16\nkey = nn.Linear(C, head_size, bias=False)\nquery = nn.Linear(C, head_size, bias=False)\nvalue = nn.Linear(C, head_size, bias=False)\nk = key(x)\nq = query(x)\nwei = q @ k.transpose(-2, -1)  # (B, T, 16) @ (B, 16, T) ---> (B, T, T)\n\ntri1 = torch.tril(torch.ones(T, T))\n# wei = torch.zeros((T, T))\nwei = wei.masked_fill(wei == 0, float('-inf'))\nwei = F.softmax(wei, dim=-1)\n\nv = value(x)\nout = wei @ v\n# out = wei @ x\n\nout.shape\n\ntorch.Size([4, 8, 16])\n```",
    "frame_4368.jpg": "```python\n# version 4: self-attention!\ntorch.manual_seed(1337)\nB,T,C = 4,0,32 # batch, time, channels\nx = torch.randn(B, T, C)\n\n# let's see a single Head perform self-attention\nhead_size = 16\nkey = nn.Linear(C, head_size, bias=False)\nquery = nn.Linear(C, head_size, bias=False)\nvalue = nn.Linear(C, head_size, bias=False)\nk = key(x)  # (B, T, 16)\nq = query(x) # (B, T, 16) Q (B, 16, T)\nwei = q @ k.transpose(-2, -1) # (B, T, 16) @ (B, 16, T) --> (B, T, T)\n\ntri = torch.tril(torch.ones((T, T)))\n# wei = torch.zeros((T, T))\nwei = wei.masked_fill(tri == 0, float('-inf'))\nv = value(x)\nout = wei @ v\n# out = wei @ x\n\nout.shape\ntorch.Size([4, 8, 16])\n```",
    "frame_4371.jpg": "```python\n# version 4: self-attention!\ntorch.manual_seed(1337)\nB,T,C = 4,8,32  # batch, time, channels\nx = torch.randn(B, T, C)\n\n# let's see a single Head perform self-attention\nhead_size = 16\nkey = nn.Linear(C, head_size, bias=False)\nquery = nn.Linear(C, head_size, bias=False)\nvalue = nn.Linear(C, head_size, bias=False)\nk = key(x)  # (B, T, 16)\nq = query(x) # (B, T, 16) @ (B, T, 16) ---> (B, T, T)\nwei = q @ k.transpose(-2, -1)  # (B, T, T)\n\ntri = torch.tril(torch.ones((T, T)))\n# wei = torch.zeros((T,T))\nwei = wei.masked_fill(wei == 0, float('-inf'))\nv = value(x)\nout = wei @ v\n# out = wei @ x\n\nout.shape\ntorch.Size([4, 8, 16])\n```",
    "frame_4376.jpg": "```python\n# version 4: self-attention!\ntorch.manual_seed(1337)\nB,T,C = 4,0,32  # batch, time, channels\nx = torch.randn(B,T,C)\n\n# let's see a single Head perform self-attention\nhead_size = 16\nkey = nn.Linear(C, head_size, bias=False)\nquery = nn.Linear(C, head_size, bias=False)\nvalue = nn.Linear(C, head_size, bias=False)\nk = key(x)  # (B, T, 16)\nq = query(x) # (B, T, 16) @ (B, 16, T) ----> (B, T, T)\nwei = q @ k.transpose(-2, -1) # (B, T, T)\n\ntri1 = torch.tril(torch.ones((T, T)))\n# wei = torch.zeros((T,T))\nwei = wei.masked_fill(tri1 == 0, float('-inf'))\nwei = F.softmax(wei, dim=-1)\n\nv = value(x)\nout = wei @ v\n# out = wei @ x\n\nout.shape\ntorch.Size([4, 8, 16])\n```",
    "frame_4378.jpg": "```python\n# version 4: self-attention!\ntorch.manual_seed(1337)\nB,T,C = 4, 8, 32  # batch, time, channels\nx = torch.rand(B, T, C)\n\n# let's see a single Head perform self-attention\nhead_size = 16\nkey = nn.Linear(C, head_size, bias=False)\nquery = nn.Linear(C, head_size, bias=False)\nvalue = nn.Linear(C, head_size, bias=False)\nk = key(x)  # (B, T, 16)\nq = query(x) # (B, T, 16) @ (B, 16, T) ---> (B, T, T)\nwei = q @ k.transpose(-2, -1) \n\ntri = torch.tril(torch.ones((T, T)))\n#wei = torch.zeros((T, T))\nwei = wei.masked_fill(tri == 0, float('-inf'))\nwei = F.softmax(wei, dim=-1)\n\nv = value(x)\nout = wei @ v\n#out = wei @ x\n\nout.shape\ntorch.Size([4, 8, 16])\n```",
    "frame_4388.jpg": "```python\n# version 4: self-attention!\ntorch.manual_seed(1337)\nB, T, C = 4, 32, 8 # batch, time, channels\nx = torch.randn(B, T, C)\n\n# let's see a single Head perform self-attention\nhead_size = 16\nkey = nn.Linear(C, head_size, bias=False)\nquery = nn.Linear(C, head_size, bias=False)\nvalue = nn.Linear(C, head_size, bias=False)\nk = key(x)  # (B, T, 16)\nq = query(x)  # (B, T, 16) @ (B, 16, T) ---> (B, T, T)\nwei = q @ k.transpose(-2, -1)  # (B, T, T)\n\ntri1 = torch.tril(torch.ones((T, T)))\n# wei = torch.zeros((T, T))\nwei = wei.masked_fill(tri1 == 0, float('-inf'))\nv = value(x)\nout = wei @ v\n# out = wei @ x\n\nout.shape\ntorch.Size([4, 8, 16])\n```",
    "frame_4391.jpg": "```python\n# version 4: self-attention!\ntorch.manual_seed(1337)\nB,T,C = 4,0,32  # batch, time, channels\nx = torch.randn(B,T,C)\n\n# let's see a single Head perform self-attention\nhead_size = 16\nkey = nn.Linear(C, head_size, bias=False)\nquery = nn.Linear(head_size, bias=False)\nvalue = nn.Linear(C, head_size, bias=False)\nk = key(x)  # (B, T, 16)\nq = query(x) # (B, T, 16) @ (B, 16, T) ---> (B, T, T)\nwei = q @ k.transpose(-2, -1) \n\ntril = torch.tril(torch.ones((T, T)))\n# wei = torch.zeros((T, T))\nwei = wei.masked_fill(tril == 0, float('-inf'))\nwei = F.softmax(wei, dim=-1)\n\nv = value(x)\nout = wei @ v\n# out = wei @ x\n\nout.shape\ntorch.Size([4, 8, 16])\n```",
    "frame_4398.jpg": "```python\n# version 4: self-attention!\ntorch.manual_seed(1337)\nB,T,C = 4,0,32  # batch, time, channels\nx = torch.randn(B, T, C)\n\n# let's see a single Head perform self-attention\nhead_size = 16\nkey = nn.Linear(C, head_size, bias=False)\nquery = nn.Linear(C, head_size, bias=False)\nvalue = nn.Linear(C, head_size, bias=False)\nk = key(x)  # (B, T, 16)\nq = query(x) # (B, T, 16) @ (B, T, 16) ---> (B, T, T)\n\nwei = q @ k.transpose(-2, -1)  # (B, T, 16) @ (B, 16, T)\ntril = torch.tril(torch.ones((T, T)))\nwei = wei.masked_fill(tril == 0, float('-inf'))\nwei = F.softmax(wei, dim=-1)\n\nv = value(x)\nout = wei @ v\n#out = wei @ x\n\nout.shape\ntorch.Size([4, 8, 16])\n```",
    "frame_4405.jpg": "```python\n# version 4: self-attention!\ntorch.manual_seed(1337)\nB,T,C = 4,0,32  # batch, time, channels\nx = torch.randn(B,T,C)\n\n# let's see a single Head perform self-attention\nhead_size = 16\nkey = nn.Linear(C, head_size, bias=False)\nquery = nn.Linear(head_size, bias=False)\nvalue = nn.Linear(head_size, bias=False)\nk = key(x)  # (B, T, 16)\nq = query(x)  # (B, T, 16)\nwei = q @ k.transpose(-2, -1)  # (B, T, 16) @ (B, 16, T) ---> (B, T, T)\n\ntri = torch.tril(torch.ones(T, T))\n# wei = torch.zeros((T,T))\nwei = wei.masked_fill(tri == 0, float('-inf'))\nv = value(x)\nout = wei @ v\n# out = wei @ x\n\nout.shape\ntorch.Size([4, 8, 16])\n```\n",
    "frame_4408.jpg": "```python\n# version 4: self-attention!\ntorch.manual_seed(1337)\nB,T,C = 4,0,32  # batch, time, channels\nx = torch.randn(B,T,C)\n\n# let's see a single Head perform self-attention\nhead_size = 16\nkey = nn.Linear(C, head_size, bias=False)\nquery = nn.Linear(head_size, bias=False)\nvalue = nn.Linear(head_size, bias=False)\nk = key(x)  # (B, T, 16)\nq = query(x) # (B, T, 16) @ (B, T, 16) ---> (B, T, T)\nwei = q @ k.transpose(-2, -1)  # (B, T, T)\n\ntri1 = torch.tril(torch.ones((T, T)))\n# wei = torch.zeros((T,T))\nwei = wei.masked_fill(tri1 == 0, float('-inf'))\nwei = F.softmax(wei, dim=-1)\n\nv = value(x)\nout = wei @ v\n# out = wei @ x\n\nout.shape\ntorch.Size([4, 8, 16])\n```",
    "frame_4435.jpg": "Here is the extracted developer code from the image:\n\n```python\nk = key(x)  # (B, T, 16)\nq = query(x) \nwei = q @ k.transpose(-2, -1)  # (B, T, 16) @ (B, 16, T) ---> (B, T, T)\n\ntri = torch.tril(torch.ones(T, T))\n# wei = torch.zeros((T,T))\nwei = wei.masked_fill(tri == 0, float('-inf'))\nwei = F.softmax(wei, dim=-1)\n\nv = value(x)\nout = wei @ v\n# out = wei @ x\n\nout.shape\ntorch.Size([4, 8, 16])\n```\n\nIf you need further assistance, feel free to ask!",
    "frame_4450.jpg": "```python\nk = key(x)\nq = query(x)  # (B, T, 16)\nwei = q @ k.transpose(-2, -1)  # (B, T, 16) @ (B, 16, T) ---> (B, T, T)\n\ntri = torch.tril(torch.ones(T, T))\n# wei = torch.zeros((T, T))\nwei = wei.masked_fill(tri == 0, float('-inf'))\nwei = F.softmax(wei, dim=-1)\n\nv = value(x)\nout = wei @ v\n# out = wei @ x\n\nout.shape\n\ntorch.Size([4, 8, 16])\n```",
    "frame_4453.jpg": "```python\nk = key(x)\nq = query(x)  # (B, T, 16)\nwei = q @ k.transpose(-2, -1)  # (B, T, 16) @ (B, 16, T) ---> (B, T, T)\n\ntri1 = torch.tril(torch.ones(T, T))\n# wei = torch.zeros((T, T))\nwei = wei.masked_fill(tri1 == 0, float('-inf'))\nwei = F.softmax(wei, dim=-1)\n\nv = value(x)\nout = wei @ v\n# out = wei @ x\n\nout.shape\ntorch.Size([4, 8, 16])\n```",
    "frame_4462.jpg": "```\n# let's see a single Head perform self-attention\nhead_size = 16\n\nkey = nn.Linear(c, head_size, bias=False)\nquery = nn.Linear(c, head_size, bias=False)\nvalue = nn.Linear(c, head_size, bias=False)\n\nk = key(x)  # (B, T, 16)\nwei = q @ k.transpose(2, -1)  # (B, T, 16) @ (B, 16, T) ----> (B, T, T)\n\ntri = torch.tril(torch.ones(T, T))\n# wei = torch.zeros((T, T))\nwei = wei.masked_fill(tri == 0, float('-inf'))\nwei = F.softmax(wei, dim=-1)\n\nv = value(x)\nout = wei @ v\n# out = wei @ x\n\nout.shape\ntorch.Size([4, 8, 16])\n```",
    "frame_4472.jpg": "```python\n# let's see a single Head perform self-attention\nhead_size = 16\nkey = nn.Linear(c, head_size, bias=False)\nquery = nn.Linear(c, head_size, bias=False)\nvalue = nn.Linear(c, head_size, bias=False)\nk = key(x)  # (B, T, 16)\nwei = q @ k.transpose(-1, -2)  # (B, T, 16) @ (B, 16, T) ---> (B, T, T)\n\ntril = torch.tril(torch.ones(T, T))\n# wei = torch.zeros((T, T))\nwei = wei.masked_fill(tril == 0, float('-inf'))\nwei = F.softmax(wei, dim=-1)\n\nv = value(x)\nout = wei @ v\n# out = wei @ x\n\nout.shape\ntorch.Size([4, 8, 16])\n```",
    "frame_4484.jpg": "```python\n# let's see a single Head perform self-attention\nhead_size = 16\nkey = nn.Linear(C, head_size, bias=False)\nquery = nn.Linear(C, head_size, bias=False)\nvalue = nn.Linear(C, head_size, bias=False)\nk = key(x)  # (B, T, 16)\nq = query(x)  # (B, T, 16)\nwei = q @ k.transpose(-1, -2)  # (B, T, 16) @ (B, 16, T) --> (B, T, T)\n\ntril = torch.tril(torch.ones(T, T))\n# wei = torch.zeros((T, T))\nwei = wei.masked_fill(tril == 0, float('-inf'))\nwei = F.softmax(wei, dim=-1)\n\nv = value(x)\nout = wei @ v\n# out = wei @ x\n\nout.shape\ntorch.Size([4, 8, 16])\n```",
    "frame_4496.jpg": "```python\n# let's see a single Head perform self-attention\nhead_size = 16\n\nkey = nn.Linear(C, head_size, bias=False)\nquery = nn.Linear(C, head_size, bias=False)\nvalue = nn.Linear(C, head_size, bias=False)\n\nk = key(x)  # (B, T, 16)\nwei = q @ k.transpose(-2, -1)  # (B, T, 16) @ (B, 16, T) ---> (B, T, T)\n\ntri1 = torch.tril(torch.ones(T, T))\n# wei = torch.zeros((T,T))\nwei = wei.masked_fill(tri1 == 0, float('-inf'))\nwei = F.softmax(wei, dim=-1)\n\nv = value(x)\nout = wei @ v\n# out = wei @ x\n\nout.shape\ntorch.Size([4, 8, 16])\n```",
    "frame_4503.jpg": "```python\n# let's see a single Head perform self-attention\nhead_size = 16\n\nkey = nn.Linear(C, head_size, bias=False)\nquery = nn.Linear(C, head_size, bias=False)\nvalue = nn.Linear(C, head_size, bias=False)\n\nk = key(x)  # (B, T, 16)\nq = query(x)  # (B, T, 16)\nwei = q @ k.transpose(-2, -1)  # (B, T, T)\n\ntril = torch.tril(torch.ones((T, T)))  # Tensor: tril\nwei = torch.zeros((T, T))  # Tensor with shape torch.Size([8, 8])\nwei = wei.masked_fill(tril == 0, float('-inf'))\nwei = F.softmax(wei, dim=-1)\n\nv = value(x)\nout = wei @ v\n# out = wei @ x\n\nout.shape\n```",
    "frame_4507.jpg": "```python\nq = query(x)  # (B, T, 16)\nwei = q @ k.transpose(-2, -1)  # (B, T, 16) @ (B, 16, T) ---> (B, T, T)\n\ntril = torch.tril(torch.ones(T, T))\nwei = torch.zeros((T, T))\nwei = wei.masked_fill(tril == 0, float('-inf'))\nwei = F.softmax(wei, dim=-1)\n\nv = value(x)\nout = wei @ v\n# out = wei @ x\n\nout.shape\ntorch.Size([4, 8, 16])\n```",
    "frame_4510.jpg": "```python\n# let's see a single Head perform self-attention\nhead_size = 16\nkey = nn.Linear(c, head_size, bias=False)\nquery = nn.Linear(c, head_size, bias=False)\nvalue = nn.Linear(c, head_size, bias=False)\nk = key(x)  # (B, T, 16)\nq = query(x)  # (B, T, 16)\nwei = q @ k.transpose(-2, -1)  # (B, T, 16) @ (B, 16, T) ---> (B, T, T)\ntrl = torch.tril(torch.ones(T, T))\n# wei = torch.zeros((T,T))\nwei = wei.masked_fill(trl == 0, float('-inf'))\nwei = F.softmax(wei, dim=-1)\n\nv = value(x)\nout = wei @ v\n# out = wei @ x\n\nout.shape\n\ntorch.Size([4, 8, 16])\n```",
    "frame_4514.jpg": "```python\n# let's see a single Head perform self-attention\nhead_size = 16\n\nkey = nn.Linear(c, head_size, bias=False)\nquery = nn.Linear(c, head_size, bias=False)\nvalue = nn.Linear(c, head_size, bias=False)\n\nk = key(x)  # (B, T, 16)\nq = query(x)  # (B, T, 16)\nwei = q @ k.transpose(-2, -1)  # (B, T, 16) @ (B, 16, T) ---> (B, T, T)\n\ntri_l = torch.tril(torch.ones(T, T))\n# wei = torch.zeros((T, T))\nwei = wei.masked_fill(tri_l == 0, float('-inf'))\nwei = F.softmax(wei, dim=-1)\n\nv = value(x)\nout = wei @ v\n# out = wei @ x\n\nout.shape\n```\n",
    "frame_4522.jpg": "```python\n# let's see a single Head perform self-attention\nhead_size = 16\n\nkey = nn.Linear(c, head_size, bias=False)\nquery = nn.Linear(c, head_size, bias=False)\nvalue = nn.Linear(c, head_size, bias=False)\n\nk = key(x)  # (B, T, 16)\nq = query(x)  # (B, T, 16)\nwei = q @ k.transpose(-2, -1)  # (B, T, 16) @ (B, 16, T) ---> (B, T, T)\n\ntril = torch.tril(torch.ones(T, T))\n# wei = torch.zeros((T,T))\nwei = wei.masked_fill(tril == 0, float('-inf'))\nwei = F.softmax(wei, dim=-1)\n\nv = value(x)\nout = wei @ v\n# out = wei @ x\n\nout.shape\ntorch.Size([4, 8, 16])\n```",
    "frame_4525.jpg": "```python\n# let's see a single Head perform self-attention\nhead_size = 16\n\nkey = nn.Linear(C, head_size, bias=False)\nquery = nn.Linear(C, head_size, bias=False)\nvalue = nn.Linear(C, head_size, bias=False)\n\nk = key(x)  # (B, T, 16)\nq = query(x)  # (B, T, 16)\nwei = q @ k.transpose(-2, -1)  # (B, T, 16) @ (B, 16, T) ---> (B, T, T)\n\ntri1 = torch.tril(torch.ones(T, T))\n# wei = torch.zeros((T, T))\nwei = wei.masked_fill(tri1 == 0, float('-inf'))\nwei = F.softmax(wei, dim=-1)\n\nv = value(x)\nout = wei @ v\n# out = wei @ x\n\nout.shape\ntorch.Size([4, 8, 16])\n```",
    "frame_4544.jpg": "```python\nquery = nn.Linear(c, head_size, bias=False)\nvalue = nn.Linear(c, head_size, bias=False)\nkey = nn.Linear(c, head_size, bias=False)\n\nk = key(x)  # (B, T, 16)\nq = query(x)  # (B, T, 16)\nwei = q @ k.transpose(-2, -1)  # (B, T, 16) @ (B, 16, T) ----> (B, T, T)\n\ntri1 = torch.tril(torch.ones(T, T))\n# wei1 = torch.zeros((T, T))\nwei = wei.masked_fill(tri1 == 0, float('-inf'))\nwei = F.softmax(wei, dim=-1)\n\nv = value(x)\nout = wei @ v\n# out = wei @ x\n\nout.shape\ntorch.Size([4, 8, 16])\n```",
    "frame_4550.jpg": "```python\n# version 4: self-attention!\ntorch.manual_seed(1337)\nB,T,C = 4,8,32  # batch, time, channels\nx = torch.randn(B,T,C)\n\n# let's see a single Head perform self-attention\nhead_size = 16\nkey = nn.Linear(C, head_size, bias=False)\nquery = nn.Linear(C, head_size, bias=False)\nvalue = nn.Linear(C, head_size, bias=False)\nk = key(x)  # (B, T, 16)\nq = query(x)  # (B, T, 16)\nwei = q @ k.transpose(-2, -1)  # (B, 16, T) @ (B, T, 16) ---> (B, T, T)\n\ntri1 = torch.tril(torch.ones(T, T))\n# wei = torch.zeros((T,T))\nwei = wei.masked_fill(tri1 == 0, float('-inf'))\nwei = F.softmax(wei, dim=-1)\n\nv = value(x)\nout = wei @ v\n# out = wei @ x\n\nout.shape\ntorch.Size([4, 8, 16])\n```",
    "frame_4562.jpg": "```python\n# version 4: self-attention!\ntorch.manual_seed(1337)\nB,T,C = 4,8,32 # batch, time, channels\nx = torch.rand(B,T,C)\n\n# let's see a single Head perform self-attention\nhead_size = 16\nkey = nn.Linear(C, head_size, bias=False)\nquery = nn.Linear(C, head_size, bias=False)\nvalue = nn.Linear(C, head_size, bias=False)\nk = key(x) # (B, T, 16)\nwei = q @ k.transpose(-2, -1) # (B, T, 16) @ (B, 16, T) -----> (B, T, T)\n\ntri1 = torch.tril(torch.ones(T, T))\n# wei = torch.zeros((T,T))\nwei = wei.masked_fill(tri1 == 0, float('-inf'))\nwei = F.softmax(wei, dim=-1)\n\nv = value(x)\nout = wei @ v\n# out = wei @ x\n\nout.shape\ntorch.Size([4, 8, 16])\n```",
    "frame_4578.jpg": "```python\n# version 4: self-attention!\ntorch.manual_seed(1337)\nB,T,C = 4,8,32  # batch, time, channels\nx = torch.randn(B,T,C)\n\n# let's see a single Head perform self-attention\nhead_size = 16\nquery = nn.Linear(C, head_size, bias=False)\nkey = nn.Linear(C, head_size, bias=False)\nvalue = nn.Linear(C, head_size, bias=False)\nk = key(x)\nwei = q @ k.transpose(-2, -1)  # (B, T, 16) @ (B, 16, T) -----> (B, T, T)\n\ntri1 = torch.tril(torch.ones(T, T))\n# wei = torch.zeros((T,T))\nwei = wei.masked_fill(tri1 == 0, float('-inf'))\nwei = F.softmax(wei, dim=-1)\n\nv = value(x)\nout = wei @ v\n# out = wei @ x\nout.shape  # torch.Size([4, 8, 16])\n```",
    "frame_4583.jpg": "```python\n# version 4: self-attention!\ntorch.manual_seed(1337)\nB,T,C = 4,8,32 # batch, time, channels\nx = torch.randn(B,T,C)\n\n# let's see a single Head perform self-attention\nhead_size = 16\nkey = nn.Linear(C, head_size, bias=False)\nquery = nn.Linear(head_size, bias=False)\nvalue = nn.Linear(C, head_size, bias=False)\nk = key(x) # (B, T, 16)\nwei = q @ k.transpose(-2, -1) # (B, T, 16) @ (B, 16, T) -----> (B, T, T)\n\ntril = torch.tril(torch.ones(T, T))\n# wei = torch.zeros((T,T))\nwei = wei.masked_fill(tril == 0, float('-inf'))\nwei = F.softmax(wei, dim=-1)\n\nv = value(x)\nout = wei @ v\n# out = wei @ x\n\nout.shape\n\ntorch.Size([4, 8, 16])\n```",
    "frame_4592.jpg": "```python\n# version 4: self-attention!\ntorch.manual_seed(1337)\nB,T,C = 4,8,32 # batch, time, channels\nx = torch.randn(B,T,C)\n\n# let's see a single Head perform self-attention\nhead_size = 16\n\nkey = nn.Linear(C, head_size, bias=False)\nquery = nn.Linear(C, head_size, bias=False)\nvalue = nn.Linear(C, head_size, bias=False)\nk = key(x)  # (B, T, 16)\nq = query(x) # (B, T, 16) @ (B, 16, T) ---> (B, T, T)\n\ntri1 = torch.tril(torch.ones(T, T))\n# wei = torch.zeros((T,T))\nwei = wei.masked_fill(tri1 == 0, float('-inf'))\nwei = F.softmax(wei, dim=-1)\n\nv = value(x)\nout = wei @ v\n# out = wei @ x\nout.shape\n\ntorch.Size([4, 8, 16])\n```",
    "frame_4599.jpg": "```python\n# version 4: self-attention!\ntorch.manual_seed(1337)\nB,T,C = 4,8,32 # batch, time, channels\nx = torch.randn(B,T,C)\n\n# let's see a single Head perform self-attention\nhead_size = 16\n\nquery = nn.Linear(C, head_size, bias=False)\nvalue = nn.Linear(C, head_size, bias=False)\nkey = nn.Linear(C, head_size, bias=False)\n\nk = key(x)\nwei = q @ k.transpose(-2, -1) # (B, T, 16) @ (B, 16, T) ----> (B, T, T)\n\ntri1 = torch.tril(torch.ones(T, T))\n# wei = torch.zeros((T,T))\nwei = wei.masked_fill(tril == 0, float('-inf'))\nwei = F.softmax(wei, dim=-1)\n\nv = value(x)\nout = wei @ v\n# out = wei @ x\nout.shape\n```",
    "frame_4603.jpg": "```python\n# version 4: self-attention!\ntorch.manual_seed(1337)\nB,T,C = 4,8,32 # batch, time, channels\nx = torch.randn(B,T,C)\n\n# let's see a single Head perform self-attention\nhead_size = 16\nquery = nn.Linear(C, head_size, bias=False)\nkey = nn.Linear(C, head_size, bias=False)\nvalue = nn.Linear(C, head_size, bias=False)\n\nk = key(x)\nwei = q @ k.transpose(-2, -1) # (B, T, 16) @ (B, 16, T) -----> (B, T, T)\n\ntri = torch.tril(torch.ones(T, T))\n# wei = torch.zeros((T,T))\nwei = wei.masked_fill(tri == 0, float('-inf'))\nwei = F.softmax(wei, dim=-1)\n\nv = value(x)\nout = wei @ v\n# out = wei @ x\n\nout.shape\n```\n",
    "frame_4605.jpg": "```python\n# version 4: self-attention!\ntorch.manual_seed(1337)\nB,T,C = 4,8,32 # batch, time, channels\nx = torch.randn(B,T,C)\n\n# let's see a single Head perform self-attention\nhead_size = 16\nquery = nn.Linear(C, head_size, bias=False)\nkey = nn.Linear(C, head_size, bias=False)\nvalue = nn.Linear(C, head_size, bias=False)\nk = key(x)\nwei = q @ k.transpose(-2, -1) # (B, T, 16) @ (B, 16, T) -----> (B, T, T)\n\ntri1 = torch.tril(torch.ones(T, T))\n# wei = torch.zeros((T,T))\nwei = wei.masked_fill(tril == 0, float('-inf'))\nwei = F.softmax(wei, dim=-1)\n\nv = value(x)\nout = wei @ v\n# out = wei @ x\n\nout.shape\n```",
    "frame_4609.jpg": "```python\n# version 4: self-attention!\ntorch.manual_seed(1337)\nB,T,C = 4,8,32  # batch, time, channels\nx = torch.randn(B,T,C)\n\n# let's see a single Head perform self-attention\nhead_size = 16\nkey = nn.Linear(C, head_size, bias=False)\nquery = nn.Linear(C, head_size, bias=False)\nvalue = nn.Linear(C, head_size, bias=False)\nk = key(x)  # (B, T, 16)\nq = query(x)  # (B, T, 16)\nwei = q @ k.transpose(-2, -1)  # (B, T, 16) @ (B, 16, T) ---> (B, T, T)\n\ntril = torch.tril(torch.ones(T, T))\n# wei = torch.zeros((T,T))\nwei = wei.masked_fill(tril == 0, float('-inf'))\nwei = F.softmax(wei, dim=-1)\n\nv = value(x)\nout = wei @ v\n# out = wei @ x\n\nout.shape\n```",
    "frame_4618.jpg": "Code NA",
    "frame_4640.jpg": "```\nk = torch.ranf(B,T,head_size)\nq = torch.ranf(B,T,head_size)\nwei = q @ k.transpose(-2, -1) * head_size**-0.5\n```",
    "frame_4644.jpg": "```python\nk = torch.randn(B,T,head_size)\nq = torch.randn(B,T,head_size)\nwei = q @ k.transpose(-2, -1) * head_size**0.5\ntorch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5]), dim=-1)\n```",
    "frame_4669.jpg": "```python\nk = torch.randn(B, T, head_size)\nq = torch.randn(B, T, head_size)\nwei = q @ k.transpose(-2, -1) * head_size**-0.5\nk.var()\ntensor(0.9006)\nq.var()\ntensor(17.4690)\ntorch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5]), dim=-1)\ntensor([0.1925, 0.1426, 0.2351, 0.1426, 0.2872])\n```",
    "frame_4673.jpg": "```python\ntorch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5]), dim=-1)\ntorch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5])*8, dim=-1)\n```",
    "frame_4674.jpg": "Here's the extracted code from the image:\n\n```python\nk = torch.randn(B, T, head_size)\nq = torch.randn(B, T, head_size)\nwei = q @ k.transpose(-2, -1) * head_size**-0.5\n\nk.var()\ntensor(0.9006)\n\nq.var()\ntensor(1.0037)\n\nwei.var()\ntensor(0.9957)\n\ntorch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5]), dim=-1)\ntensor([0.1925, 0.1426, 0.2351, 0.1426, 0.2872])\n\ntorch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5])*8, dim=-1)\ntensor([0.0326, 0.0030, 0.1615, 0.0030, 0.8000])\n```",
    "frame_4676.jpg": "```python\nhead_size = 16\n\nkey = nn.Linear(C, head_size, bias=False)\nquery = nn.Linear(C, head_size, bias=False)\nvalue = nn.Linear(C, head_size, bias=False)\n\nk = key(x)  # (B, T, 16)\nq = query(x)  # (B, T, 16)\n\nwei = q @ k.transpose(-1)  # (B, T, 16) @ (B, T, 16) ---> (B, T, T)\n\ntrl = torch.tril(torch.ones(T, T))\n# wei = torch.zeros((T,T))\nwei = wei.masked_fill(tril == 0, float('-inf'))\nwei = F.softmax(wei, dim=-1)\n\nv = value(x)\nout = wei @ v\n# out = wei @ x\n\nout.shape\ntorch.Size([4, 8, 16])\n```",
    "frame_4684.jpg": "```python\ntorch.manual_seed(1337)\n\nB, T, C = 4, 32 # batch, time, channels\nx = torch.randn(B, T, C)\n\n# let's see a single Head perform self-attention\nhead_size = 16\n\nkey = nn.Linear(C, head_size, bias=False)\nnn = nn.Linear(C, head_size, bias=False)\nvalue = nn.Linear(C, head_size, bias=False)\n\nk = key(x)  # (B, T, head_size)\nq = query(x)  # (B, T, head_size)\nwei = q @ k.transpose(-2, -1)  # (B, T, 16) @ (B, 16, T) ---> (B, T, T)\n\ntril = torch.tril(torch.ones(T, T))\nwei = torch.zeros((T, T))\nwei = wei.masked_fill(tril == 0, float('-inf'))\nwei = F.softmax(wei, dim=-1)\n\nv = value(x)\nout = wei @ v\nout.shape\n\ntorch.Size([4, 8, 16])\n```",
    "frame_4688.jpg": "```python\nq = query(x)  # (B, 1, T)\nwei = q @ k.transpose(-2, -1)  # (B, T, 16) q (B, 16, T) -----> (B, T, T)\n\ntri1 = torch.tril(torch.ones(T, T))\nwei = torch.zeros((T, T))\nwei = wei.masked_fill(tri1 == 0, float('-inf'))\nwei = F.softmax(wei, dim=-1)\n\nv = value(x)\nout = wei @ v\n# out = wei @ x\n\nout.shape\n\ntorch.Size([4, 8, 16])\n```",
    "frame_4689.jpg": "Here is the extracted developer code from the image:\n\n```python\nwei = q.k.transpose(-2, -1)  # (B, T, 16) Q (B, 16, T) ---> (B, T, T)\ntri = torch.tril(torch.ones(T, T))\n# wei = torch.zeros((T, T))\nwei = wei.masked_fill(tri == 0, float('-inf'))\nwei = F.softmax(wei, dim=-1)\n\nv = value(x)\nout = wei @ v\n# out = wei @ x\n\nout.shape\n\ntorch.Size([4, 8, 16])\nwei[]\n```",
    "frame_4690.jpg": "```python\ntril = torch.tril(torch.ones(T, T))\nwei = torch.zeros((T, T))\nwei = wei.masked_fill(wei == 0, float('-inf'))\nwei = F.softmax(wei, dim=-1)\n\nv = value(x)\nout = wei @ v\n# out = wei @ x\n\nout.shape\n\ntorch.Size([4, 8, 16])\n\nwei[0]\ntensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.2088, 0.1466, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.5792, 0.1710, 0.1889, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.0294, 0.1052, 0.0469, 0.0276, 0.7999, 0.0000, 0.0000, 0.0000],\n        [0.0215, 0.0839, 0.6812, 0.0019, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.1691, 0.4066, 0.0438, 0.0016, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.0210, 0.0843, 0.0555, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]])\n```",
    "frame_4698.jpg": "```python\nhead_size = 16\nkey = nn.Linear(C, head_size, bias=False)\nquery = nn.Linear(C, head_size, bias=False)\nvalue = nn.Linear(C, head_size, bias=False)\nk = key(x)  # (B, T, 16)\nq = query(x)  # (B, T, 16)\nwei = q @ k.transpose(-2, -1)  # (B, T, 16) @ (B, 16, T) ---> (B, T, T)\ntrl = torch.tril(torch.ones(T, T))\n# wei = torch.zeros((T, T))\nwei = wei.masked_fill(trl == 0, float('-inf'))\nwei = F.softmax(wei, dim=-1)\n\nv = value(x)\nout = wei @ v\n# out = wei @ x\n\nout.shape\ntorch.Size([4, 8, 16])\n```\n",
    "frame_4700.jpg": "```python\nhead_size = 16\nkey = nn.Linear(C, head_size, bias=False)\nquery = nn.Linear(C, head_size, bias=False)\nvalue = nn.Linear(C, head_size, bias=False)\nk = key(x)  # (B, T, 16)\nq = query(x) @ k.transpose(-1)  # (B, T, 16) @ (B, 16, T) ---> (B, T, T)\nwei = q @ k.transpose(2, -1) # (B, T, 16) @ (B, 16, T)\ntri = torch.tril(torch.ones(T, T))\nwei = torch.zeros((T, T))\nwei = wei.masked_fill(tri == 0, float('-inf'))\nwei = F.softmax(wei, dim=-1)\n\nv = value(x)\nout = wei @ v\nout.shape\n\ntorch.Size([4, 8, 16])\n\nwei[0]\n```",
    "frame_4706.jpg": "```\nk = torch.randn(B, T, head_size)\nq = torch.randn(B, T, head_size)\nwei = q @ k.transpose(-2, -1) * head_size**-0.5\nk.var()\nq.var()\nwei.var()\ntorch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5]), dim=-1)\ntorch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5])*8, dim=-1)\n```",
    "frame_4707.jpg": "```python\ntorch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5]), dim=-1)\ntorch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5])*8, dim=-1)\n```",
    "frame_4727.jpg": "```python\nwei = q @ k.transpose(-2, -1) * head_size ** -0.5\nk.var()\nq.var()\nwei.var()\ntorch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, -0.5]), dim=-1)\ntorch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5]) * 8, dim=-1)\n```",
    "frame_4734.jpg": "```python\nwei = q @ k.transpose(-2, -1) * head_size**-0.5\nk.var()\ntensor(0.9006)\nq.var()\ntensor(1.0037)\nwei.var()\ntensor(0.9957)\ntorch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5]), dim=-1)\ntensor([0.1925, 0.1426, 0.2351, 0.1426, 0.2872])\ntorch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5])*8, dim=-1)\ntensor([0.0326, 0.0030, 0.1615, 0.0030, 0.8000])\n```",
    "frame_4743.jpg": "```python\nwei = q.k.transpose(-2, -1) * head_size**-0.5\nk.var()\nq.var()\nwei.var()\ntorch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5]), dim=-1)\ntorch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5])*8, dim=-1)\n```",
    "frame_4748.jpg": "Here is the extracted code from the image:\n\n```python\nk = torch.randn(B, T, head_size)\nq = torch.randn(B, T, head_size)\nwei = q @ k.transpose(-2, -1) * head_size**-0.5\n\nk.var()\ntensor(0.9006)\n\nq.var()\ntensor(1.0037)\n\nwei.var()\ntensor(0.9557)\n\ntorch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5]), dim=-1)\ntensor([0.1925, 0.1426, 0.2351, 0.1426, 0.2872])\n\ntorch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5])*8, dim=-1)\ntensor([0.0326, 0.0030, 0.1615, 0.0030, 0.8000])\n```\n\nIf you need further assistance, let me know!",
    "frame_4753.jpg": "```python\n# version 4: self-attention!\ntorch.manual_seed(1337)\nB,T,C = 4,8,32  # batch, time, channels\nx = torch.randn(B,T,C)\n\n# let's see a single Head perform self-attention\nhead_size = 16\nkey = nn.Linear(C, head_size, bias=False)\nquery = nn.Linear(C, head_size, bias=False)\nvalue = nn.Linear(C, head_size, bias=False)\nk = key(x)  # (B, T, 16)\nq = query(x) # (B, T, 16) @ (B, 16, T) ---> (B, T, T)\n\ntri1 = torch.tril(torch.ones(T, T))\n# wei = torch.zeros((T, T))\nwei = wei.masked_fill(tril == 0, float('-inf'))\nwei = F.softmax(wei, dim=-1)\n\nv = value(x)\nout = wei @ v\nout.shape\n\ntorch.Size([4, 8, 16])\nwei[0]\ntensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000],\n        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000],\n        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000],\n        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000]])\n```",
    "frame_4756.jpg": "```python\n# version 4: self-attention!\ntorch.manual_seed(1337)\nB,T,C = 4,8,32  # batch, time, channels\nx = torch.randn(B,T,C)\n\n# let's see a single Head perform self-attention\nhead_size = 16\nkey = nn.Linear(C, head_size, bias=False)\nquery = nn.Linear(C, head_size, bias=False)\nvalue = nn.Linear(C, head_size, bias=False)\n\nk = key(x)  # (B, T, 16)\nq = query(x) # (B, T, 16)\nv = value(x)\n\nq = q.k.transpose(-2, -1)  # (B, 16, T) @ (B, T, 16) ---> (B, T, T)\n\ntri1 = torch.tril(torch.ones(T, T))\nwei = torch.zeros((T, T))\nwei = wei.masked_fill(tri1 == 0, float('-inf'))\nwei = F.softmax(wei, dim=-1)\n\nv = value(x)\nout = wei @ v\nout.shape\n\ntorch.Size([4, 8, 16])\n```",
    "frame_4758.jpg": "```python\nclass Head(nn.Module):\n    \"\"\" one head of self-attention \"\"\"\n\n    def __init__(self, head_size):\n        super().__init__()\n        self.key = nn.Linear(n_embd, head_size, bias=False)\n        self.query = nn.Linear(n_embd, head_size, bias=False)\n        self.value = nn.Linear(n_embd, head_size, bias=False)\n        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n\n    def forward(self, x):\n        B, T, C = x.shape\n        k = self.key(x)  # (B, T, C)\n        q = self.query(x)  # (B, T, C)\n        # compute attention scores (\"affinities\")\n        wei = q @ k.transpose(-2, -1) * C**-0.5  # (B, T, T)\n        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n        wei = F.softmax(wei, dim=-1)\n        # perform the weighted aggregation of the values\n        v = self.value(x)  # (B, T, C)\n        out = v @ wei  # (B, T, C)\n        return out\n\n# super simple bigram model\nclass BigramLanguageModel(nn.Module):\n```\n",
    "frame_4795.jpg": "```python\ndef __init__(self, head_size):\n    super().__init__()\n    self.key = nn.Linear(n_embd, head_size, bias=False)\n    self.query = nn.Linear(n_embd, head_size, bias=False)\n    self.value = nn.Linear(n_embd, head_size, bias=False)\n    self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n\ndef forward(self, x):\n    B, T, C = x.shape\n    k = self.key(x)  # (B, T, C)\n    q = self.query(x)  # (B, T, C)\n    # compute attention scores (\"affinities\")\n    wei = q @ k.transpose(-2, -1) * ... # (B, T, C) @ (B, C, T) -> (B, T, T)\n    wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n    wei = F.softmax(wei, dim=-1)  # perform the weighted aggregation of the values\n    out = wei @ self.value(x)  # (B, T, C) -> (B, T, C)\n    return out\n\nclass BigramLanguageModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        # each token directly reads off the logits for the next token from a lookup table\n        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n```",
    "frame_4817.jpg": "```python\nclass BigramLanguageModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        # each token directly reads off the logits for the next token from a lookup table\n        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n        self.sa_head = Head(n_embd)\n        self.lm_head = nn.Linear(n_embd, vocab_size)\n\n    def forward(self, idx, targets=None):\n        # idx and targets are both (B,T) tensor of integers\n        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n        pos_emb = self.position_embedding_table(torch.arange(device)) # (T,C)\n        x = tok_emb + pos_emb # (B,T,C)\n        x = self.sa_head(x) # apply one head of self-attention. (B,T,C)\n        logits = self.lm_head(x) # (B,T,vocab_size)\n\n        if targets is None:\n            loss = None\n        else:\n            ...\n```",
    "frame_4851.jpg": "```python\ndef forward(self, idx, targets=None):\n    B, T = idx.shape\n    tok_emb = self.token_embedding_table(idx)  # (B,T,C)\n    pos_emb = self.position_embedding_table(torch.arange(B, device=device))  # (T,C)\n    x = tok_emb + pos_emb  # apply one head of self-attention. (B,T,C)\n    logits = self.lm_head(x)\n\n    if targets is None:\n        loss = None\n    else:\n        B, T, C = logits.shape\n        logits = logits.view(B*T, C)\n        loss = F.cross_entropy(logits, targets)\n\n    return logits, loss\n\ndef generate(self, idx, max_new_tokens):\n    # idx is (B, T) array of indices in the current context\n    for _ in range(max_new_tokens):\n        # crop idx to the last block_size tokens\n        idx_cond = idx[:, -block_size:]\n        # get the predictions\n        logits, loss = self(idx_cond)\n        logits = logits[:, -1, :]  # becomes (B, C)\n        # apply softmax to get probabilities\n```",
    "frame_4852.jpg": "```python\ndef generate(self, idx, max_new_tokens):\n    # idx is (B, T) array of indices in the current context\n    for _ in range(max_new_tokens):\n        # crop idx to the last block_size tokens\n        idx_cond = idx[:, -block_size:]  \n        # get the predictions\n        logits, _ = self(idx_cond)     \n        # focus only on the last time step\n        logits = logits[:, -1, :]  # (B, C)\n        # apply softmax to get probabilities\n        probs = F.softmax(logits, dim=-1)  # (B, C)\n        # sample from the distribution\n        idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n        # append index to the running sequence\n        idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n    \n    return idx\n```",
    "frame_4863.jpg": "```python\npos_emb = self.position_embedding_table(torch.arange(T, device=device))  # (B, T, C)\nx = tok_emb + pos_emb  # apply one head of self-attention. (B, T, C)\nlogits = self.lm_head(x)  # (B, T, vocab_size)\n\nif targets is None:\n    loss = None\nelse:\n    B, T, C = logits.shape\n    logits = logits.view(B*T, C)\n    targets = targets.view(B*T)\n    loss = F.cross_entropy(logits, targets)\n\nreturn logits, loss\n\ndef generate(self, idx, max_new_tokens):\n    # idx is (B, T) array of indices in the current context\n    for _ in range(max_new_tokens):\n        # crop idx to the last block_size tokens\n        idx_cond = idx[:, -block_size:]\n        logits, loss = self(idx_cond)\n        # focus only on the last time step\n        logits = logits[:, -1, :]  # becomes (B, C)\n        probs = F.softmax(logits, dim=-1)  # (B, C)\n        idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n        # append sampled index to the running sequence\n        idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n    return idx\n```",
    "frame_4865.jpg": "```python\ndef generate(self, idx, max_new_tokens):\n    # idx is (B, T) array of indices in the current context\n    for _ in range(max_new_tokens):\n        # crop idx to the last block_size tokens\n        idx_cond = idx[:, -self.block_size:]\n        # get the predictions\n        logits, loss = self(idx_cond)\n        # focus only on the last time step\n        logits = logits[:, -1, :]  # becomes (B, C)\n        # apply softmax to get probabilities\n        probs = F.softmax(logits, dim=-1)  # (B, C)\n        # sample from the distribution\n        idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n        # append sampled index to the running sequence\n        idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n    return idx\n```",
    "frame_4867.jpg": "```python\ndef forward(self, idx, targets=None):\n    B, T = idx.shape\n    tok_emb = self.token_embedding_table(idx)  # (B, T, C)\n    pos_emb = self.position_embedding_table(torch.arange(B, device=device))  # (B, T, C)\n    x = self.sa_head(x)  # apply one head of self-attention. (B, T, vocab_size)\n\n    if targets is None:\n        loss = None\n    else:\n        B, T, C = logits.shape\n        logits = logits.view(B*T, C)\n        targets = targets.view(B*T)\n        loss = F.cross_entropy(logits, targets)\n\n    return logits, loss\n\ndef generate(self, idx, max_new_tokens):\n    # idx is (B, T) array of indices in the current context\n    for _ in range(max_new_tokens):\n        idx_cond = idx[:, -block_size:]\n        # get the predictions\n        logits, loss = self(idx_cond)\n        logits = logits[:, -1, :]  # becames (B, C)\n        # apply softmax to get probabilities\n```\n",
    "frame_4871.jpg": "```python\nclass BigramLanguageModel(nn.Module):\n\n    def __init__(self):\n        super().__init__()\n        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n        self.sa_head = Head(n_embd)\n        self.lm_head = nn.Linear(n_embd, vocab_size)\n\n    def forward(self, idx, targets=None):\n        B, T = idx.shape\n        tok_emb = self.token_embedding_table(idx)  # (B,T,C)\n        pos_emb = self.position_embedding_table(torch.arange(T, device=device))  # (T,C)\n        x = tok_emb + pos_emb  # (B,T,C)\n        x = self.sa_head(x)  # apply one head of self-attention. (B,T,C)\n        logits = self.lm_head(x)\n\n        if targets is None:\n            loss = None\n        else:\n            B, T, C = logits.shape\n            logits = logits.view(B*T, C)\n            targets = targets.view(B*T)\n            loss = F.cross_entropy(logits, targets)\n\n        return logits, loss\n```",
    "frame_4881.jpg": "```python\ndef generate(self, idx, max_new_tokens):\n    # idx is (B, T) array of indices in the current context\n    for _ in range(max_new_tokens):\n        # crop idx to the last block.size tokens\n        idx_cond = idx[:, -block_size:]\n        # get the predictions\n        logits, loss = self(idx_cond)\n        # focus only on the last time step\n        logits = logits[:, -1, :]  # becomes (B, C)\n        # apply softmax to get probabilities\n        probs = F.softmax(logits, dim=-1)  # (B, C)\n        # sample from the distribution\n        idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n        # append sampled index to the running sequence\n        idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n\n    return idx\n```",
    "frame_4887.jpg": "```python\npos_emb = self.position_embedding_table(torch.arange(T, device=device))  # (T,C)\nx = self.tok_emb + pos_emb  # apply one head of self-attention. (B,T,C)\nlogits = self.lm_head(x)\n\nif targets is None:\n    loss = None\nelse:\n    B, T, C = logits.shape\n    logits = logits.view(B*T, C)\n    targets = targets.view(B*T)\n    loss = F.cross_entropy(logits, targets)\n\nreturn logits, loss\n\ndef generate(self, idx, max_new_tokens):\n    # idx is (B, T) array of indices in the current context\n    for _ in range(max_new_tokens):\n        # crop idx to the last block_size tokens\n        idx_cond = idx[:, -block_size:]\n        # get the predictions\n        logits, loss = self(idx_cond)\n        # focus only on the last time step\n        logits = logits[:, -1, :]  # becomes (B, C)\n        probs = F.softmax(logits, dim=-1)  # (B, C)\n        # sample from the distribution\n        idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n        # append sampled index to the running sequence\n        idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n\n    return idx\n```",
    "frame_4889.jpg": "```python\nimport torch\nimport torch.nn as functional as F\n\n# hyperparameters\nbatch_size = 32  # how many independent sequences will we process in parallel?\nblock_size = 8  # what is the maximum context length for predictions?\nmax_iters = 5000\neval_interval = 500\nlearning_rate = 1e-3\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\neval_iters = 200\nn_embd = 32\n\ntorch.manual_seed(1337)\n\n# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\nwith open('input.txt', 'r', encoding='utf-8') as f:\n    text = f.read()\n\n# here are all the unique characters that occur in this text\nchars = sorted(list(set(text)))\nvocab_size = len(chars)\n\n# create a mapping from characters to integers\nstoi = {ch: i for i, ch in enumerate(chars)}\nitos = {i: ch for i, ch in enumerate(chars)}\nencode = lambda s: [stoi[c] for c in s]  # encoder: take a string, output a list of integers\ndecode = lambda l: ''.join([itos[i] for i in l])  # decoder: take a list of integers, output a string\n```",
    "frame_4910.jpg": "```python\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\n\n# hyperparameters\nbatch_size = 32  # how many independent sequences will we process in parallel?\nblock_size = 8  # what is the maximum context length for predictions?\nmax_iters = 5000\neval_interval = 500\nlearning_rate = 1e-3\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\neval_iters = 200\nn_embd = 32\n\ntorch.manual_seed(1337)\n\n# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\nwith open('input.txt', 'r', encoding='utf-8') as f:\n    text = f.read()\n\n# here are all the unique characters that occur in this text\nchars = sorted(list(set(text)))\nvocab_size = len(chars)\n\n# create a mapping from characters to integers\nstoi = {c: i for i, c in enumerate(chars)}\nitos = {i: c for i, c in enumerate(chars)}\nencode = lambda s: [stoi[c] for c in s]  # encoder: take a string, output a list of integers\ndecode = lambda l: ''.join([itos[i] for i in l])  # decoder: take a list of integers, output a string\n```",
    "frame_4917.jpg": "```\n# create a mapping from characters to integers\nitos = {i: ch for i, ch in enumerate(chars)}\nencode = lambda s: [itos[c] for c in s]  # encoder: take a string, output a list of integers\ndecode = lambda l: ''.join([itos[i] for i in l])  # decoder: take a list of integers, output a string\n\n# Train and test splits\ndata = torch.tensor(encode(text), dtype=torch.long)\nn = int(0.9 * len(data))  # first 90% will be train, rest val\ntrain_data = data[:n]\nval_data = data[n:]\n\n# data loading\ndef get_batch(split):\n    # generate a small batch of data of inputs x and targets y\n    data = train_data if split == 'train' else val_data\n    ix = torch.randint(len(data) - block_size, (batch_size,))\n    x = torch.stack([data[i:i + block_size] for i in ix])\n    y = torch.stack([data[i + 1:i + block_size + 1] for i in ix])\n    return x, y\n\n@torch.no_grad()\ndef estimate_loss():\n    out = {}\n    model.eval()\n    for split in ['train', 'val']:\n        losses = torch.zeros(eval_iters)\n        for k in range(eval_iters):\n            X, Y = get_batch(split)\n            logits, loss = model(X, Y)\n            losses[k] = loss.item()\n    return out\n```",
    "frame_4919.jpg": "```python\nself.key = nn.Linear(n_embd, head_size, bias=False)\nself.query = nn.Linear(n_embd, head_size, bias=False)\nself.value = nn.Linear(n_embd, head_size, bias=False)\nself.register_buffer('tril', torch.tril(torch.ones(block_size)))\n\ndef forward(self, x: (B, T, C)):\n    B, T, C = x.shape\n    k = self.key(x)\n    q = self.query(x)\n    # compute attention scores (\"affinities\")\n    wei = q @ k.transpose(-2, -1) * C**-0.5  # (B, C) @ (B, C) -> (B, T, T)\n    wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))  # (B, T, T)\n    wei = F.softmax(wei, dim=-1)  # perform the weighted aggregation of the values\n    v = self.value(x)  # (B, T, C) -> (B, T, C)\n    out = v @ wei  # (B, T, C) -> (B, T, C)\n    return out\n\n# super simple bigram model\nclass BigramLanguageModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        # each token directly reads off the logits for the next token from a lookup table\n        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n        self.sa_head = Head(n_embd)\n        self.lm_head = nn.Linear(n_embd, vocab_size)\n\ndef forward(self, idx, targets=None):\n    B, T = idx.shape\n```\n",
    "frame_4920.jpg": "Code NA",
    "frame_4928.jpg": "MultiHead(Q, K, V) = Concat(head\u2081, \u22ef , head\u2095)W\u2070  \nwhere head\u1d62 = Attention(QW\u206eQ\u1d62, KW\u206eK\u1d62, VW\u206eV\u1d62)\n\nWhere the projections are parameter matrices W\u206eQ\u1d62 \u2208 \u211d^(d_model\u00d7dk), W\u206eK\u1d62 \u2208 \u211d^(d_hd\u00d7dk), W\u206eV\u1d62 \u2208 \u211d^(d_hd\u00d7d_model).\n\nIn this work we employ h = 8 parallel attention layers, or heads. For each of them, dk = d_model/h = 64.",
    "frame_4929.jpg": "Code NA",
    "frame_4932.jpg": "MultiHead(Q, K, V) = Concate(head\u2081, ..., head\u2095)W\u2070  \nwhere head\u1d62 = Attention(QW\u2071\u1d62, KW\u2071\u1d62, VW\u2071\u1d62)  \n\nCode NA",
    "frame_4934.jpg": "Code NA",
    "frame_4942.jpg": "```python\nclass BigramLanguageModel(nn.Module):\n\n    def __init__(self):\n        super().__init__()\n        # each token directly reads off the logits for the next token from a lookup table\n        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n        self.sa_heads = MultiHeadAttention(n_embd // 4)  # i.e. 4 heads of 8-dimensional self-attention\n        self.lm_head = nn.Linear(n_embd, vocab_size)\n\n    def forward(self, idx, targets=None):\n        B, T = idx.shape\n        \n        # idx and targets are both (B, T) tensor of integers\n        tok_emb = self.token_embedding_table(idx)  # (B, T, C)\n        pos_emb = self.position_embedding_table(torch.arange(T, device=device))  # (T, C)\n        x = tok_emb + pos_emb  # apply one head of self-attention. (B,T,C)\n        logits = self.lm_head(x)  # (B, vocab_size)\n```",
    "frame_4943.jpg": "Here is the extracted developer code from the image:\n\n```python\ndef forward(self, x):\n    B, T, C = x.shape\n    k = self.key(x)  # (B, T, C)\n    q = self.query(x)  # (B, T, C)\n    v = self.value(x)  # (B, T, C)\n\n    # compute attention scores (\"affinities\")\n    attn_scores = (q @ k.transpose(-2, -1) / (C**0.5))  # (B, T, T)\n    attn_scores = attn_scores.masked_fill(self.tril.mask(), float('-inf'))  # (B, T, T)\n    \n    attn_weights = F.softmax(attn_scores, dim=-1)  # (B, T, T) \n    # perform the weighted aggregation of the values\n    out = attn_weights @ v  # (B, T, C)\n    return out\n\nclass MultiHeadAttention(nn.Module):\n    \"\"\" multiple heads of self-attention in parallel \"\"\"\n    \n    def __init__(self, num_heads, head_size):\n        super().__init__()\n        self.heads = nn.ModuleList(Head(head_size) for _ in range(num_heads))\n    \n    def forward(self, x):\n        return torch.cat([h(x) for h in self.heads], dim=-1)\n```\n\nIf you need any further assistance, feel free to ask!",
    "frame_4945.jpg": "```python\nclass MultiHeadAttention(nn.Module):\n    \"\"\" multiple heads of self-attention in parallel \"\"\"\n    \n    def __init__(self, num_heads, head_size):\n        super().__init__()\n        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n\n    def forward(self, x):\n        return torch.cat([h(x) for h in self.heads], dim=-1)\n\n# super simple bigram model\nclass BigramLanguageModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n```",
    "frame_4973.jpg": "```python\nclass MultiHeadAttention(nn.Module):\n    \"\"\" multiple heads of self-attention in parallel \"\"\"\n\n    def __init__(self, num_heads, head_size):\n        super().__init__()\n        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n\n    def forward(self, x):\n        return torch.cat([h(x) for h in self.heads], dim=-1)\n\n# super simple bigram model\nclass BigramLanguageModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n        self.sa_heads = MultiHeadAttention(4, n_embd // 4)  # i.e. 4 heads of 8-dimensional self-attention\n        self.lm_head = nn.Linear(n_embd, vocab_size)\n\n    def forward(self, idx, targets=None):\n        B, T = idx.shape\n```",
    "frame_4997.jpg": "```python\nclass MultiHeadAttention(nn.Module):\n    \"\"\" multiple heads of self-attention in parallel \"\"\"\n    \n    def __init__(self, num_heads, head_size):\n        super().__init__()\n        self.sa_heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n        \n    def forward(self, x):\n        return torch.cat([h(x) for h in self.sa_heads], dim=-1)\n\nclass BigramLanguageModel(nn.Module):\n    \n    def __init__(self):\n        super().__init__()\n        # each token directly reads off the embedding\n        self.token_embedding_table = nn.Embedding(\n            num_embeddings, embedding_dim)\n        self.position_embedding_table = nn.Embedding(\n            max_length, embedding_dim)\n        self.sa_heads = MultiHeadAttention(4, n_embd/4) # i.e. 4 heads of 8-dimensional self-attention\n        self.lm_head = nn.Linear(n_embd, vocab_size)\n\n    def forward(self, idx, targets=None):\n        ...\n```",
    "frame_5008.jpg": "```python\nclass MultiHeadAttention(nn.Module):\n    \"\"\" multiple heads of self-attention in parallel \"\"\"\n    \n    def __init__(self, num_heads, head_size):\n        super().__init__()\n        self.heads = nn.ModuleList(Head(head_size) for _ in range(num_heads))\n\n    def forward(self, x):\n        return torch.cat([h(x) for h in self.heads], dim=-1)\n\n# super simple bigram model\nclass BigramLanguageModel(nn.Module):\n\n    def __init__(self):\n        super().__init__()\n        # each token directly reads off the logits for the next token from a lookup table\n        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n        self.sa_heads = MultiHeadAttention(4, n_embd // 4)  # i.e. 4 heads of 8-dimensional self-attention\n        self.lm_head = nn.Linear(n_embd, vocab_size)\n\n    def forward(self, idx, targets=None):\n```",
    "frame_5019.jpg": "```python\nclass MultiHeadAttention(nn.Module):\n    \"\"\" multiple heads of self-attention in parallel \"\"\"\n\n    def __init__(self, num_heads, head_size):\n        super().__init__()\n        self.heads = nn.ModuleList(Head(head_size) for _ in range(num_heads))\n\n    def forward(self, x):\n        return torch.cat([h(x) for h in self.heads], dim=-1)\n\n# super simple bigram model\nclass BigramLanguageModel(nn.Module):\n\n    def __init__(self):\n        super().__init__()\n        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n        self.sa_heads = MultiHeadAttention(4, n_embd // 4)  # i.e. 4 heads of 8-dimensional self-attention\n        self.lm_head = nn.Linear(n_embd, vocab_size)\n\n    def forward(self, idx, targets=None):\n```",
    "frame_5023.jpg": "```python\ndef forward(self, x):\n    return torch.cat(h(x) for h in self.heads), dim=-1\n\nclass BigramLanguageModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n        self.sa_heads = MultiHeadAttention(4, n_embd//4)  # i.e. 4 heads of 8-dimensional self-attention\n        self.lm_head = nn.Linear(n_embd, vocab_size)\n\n    def forward(self, idx, targets=None):\n        tok_emb = self.token_embedding_table(idx)  # (B,T,C)\n        pos_emb = self.position_embedding_table(torch.arange(T, device=device))  # (T,C)\n        x = tok_emb + pos_emb  # (B,T,C)\n        x = self.sa_heads(x)  # apply one head of self-attention. (B,T,C)\n        logits = self.lm_head(x)  # (B,vocab_size)\n\n        if targets is not None:\n            loss = None\n```\n",
    "frame_5030.jpg": "```python\nclass BigramLanguageModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n        self.sa_heads = MultiHeadAttention(4, n_embd/4)  # i.e. 4 heads of 8-dimensional self-attention\n        self.lm_head = nn.Linear(n_embd, vocab_size)\n\n    def forward(self, idx, targets=None):\n        B, T = idx.shape\n        tok_emb = self.token_embedding_table(idx)  # (B, T, C)\n        pos_emb = self.position_embedding_table(torch.arange(T, device=device))  # (T, C)\n        x = tok_emb + pos_emb  # (B, T, C)\n        x = self.sa_heads(x)  # apply one head of self-attention. (B, T, C)\n        logits = self.lm_head(x)  # (B, vocab_size)\n```\n",
    "frame_5032.jpg": "```python\ndef forward(self, x):\n    return torch.cat(h(x) for h in self.heads), dim=-1\n\nclass BigramLanguageModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n        self.sa_heads = MultiHeadAttention(4, n_embd/4)  # i.e. 4 heads of 8-dimensional self-attention\n        self.lm_head = nn.Linear(n_embd, vocab_size)\n\n    def forward(self, idx, targets=None):\n        tok_emb = self.token_embedding_table(idx)  # (B,T,C)\n        pos_emb = self.position_embedding_table(torch.arange(T, device=device))  # (T,C)\n        x = tok_emb + pos_emb  # (B,T,C)\n        x = self.sa_heads(x)  # apply one head of self-attention. (B,T,C)\n        logits = self.lm_head(x)  # (B,T,vocab_size)\n```\n\n",
    "frame_5038.jpg": "```python\ndef forward(self, x):\n    return torch.cat((h(x) for h in self.heads), dim=-1)\n\nclass BigramLanguageModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n        self.sa_heads = MultiHeadAttention(4, n_embd//4) # i.e. 4 heads of 8-dimensional self-attention\n        self.lm_head = nn.Linear(n_embd, vocab_size)\n\n    def forward(self, idx, targets=None):\n        B, T = idx.shape\n        tok_emb = self.token_embedding_table(idx) # (B, T, C)\n        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T, C)\n        x = tok_emb + pos_emb # (B, T, C)\n        x = self.sa_heads(x) # apply one head of self-attention. (B, T, C)\n        logits = self.lm_head(x) # (B, T, vocab_size)\n```\n",
    "frame_5045.jpg": "```python\ndef forward(self, x):\n    return torch.cat(h(x) for h in self.heads), dim=-1\n\nclass BigramLanguageModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n        self.sa_heads = MultiHeadAttention(4, n_embd/4)  # i.e. 4 heads of 8-dimensional self-attention\n        self.lm_head = nn.Linear(n_embd, vocab_size)\n\n    def forward(self, idx, targets=None):\n        tok_emb = self.token_embedding_table(idx)  # (B, T, C)\n        pos_emb = self.position_embedding_table(torch.arange(block_size, device=device))  # (T, C)\n        x = tok_emb + pos_emb  # (B, T, C)\n        x = self.sa_heads(x)  # apply one head of self-attention. (B, T, C)\n        logits = self.lm_head(x)  # (B, T, vocab_size)\n```\n",
    "frame_5047.jpg": "```python\ndef forward(self, idx, targets=None):\n    tok_emb = self.token_embedding_table(idx)  # (B,T,C)\n    pos_emb = self.position_embedding_table(torch.arange(B, device=device))  # (T,C)\n    x = tok_emb + pos_emb  # (B,T,C)\n    x = self.sa_heads(x)  # apply one head of self-attention. (B,T,C)\n    logits = self.lm_head(x)  # (B,vocab_size)\n```",
    "frame_5050.jpg": "```python\ndef forward(self, x):\n    return torch.cat((h(x) for h in self.heads), dim=-1)\n\nclass BigramLanguageModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n        self.sa_heads = MultiHeadAttention(4, n_embd/4)  # i.e. 4 heads of 8-dimensional self-attention\n        self.lm_head = nn.Linear(n_embd, vocab_size)\n\n    def forward(self, idx, targets=None):\n        tok_emb = self.token_embedding_table(idx)  # (B, T, C)\n        pos_emb = self.position_embedding_table(torch.arange(T, device=device))  # (T, C)\n        x = tok_emb + pos_emb  # (B, T, C)\n        x = self.sa_heads(x)  # apply one head of self-attention. (B, T, C)\n        logits = self.lm_head(x)  # (B, T, vocab_size)\n```",
    "frame_5052.jpg": "```python\ndef forward(self, idx, targets=None):\n    tok_emb = self.token_embedding_table(idx)  # (B,T,C)\n    pos_emb = self.position_embedding_table(torch.arange(T, device=device))  # (T,C)\n    x = tok_emb + pos_emb  # (B,T,C)\n    x = self.sa_heads(x)  # apply one head of self-attention. (B,T,C)\n    logits = self.lm_head(x)  # (B,vocab_size)\n```",
    "frame_5053.jpg": "```python\ndef forward(self, idx, targets=None):\n    return torch.cat((h(x) for h in self.heads), dim=-1)\n\nclass BigramLanguageModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n        self.sa_heads = MultiHeadAttention(4, n_embd/4) # i.e. 4 heads of 8-dimensional self-attention\n        self.lm_head = nn.Linear(n_embd, vocab_size)\n\ndef forward(self, idx, targets=None):\n    tok_emb = self.token_embedding_table(idx) # (B,T,C)\n    pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n    x = tok_emb + pos_emb # (B,T,C)\n    x = self.sa_heads(x) # apply one head of self-attention. (B,T,C)\n    logits = self.lm_head(x) # (B,T,vocab_size)\n```",
    "frame_5054.jpg": "```python\ndef forward(self, idx, targets=None):\n    tok_emb = self.token_embedding_table(idx)  # (B, T, C)\n    pos_emb = self.position_embedding_table(torch.arange(T, device=device))  # (T, C)\n    x = tok_emb + pos_emb  # (B, T, C)\n    x = self.sa_heads(x)  # apply one head of self-attention. (B, T, C)\n    logits = self.lm_head(x)  # (B, vocab_size)\n```",
    "frame_5061.jpg": "```python\ndef forward(self, idx, targets=None):\n    return torch.cat(h(x) for h in self.heads), dim=-1\n\nclass BigramLanguageModel(nn.Module):\n\n    def __init__(self):\n        super().__init__()\n        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n        self.sa_heads = MultiHeadAttention(4, n_embd/4)  # i.e. 4 heads of 8-dimensional self-attention\n        self.lm_head = nn.Linear(n_embd, vocab_size)\n        \n    def forward(self, idx, targets=None):\n        tok_emb = self.token_embedding_table(idx)  # (B, T, C)\n        pos_emb = self.position_embedding_table(torch.arange(T, device=device))  # (T, C)\n        x = tok_emb + pos_emb  # (B, T, C)\n        x = self.sa_heads(x)  # apply one head of self-attention. (B, T, C)\n        logits = self.lm_head(x)  # (B, T, vocab_size)\n```",
    "frame_5067.jpg": "Code NA",
    "frame_5102.jpg": "MultiHead(Q, K, V) = Concat(head\u2081, ..., head\u2095)W\u2070  \nwhere head\u1d62 = Attention(QW\u1d60\u1d62, KW\u1d4f\u1d62, VW\u1d5b\u1d62)  \n\nWhere the projections are parameter matrices W\u1d60\u1d62 \u2208 \u211d^(d_model \u00d7 d\u2096), W\u1d4f\u1d62 \u2208 \u211d^(d_model \u00d7 d\u2096), W\u1d5b\u1d62 \u2208 \u211d^(d_model \u00d7 d\u1d5b).",
    "frame_5103.jpg": "Code NA",
    "frame_5110.jpg": "FFN(x) = max(0, xW\u2081 + b\u2081)W\u2082 + b\u2082 \n\nd\u2093 = 512, and d\u2091\u2097 = 2048. \n\nCode NA.",
    "frame_5118.jpg": "```python\nclass FeedForward(nn.Module):\n    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n\n    def __init__(self, n_embd):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(n_embd, n_embd),\n            nn.ReLU()\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n# super simple bigram model\nclass BigramLanguageModel(nn.Module):\n\n    def __init__(self):\n        super().__init__()\n        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n        self.sa_heads = MultiHeadAttention(4, n_embd/4)  # i.e. 4 heads of 8-dimensional self-attention\n        self.fwd = FeedForward(n_embd)\n        self.lm_head = nn.Linear(n_embd, vocab_size)\n\n    def forward(self, idx, targets=None):\n        B, T = idx.shape\n        # if idx and targets are both (B, T) tensor of integers\n```\n",
    "frame_5122.jpg": "Here's the extracted developer code from the image:\n\n```python\nclass BigramLanguageModel(nn.Module):\n\n    def __init__(self):\n        super().__init__()\n        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n        self.sa_heads = MultiHeadAttention(4, n_embd / 4)  # i.e. 4 heads of 8-dimensional self-attention\n        self.ffwd = FeedForward(n_embd)\n        self.lm_head = nn.Linear(n_embd, vocab_size)\n\n    def forward(self, x, targets=None):\n        # idx and targets are both (B, T) tensor of integers\n        B, T = idx.shape\n        tok_emb = self.token_embedding_table(idx)  # (B, T, C)\n        pos_emb = self.position_embedding_table(torch.arange(T, device=device))  # (T, C)\n        x = tok_emb + pos_emb  # (B, T, C)\n        x = self.ffwd(x)  # apply one head of self-attention. (B, T, C)\n```\n\nIf you need any more information or details, feel free to ask!",
    "frame_5127.jpg": "```python\nclass BigramLanguageModel(nn.Module):\n\n    def __init__(self):\n        super().__init__()\n        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n        self.sa_heads = MultiHeadAttention(4, n_embd/4)  # i.e. 4 heads of 8-dimensional self-attention\n        self.ffwd = FeedForward(n_embd)\n        self.lm_head = nn.Linear(n_embd, vocab_size)\n\n    def forward(self, idx, targets=None):\n        # idx and target (variable)\n        tok_emb = self.token_embedding_table(idx)  # (B,T,C)\n        pos_emb = self.position_embedding_table(torch.arange(T, device=device))  # (T,C)\n        x = tok_emb + pos_emb  # apply one head of self-attention. (B,T,C)\n        x = self.ffwd(x)  # (B,T,C)\n        logits = self.lm_head(x)  # (B,T,vocab_size)\n\n        if targets is None:\n            loss = None\n        else:\n            B, T, C = logits.shape\n```",
    "frame_5134.jpg": "```python\ndef forward(self, idx, targets=None):\n    # idx and targets are both (B,T) tensor of integers\n    tok_emb = self.token_embedding_table(idx)  # (B,T,C)\n    pos_emb = self.position_embedding_table(orch.arange(T, device=device))  # (T,C)\n    x = tok_emb + pos_emb  # (B,T,C)\n    x = self.sa_heads(x)  # apply one head of self-attention. (B,T,C)\n    logits = self.lm_head(x)  # (B,T,vocab_size)\n\n    if targets is None:\n        loss = None\n    else:\n        B, T, C = logits.shape\n```",
    "frame_5136.jpg": "```python\ndef forward(self, x):\n    return self.net(x)\n\nclass BigramLanguageModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n        self.sa_heads = MultiHeadAttention(4, n_embd // 4)  # i.e. 4 heads of 8-dimensional self-attention\n        self.ffwd = FeedForward(n_embd)\n        self.lm_head = nn.Linear(n_embd, vocab_size)\n\n    def forward(self, idx, targets=None):\n        tok_emb = self.token_embedding_table(idx)  # (B,T,C)\n        pos_emb = self.position_embedding_table(torch.arange(T, device=device))  # (T,C)\n        x = tok_emb + pos_emb  # apply one head of self-attention. (B,T,C)\n        logits = self.lm_head(x)  # (B,T,vocab_size)\n\n        if targets is None:\n            loss = None\n        else:\n            B, T, C = logits.shape\n```",
    "frame_5150.jpg": "```python\nclass FeedForward(nn.Module):\n    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n    \n    def __init__(self, n_embd):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(n_embd, n_embd),\n            nn.ReLU()\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n# super simple bigram model\nclass BigramLanguageModel(nn.Module):\n    \n    def __init__(self):\n        super().__init__()\n        # each token directly reads off the logits for the next token from a lookup table\n        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n        self.sa_heads = MultiHeadAttention(4, n_embd/4) # i.e. 4 heads of 8-dimensional self-attention\n        self.ffwd = FeedForward(n_embd)\n        self.lm_head = nn.Linear(n_embd, vocab_size)\n\n    def forward(self, idx, targets=None):\n        B, T = idx.shape\n```",
    "frame_5151.jpg": "Here\u2019s the extracted developer code from the image:\n\n```python\nclass BigramLanguageModel(nn.Module):\n\n    def __init__(self):\n        super().__init__()\n        # each token directly reads off the logits for the next token from a lookup table\n        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n        self.sa_heads = MultiHeadAttention(4, n_embd/4) # i.e. 4 heads of 8-dimensional self-attention\n        self.ffg = FeedForward(n_embd)\n        self.lm_head = nn.Linear(n_embd, vocab_size)\n\n    def forward(self, x, targets=None):\n        # idx and targets are both (B,T) tensor of integers\n        tok_emb = self.token_embedding_table(idx)  # (B, T, C)\n        pos_emb = self.position_embedding_table(torch.arange(B, device=device)) # (B, T, C)\n        x = tok_emb + pos_emb # apply one head of self-attention. (B,T,C)\n```\n\nIf you need any more information or assistance, feel free to ask!",
    "frame_5154.jpg": "```python\ndef forward(self, idx, targets=None):\n    B, T = idx.shape\n    tok_emb = self.token_embedding_table(idx)  # (B,T) tensor of integers\n    pos_emb = self.position_embedding_table(torch.arange(T, device=device))  # (T,C)\n    x = tok_emb + pos_emb  # apply one head of self-attention. (B,T,C)\n    x = self.fwd(x)  # (B,T,C)\n    logits = self.lm_head(x)  # (B,T,vocab_size)\n\n    if targets is None:\n        loss = None\n    else:\n        B, T, C = logits.shape\n```\n",
    "frame_5163.jpg": "```python\nclass FeedForward(nn.Module):\n    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n\n    def __init__(self, n_embd):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(n_embd, n_embd),\n            nn.ReLU(),\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n# super simple bigram model\nclass BigramLanguageModel(nn.Module):\n\n    def __init__(self):\n        super().__init__()\n        # each token directly reads off the logits for the next token from a lookup table\n        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n        self.sa_heads = MultiHeadAttention(4, n_embd / 4)  # i.e. 4 heads of 8-dimensional self-attention\n        self.ffwd = FeedForward(n_embd)\n```",
    "frame_5169.jpg": "```python\nclass FeedForward(nn.Module):\n    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n    \n    def __init__(self, n_embd):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(n_embd, n_embd),\n            nn.ReLU()\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n# super simple bigram model\nclass BigramLanguageModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        # each token directly reads off the logits for the next token from a lookup table\n        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n        self.sa_heads = MultiHeadAttention(4, n_embd // 4)  # i.e. 4 heads of 8-dimensional self-attention\n        self.ffwd = FeedForward(n_embd)\n```",
    "frame_5173.jpg": "```python\nclass FeedForward(nn.Module):\n    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n    \n    def __init__(self, n_embd):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(n_embd, n_embd),\n            nn.ReLU()\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n# super simple bigram model\nclass BigramLanguageModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        # each token directly reads off the logits for the next token from a lookup table\n        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n        self.sa_heads = MultiHeadAttention(4, n_embd // 4)  # i.e. 4 heads of 8-dimensional self-attention\n        self.ffwd = FeedForward(n_embd)\n```",
    "frame_5175.jpg": "Here is the extracted developer code from the image:\n\n```python\nclass FeedForward(nn.Module):\n    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n\n    def __init__(self, n_embd):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(n_embd, n_embd),\n            nn.ReLU()\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\nclass BigramLanguageModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n        self.sa_heads = MultiHeadAttention(4, n_embd//4)  # i.e., 4 heads of 8-dimensional self-attention\n        self.ffwd = FeedForward(n_embd)\n```\n\nIf you need anything else, feel free to ask!",
    "frame_5176.jpg": "```python\nclass FeedForward(nn.Module):\n    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n\n    def __init__(self, n_embd):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(n_embd, n_embd),\n            nn.ReLU(),\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n# super simple bigram model\nclass BigramLanguageModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n        self.sa_heads = MultiHeadAttention(4, n_embd / 4)  # i.e. 4 heads of 8-dimensional self-attention\n        self.ffwd = FeedForward(n_embd)\n```",
    "frame_5178.jpg": "```python\ndef forward(self, idx, targets=None):\n    B, T = idx.shape\n\n    # idx and targets are both (B,T) tensor of integers\n    tok_emb = self.token_embedding_table(idx)  # (B,T,C)\n    pos_emb = self.position_embedding_table(torch.arange(T, device=device))  # (T,C)\n    x = tok_emb + pos_emb  # (B,T,C)\n    x = self.sa_heads(x)  # apply one head of self-attention, (B,T,C)\n    logits = self.lm_head(x)  # (B,T,vocab_size)\n\n    if targets is None:\n        loss = None\n    else:\n        B, T, C = logits.shape\n```",
    "frame_5194.jpg": "```python\nclass BigramLanguageModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n        self.sa_heads = MultiHeadAttention(4, n_embd/4)  # i.e. 4 heads of 8-dimensional self-attention\n        self.ffwd = FeedForward(n_embd)\n        self.lm_head = nn.Linear(n_embd, vocab_size)\n\n    def forward(self, idx, targets=None):\n        B, T = idx.shape\n        tok_emb = self.token_embedding_table(idx)  # (B,T,C)\n        pos_emb = self.position_embedding_table(torch.arange(T, device=device))  # (T,C)\n        x = tok_emb + pos_emb  # (B,T,C)\n        x = self.sa_heads(x)  # apply one head of self-attention. (B,T,C)\n        logits = self.lm_head(x)  # (B,T,vocab_size)\n\n        if targets is None:\n            loss = None\n        else:\n            B, T, C = logits.shape\n            logits = logits.view(B*T, C)\n            targets = targets.view(B*T)\n```",
    "frame_5197.jpg": "Here is the extracted developer code from the image:\n\n```python\nclass BigramLanguageModel(nn.Module):\n\n    def __init__(self):\n        super().__init__()\n        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n        self.sa_heads = MultiHeadAttention(4, n_embd/4)  # i.e. 4 heads of 8-dimensional self-attention\n        self.ffwd = FeedForward(n_embd, vocab_size)\n        self.lm_head = nn.Linear(n_embd, vocab_size)\n\n    def forward(self, idx, targets=None):\n        B, T = idx.shape\n\n        tok_emb = self.token_embedding_table(idx)  # (B, T, C)\n        pos_emb = self.position_embedding_table(torch.arange(T, device=device))  # (T, C)\n        x = tok_emb + pos_emb  # (B, T, C)\n        x = self.sa_heads(x)  # apply one head of self-attention. (B, T, C)\n        logits = self.lm_head(x)  # (B, T, vocab_size)\n\n        if targets is None:\n            loss = None\n        else:\n            B, T, C = logits.shape\n            logits = logits.view(B*T, C)\n            targets = targets.view(B*T)\n```\n\nThis code defines a simple bigram language model using PyTorch.",
    "frame_5199.jpg": "```python\nclass BigramLanguageModel(nn.Module):\n\n    def __init__(self):\n        super().__init__()\n        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n        self.sa_heads = MultiHeadAttention(4, n_embd/4)  # i.e. 4 heads of 8-dimensional self-attention\n        self.ffwd = FeedForward(n_embd, vocab_size)\n        self.lm_head = nn.Linear(n_embd, vocab_size)\n\n    def forward(self, idx, targets=None):\n        B, T = idx.shape\n        tok_emb = self.token_embedding_table(idx)  # (B, T, C)\n        pos_emb = self.position_embedding_table(torch.arange(T, device=device))  # (T, C)\n        x = tok_emb + pos_emb  # (B, T, C)\n        x = self.sa_heads(x)  # apply one head of self-attention. (B, T, C)\n        logit = self.lm_head(x)  # (B, T, vocab_size)\n\n        if targets is None:\n            loss = None\n        else:\n            B, T, C = logit.shape\n            logits = logit.view(B*T, C)\n            targets = targets.view(B*T)\n```\n",
    "frame_5201.jpg": "Code NA",
    "frame_5210.jpg": "```python\ndef __init__(self, n_embd):\n    super().__init__()\n    self.net = nn.Sequential(\n        nn.Linear(n_embd, n_embd),\n        nn.ReLU(),\n    )\n\ndef forward(self, x):\n    return self.net(x)\n\nclass Block(nn.Module):\n    \"\"\" Transformer block: communication followed by computation \"\"\"\n    \n    def __init__(self, n_embd, n_head):\n        super().__init__()\n        head_size = n_embd // n_head\n        self.sa = MultiHeadAttention(n_head, head_size)\n        self.ffwd = FeedForward(n_embd)\n\n    def forward(self, x):\n        x = self.sa(x)\n        x = self.ffwd(x)\n        return x\n\nclass BigramLanguageModel(nn.Module):\n    \n    def __init__(self):\n```",
    "frame_5214.jpg": "Code NA",
    "frame_5261.jpg": "```python\nclass Block(nn.Module):\n    \"\"\" Transformer block: communication followed by computation \"\"\"\n    \n    def __init__(self, n_embd, n_head):\n        # n_embd: embedding dimension, n_head: the number of heads we'd like\n        super().__init__()\n        head_size = n_embd // n_head\n        self.sa = MultiHeadAttention(n_head, head_size)\n        self.ffwd = FeedForward(n_embd)\n\n    def forward(self, x):\n        x = self.sa(x)\n        x = self.ffwd(x)\n        return x\n\n# super simple bigram model\nclass BigramLanguageModel(nn.Module):\n    \n    def __init__(self):\n        super().__init__()\n        # each token directly reads off the logits for the next token from a lookup table\n        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n        self.blocks = nn.Sequential(\n            Block(n_embd, n_head=4),\n            Block(n_embd, n_head=4),\n            Block(n_embd, n_head=4),\n            Block(n_embd, n_head=4),\n        )\n        self.lm_head = nn.Linear(n_embd, vocab_size)\n```",
    "frame_5269.jpg": "```\nclass BigramLanguageModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n        self.blocks = nn.Sequential(\n            Block(n_embd, n_head=4),\n            Block(n_embd, n_head=4),\n            Block(n_embd, n_head=4),\n            Block(n_embd, n_head=4),\n        )\n        self.lm_head = nn.Linear(n_embd, vocab_size)\n\n    def forward(self, idx, targets=None):\n        B, T = idx.shape\n        tok_emb = self.token_embedding_table(idx)\n```\n",
    "frame_5271.jpg": "```python\ndef forward(self, x):\n    x = self.sa(x)\n    x = self.fwd(x)\n    return x\n\nclass BigramLanguageModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n        self.blocks = nn.Sequential(\n            Block(n_embd, n_head=4),\n            Block(n_embd, n_head=4),\n            Block(n_embd, n_head=4),\n        )\n        self.lm_head = nn.Linear(n_embd, vocab_size)\n\ndef forward(self, idx, targets=None):\n    B, T = idx.shape\n    tok_emb = self.token_embedding_table(idx)  # (B, T, C)\n    pos_emb = self.position_embedding_table(torch.arange(T, device=device))  # (T, C)\n    x = tok_emb + pos_emb  # (B, T, C)\n    logits = self.lm_head(x)  # (B, T, vocab_size)\n```",
    "frame_5275.jpg": "```python\nclass Block(nn.Module):\n    \"\"\" Transformer block: communication followed by computation \"\"\"\n    \n    def __init__(self, n_embd, n_head):\n        # n_embd: embedding dimension, n_head: the number of heads we'd like\n        super().__init__()\n        head_size = n_embd // n_head\n        self.sa = MultiHeadAttention(n_head, head_size)\n        self.ffwd = FeedForward(n_embd)\n\n    def forward(self, x):\n        x = self.sa(x)\n        x = self.ffwd(x)\n        return x\n\n# super simple bigram model\nclass BigramLanguageModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        # each token directly reads off the logits for the next token from a lookup table\n        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n        self.blocks = nn.Sequential(\n```",
    "frame_5278.jpg": "```python\nclass BigramLanguageModel(nn.Module):\n\n    def __init__(self):\n        super().__init__()\n        # each token directly reads off the logits for the next token from a lookup table\n        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n        self.blocks = nn.Sequential(\n            Block(n_embd, n_head=4),\n            Block(n_embd, n_head=4),\n            Block(n_embd, n_head=4),\n            Block(n_embd, n_head=4),\n        )\n        self.lm_head = nn.Linear(n_embd, vocab_size)\n\n    def forward(self, idx, targets=None):\n        B, T = idx.shape\n        tok_emb = self.token_embedding_table(idx)  # (B,T,C)\n        pos_emb = self.position_embedding_table(torch.arange(T, device=device))  # (T,C)\n        x = tok_emb + pos_emb  # (B,T,C)\n        x = self.blocks(x)  # (B,T,C)\n        logits = self.lm_head(x)  # (B,T,vocab_size)\n\n        if targets is None:\n            loss = None\n        else:\n            B, T, C = logits.shape\n            logits = logits.view(B*T, C)\n            targets = targets.view(B*T)\n```",
    "frame_5289.jpg": "```python\nclass Block(nn.Module):\n    \"\"\" Transformer block: communication followed by computation \"\"\"\n    \n    def __init__(self, n_embd, n_head):\n        super().__init__()\n        self.head_size = n_embd // n_head\n        self.self_attn = MultiHeadAttention(n_head, head_size)\n        self.ffwd = FeedForward(n_embd)\n\n    def forward(self, x):\n        x = self.self_attn(x)\n        x = self.ffwd(x)\n        return x\n\n# super simple bigram model\nclass BigramLanguageModel(nn.Module):\n\n    def __init__(self):\n        super().__init__()\n        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n        self.blocks = nn.Sequential(\n            Block(n_embd, n_head=4),\n            Block(n_embd, n_head=4),\n            Block(n_embd, n_head=4),\n            Block(n_embd, n_head=4),\n        )\n        self.lm_head = nn.Linear(n_embd, vocab_size)\n```",
    "frame_5291.jpg": "```python\nclass BigramLanguageModel(nn.Module):\n\n    def __init__(self):\n        super().__init__()\n        # each token directly reads off the logits for the next token from a lookup table\n        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n        self.blocks = nn.Sequential(\n            Block(n_embd, n_head=4),\n            Block(n_embd, n_head=4),\n            Block(n_embd, n_head=4),\n            Block(n_embd, n_head=4),\n        )\n        self.lm_head = nn.Linear(n_embd, vocab_size)\n\n    def forward(self, idx, targets=None):\n        B, T = idx.shape\n        tok_emb = self.token_embedding_table(idx)\n```",
    "frame_5300.jpg": "Code NA",
    "frame_5340.jpg": "```python\nR(x) = Output - Input = H(x) - x\n```",
    "frame_5341.jpg": "H(x) = R(x) + x",
    "frame_5342.jpg": "Code NA",
    "frame_5431.jpg": "```python\nclass Block(nn.Module):\n    \"\"\" Transformer block: communication followed by computation \"\"\"\n\n    def __init__(self, n_embd, n_head):\n        # n_embd: embedding dimension, n_head: the number of heads we'd like\n        super().__init__()\n        head_size = n_embd // n_head\n        self.sa = MultiHeadAttention(n_head, head_size)\n        self.ffwd = FeedForward(n_embd)\n\n    def forward(self, x):\n        x = self.sa(x)\n        x = self.ffwd(x)\n        return x\n\nclass BigramLanguageModel(nn.Module):\n```",
    "frame_5452.jpg": "```python\nclass MultiHeadAttention(nn.Module):\n    \"\"\" multiple heads of self-attention in parallel \"\"\"\n    \n    def __init__(self, num_heads, head_size):\n        super().__init__()\n        self.heads = nn.ModuleList(\n            Head(head_size) for _ in range(num_heads)\n        )\n\n    def forward(self, x):\n        return torch.cat([h(x) for h in self.heads], dim=-1)\n\nclass FeedForward(nn.Module):\n    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n\n    def __init__(self, n_embd):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(n_embd, n_embd),\n            nn.ReLU()\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\nclass Block(nn.Module):\n    \"\"\" Transformer block: communication followed by computation \"\"\"\n```",
    "frame_5480.jpg": "```python\ndef __init__(self, num_heads, head_size):\n    super().__init__()\n    self.heads = nn.ModuleList(\n        [Head(head_size) for _ in range(num_heads)]\n    )\n    self.proj = nn.Linear(n_embd, n_embd)\n\ndef forward(self, x):\n    out = torch.cat((h(x) for h in self.heads), dim=-1)\n    out = self.proj(out)\n    return out\n\nclass FeedForward(nn.Module):\n    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n\n    def __init__(self, n_embd):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(n_embd, n_embd),\n            nn.ReLU(),\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\nclass Block(nn.Module):\n    \"\"\" Transformer block: communication followed by computation \"\"\"\n\n    def __init__(self, n_embd, n_head):\n        # _n_embd: embedding dimension, n_head: the number of heads we'd like\n        super().__init__()\n```",
    "frame_5483.jpg": "Here is the extracted code from the image:\n\n```python\nclass FeedForward(nn.Module):\n    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n    \n    def __init__(self, n_embd):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(n_embd, n_embd),\n            nn.ReLU(),\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\nclass Block(nn.Module):\n    \"\"\" Transformer block: communication followed by computation \"\"\"\n    \n    def __init__(self, n_embd, n_head):\n        # n_embd: embedding dimension, n_head: the number of heads we'd like\n        super().__init__()\n        head_size = n_embd // n_head\n        self.sa = MultiHeadAttention(n_head, head_size)\n        self.ffwd = FeedForward(n_embd)\n\n    def forward(self, x):\n        ...\n```\n\nIf there was no code, I would return 'Code NA'.",
    "frame_5489.jpg": "```python\ndef forward(self, x):\n    out = torch.cat((h(x) for h in self.heads), dim=-1)\n    return out\n\nclass FeedForward(nn.Module):\n    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n    \n    def __init__(self, n_embd):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(n_embd, n_embd),\n            nn.ReLU(),\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\nclass Block(nn.Module):\n    \"\"\" Transformer block: communication followed by computation \"\"\"\n    \n    def __init__(self, n_embd, n_head):\n        # n_embd: embedding dimension, n_head: the number of heads we'd like\n        super().__init__()\n        head_size = n_embd // n_head\n        self.sa = MultiHeadAttention(n_head, head_size)\n        self.ffwd = FeedForward(n_embd)\n\n    def forward(self, x):\n        ...\n```",
    "frame_5499.jpg": "```python\nclass FeedForward(nn.Module):\n    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n    \n    def __init__(self, n_embd):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(n_embd, n_embd),\n            nn.ReLU(),\n            nn.Linear(n_embd, n_embd),\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\nclass Block(nn.Module):\n    \"\"\" Transformer block: communication followed by computation \"\"\"\n\n    def __init__(self, n_embd, n_head):\n        # n_embd: embedding dimension, n_head: the number of heads we'd like\n        super().__init__()\n        head_size = n_embd // n_head\n        self.sa = MultiHeadAttention(n_head, head_size)\n        self.ffwd = FeedForward(n_embd)\n\n    def forward(self, x):\n        x = x + self.sa(x)\n        x = x + self.ffwd(x)\n        return x\n```\n",
    "frame_5500.jpg": "```python\nclass Block(nn.Module):\n    def __init__(self, n_embd, n_head):\n        # n_embd: embedding dimension, n_head: the number of heads we'd like\n        super().__init__()\n        head_size = n_embd // n_head\n        self.sa = MultiHeadAttention(n_head, head_size)\n        self.ffwd = FeedForward(n_embd)\n\n    def forward(self, x):\n        x = x + self.sa(x)\n        x = x + self.ffwd(x)\n        return x\n\nclass BigramLanguageModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        # each token directly reads off the logits for the next token from a lookup table\n        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n        self.blocks = nn.Sequential(\n            Block(n_embd, n_head=4),\n            Block(n_embd, n_head=4)\n        )\n```",
    "frame_5502.jpg": "```python\nclass FeedForward(nn.Module):\n    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n    \n    def __init__(self, n_embd):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(n_embd, n_embd),\n            nn.ReLU(),\n            nn.Linear(n_embd, n_embd),\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\nclass Block(nn.Module):\n    \"\"\" Transformer block: communication followed by computation \"\"\"\n\n    def __init__(self, n_embd, n_head):\n        f_embd = n_embd // n_head  # embedding dimension, n_head: the number of heads we'd like\n        super().__init__()\n        head_size = n_embd // n_head\n        self.sa = MultiHeadAttention(n_head, head_size)\n        self.ffwd = FeedForward(n_embd)\n\n    def forward(self, x):\n        x = x + self.sa(x)\n        x = x + self.ffwd(x)\n        return x\n```\n",
    "frame_5505.jpg": "Code NA",
    "frame_5534.jpg": "```python\nclass FeedForward(nn.Module):\n    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n    \n    def __init__(self, n_embd):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(n_embd, 4 * n_embd),\n            nn.ReLU(),\n            nn.Linear(4 * n_embd, n_embd),\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\nclass Block(nn.Module):\n    \"\"\" Transformer block: communication followed by computation \"\"\"\n    \n    def __init__(self, n_embd, n_head):\n        # n_embd: embedding dimension, n_head: the number of heads we'd like\n        super().__init__()\n        head_size = n_embd // n_head\n        self.sa = MultiHeadAttention(n_embd, head_size)\n        self.ffwd = FeedForward(n_embd)\n\n    def forward(self, x):\n        x = self.sa(x)\n        x = self.ffwd(x)\n        return x\n```",
    "frame_5559.jpg": "```python\nclass FeedForward(nn.Module):\n    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n    \n    def __init__(self, n_embd):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(n_embd, 4 * n_embd),\n            nn.ReLU(),\n            nn.Linear(4 * n_embd, n_embd),\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\nclass Block(nn.Module):\n    \"\"\" Transformer block: communication followed by computation \"\"\"\n    \n    def __init__(self, n_embd, n_head):\n        # n_embd: embedding dimension, n_head: the number of heads we'd like\n        super().__init__()\n        head_size = n_embd // n_head\n        self.sa = MultiHeadAttention(n_embd, head_size)\n        self.ffwd = FeedForward(n_embd)\n\n    def forward(self, x):\n        x = self.sa(x)\n        x = self.ffwd(x)\n        return x\n```",
    "frame_5572.jpg": "Code NA",
    "frame_5594.jpg": "```python\nwith torch.no_grad():  # pass the training set through\n    emb = c(Xtr)\n    embact = emb.view(1, -1)  # ...\n\n# calibrate the batch norm at the end of training\n@torch.no_grad()  # this decorator disables gradient tracking\ndef split_loss(split):\n    x, y = ...\n    # train: (Xtr, Ytr), val: (Xdev, Ydev), test: (Xte, Yte)\n    \n    emb = c(X) / N, block_size, n_embd)\n    embact = emb.view(1, -1)  # ...\n    ...\n```\n",
    "frame_5595.jpg": "```python\nimport torch\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt  # for making figures\n\n# read in all the words\nwords = open('names.txt', 'r').read().splitlines()\nwords[:8]\n\nlen(words)  # 3203\n\n# build the vocabulary of characters and mappings to/from integers\nchars = sorted(list(set(''.join(words))))\nstoi = {s: i for i, s in enumerate(chars)}\nstoi[''] = 1\nitos = {i: s for i, s in stoi.items()}\nvocab_size = len(itos)\nprint(vocab_size)\n```",
    "frame_5597.jpg": "```\n# build the vocabulary of characters and mappings to/from integers\nchars = sorted(list(set(''.join(words))))\nstoi = { ch:i for i,ch in enumerate(chars) }\nitos = { i:s for i,s in enumerate(chars) }\nvocab_size = len(stoi)\nprint(itos)\nprint(vocab_size)\n\n# build the dataset\nblock_size = 3 # context length; how many characters do we take to predict the next one?\ndef build_dataset(words):\n    X, Y = [], []\n    for w in words:\n        context = [0] * block_size\n        for ch in w + ' ':\n            ix = stoi[ch]\n            X.append(context)\n            Y.append(ix)\n            context = context[1:] + [ix] # crop and append\n    X = torch.tensor(X)\n    Y = torch.tensor(Y)\n    print(X.shape, Y.shape)\n    return X, Y\n\nimport random\nrandom.seed(42)\nrandom.shuffle(words)\nn1 = int(0.8*len(words))\nn2 = int(0.9*len(words))\nXtr, Ytr = build_dataset(words[:n1]) # 80%\nXdev, Ydev = build_dataset(words[n1:n2]) # 10%\nXte, Yte = build_dataset(words[n2:]) # 10%\n\ntorch.Size([182625, 3]) , torch.Size([182625])\ntorch.Size([2255, 3]) , torch.Size([2255])\ntorch.Size([2266, 3]) , torch.Size([2266])\n\n# MLP revisited\nn_embd = 10 # the dimensionality of the character embedding vectors\nn_hidden = 200 # the number of neurons in the hidden layer of the MLP\n\ng = torch.Generator().manual_seed(2147483647) # for reproducibility\nc = torch.randn((n_embd, vocab_size), generator=g)\nw1 = torch.randn((n_hidden, n_embd), generator=g) * (5/3) / (n_embd * block_size)**0.5 # 0.2\nb1 = torch.randn(n_hidden, generator=g) * 0.01\n```",
    "frame_5598.jpg": "```python\n# update\nlr = 0.1 if i < 100000 else 0.01  # step learning rate decay\nfor p in parameters:\n    p.data += -lr * p.grad\n\n# track stats\nif i % 10000 == 0:  # print every once in a while\n    print(f'{i}/ {max_steps:7d}; {loss.item():.4f}')\n    loss_values.append(loss.log().item())\n\nplt.plot(loss_values)\n\n# calibrate the batch norm at the end of training\nwith torch.no_grad():\n    # pass the training set through\n    emb = emb(X_tr)\n    embcat = emb.view(emb.shape[0], -1)\n```",
    "frame_5599.jpg": "```python\nemb = C(X)  # (N, block_size, n_embd)\nemb = emb.view(emb.shape[0], -1)  # concat into (N, block_size * n_embd)\nlogits = h @ W2 + b2\nloss = F.cross_entropy(logits, y)\nprint(split, loss.item())\n\nclass Linear:\n    def __init__(self, fan_in, fan_out, bias=True):\n```",
    "frame_5601.jpg": "```python\nclass BatchNorm1d:\n    def __init__(self, dim, eps=1e-5, momentum=0.1):\n        self.eps = eps\n        self.momentum = momentum\n        self.training = True\n        self.parameters = (trained with backprop)\n        self.gamma = torch.ones(dim)\n        self.beta = torch.zeros(dim)\n        self.buffers = (trained with backprop)\n        self.running_mean = torch.zeros(dim)\n        self.running_var = torch.ones(dim)\n\n    def __call__(self, x):\n        # calculate the forward pass\n        if self.training:\n            xmean = x.mean(0, keepdim=True)  # batch mean\n            xvar = x.var(0, keepdim=True)    # batch variance\n        else:\n            xmean = self.running_mean\n            xvar = self.running_var\n        \n        xhat = (x - xmean) / torch.sqrt(xvar + self.eps)  # normalize to unit variance\n        self.out = xhat * self.gamma + self.beta\n        \n        # update the buffers\n        if self.training:\n            with torch.no_grad():\n                self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * xmean\n                self.running_var = (1 - self.momentum) * self.running_var + self.momentum * xvar\n        \n        return self.out\n\n    def parameters(self):\n        return (self.gamma, self.beta)\n\nclass Tanh:\n    def __call__(self, x):\n        self.out = torch.tanh(x)\n        return self.out\n\nn_embd = 10  # the dimensionality of the character embedding vectors\nn_hidden = 100  # number of neurons in the hidden layer of the MLP\ng = torch.Generator().manual_seed(2147483647)  # for reproducibility\n\nc = torch.rand((vocab_size, n_embd), generator=g)\nlayers = [\n    Linear(n_embd, n_hidden, bias=False),\n    BatchNorm1d(n_hidden),\n    Tanh(),\n    Linear(n_hidden, n_hidden, bias=False),\n    BatchNorm1d(n_hidden),\n    Tanh(),\n    Linear(n_hidden, n_hidden, bias=False),\n    BatchNorm1d(n_hidden),\n    Tanh(),\n]\n```",
    "frame_5604.jpg": "Code NA",
    "frame_5605.jpg": "```python\nclass BatchNorm1d:\n    def __init__(self, dim, eps=1e-5, momentum=0.1):\n        self.eps = eps\n        self.momentum = momentum\n        self.training = True\n        # parameters (trained with backprop)\n        self.gamma = torch.zeros(dim)\n        self.beta = torch.zeros(dim)\n        self.running_mean = torch.zeros(dim)\n        self.running_var = torch.ones(dim)\n\n    def __call__(self, x):\n        # calculate the forward pass\n        if self.training:\n            xmean = x.mean(0, keepdim=True)  # batch mean\n            xvar = x.var(0, keepdim=True)  # batch variance\n        else:\n            xmean = self.running_mean\n            xvar = self.running_var\n            \n        xhat = (x - xmean) / torch.sqrt(xvar + self.eps)  # normalize to unit variance\n        self.out = self.gamma * xhat + self.beta  # update the buffers\n        if self.training:\n            with torch.no_grad():\n                self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * xmean\n                self.running_var = (1 - self.momentum) * self.running_var + self.momentum * xvar\n            \n        return self.out\n\n    def parameters(self):\n        return [self.gamma, self.beta]\n\nclass Tanh:\n    def __call__(self, x):\n        self.out = torch.tanh(x)\n        return self.out\n\n    def parameters(self):\n        return []\n\nn_embd = 10  # the dimensionality of the character embedding vectors\nn_hidden = 100  # the number of neurons in the hidden layer of the MLP\ng = torch.Generator().manual_seed(2147483647)  # for reproducibility\n\nc = torch.randn((vocab_size, n_embd), generator=g)\n\nlayers = [\n    Linear(n_embd, n_hidden, bias=False), \n    BatchNorm1d(n_hidden), \n    Tanh(), \n    Linear(n_hidden, n_hidden, bias=False), \n    BatchNorm1d(n_hidden), \n    Tanh(), \n    Linear(n_hidden, vocab_size, bias=False), \n]\n```",
    "frame_5607.jpg": "Here's the extracted developer code from the image:\n\n```python\nclass BatchNorm1d:\n    def __init__(self, dim, eps=1e-5, momentum=0.1):\n        self.eps = eps\n        self.momentum = momentum\n        self.training = True\n        self.parameters (trained with backprop)\n        self.gamma = torch.zeros(dim)\n        self.beta = torch.zeros(dim)\n        self.running_mean = torch.zeros(dim)\n        self.running_var = torch.ones(dim)\n\n    def __call__(self, x):\n        # calculate the forward pass\n        if self.training:\n            xmean = x.mean(0, keepdim=True)  # batch mean\n            xvar = x.var(0, keepdim=True)    # batch variance\n        else:\n            xmean = self.running_mean\n            xvar = self.running_var\n\n        xhat = (x - xmean) / torch.sqrt(xvar + self.eps)  # normalize to unit variance\n        self.out = self.gamma * xhat + self.beta  # update the output\n\n        if self.training:\n            with torch.no_grad():\n                self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * xmean\n                self.running_var = (1 - self.momentum) * self.running_var + self.momentum * xvar\n\n        return self.out\n\n    def parameters(self):\n        return [self.gamma, self.beta]\n\nclass Tanh:\n    def __call__(self, x):\n        self.out = torch.tanh(x)\n        return self.out\n\n    def parameters(self):\n        return []\n    \nn_embd = 10  # the dimensionality of the character embedding vectors\nn_hidden = 100  # the number of neurons in the hidden layer of the MLP\ng = torch.Generator().manual_seed(2147483647)  # for reproducibility\n\nc = torch.randn((vocab_size, n_embd), generator=g)\n\nlayers = [\n    Linear(n_embd, n_hidden, bias=False), \n    BatchNorm1d(n_hidden), \n    Tanh(),\n    Linear(n_hidden, n_hidden, bias=False), \n    BatchNorm1d(n_hidden), \n    Tanh(),\n    Linear(n_hidden, n_hidden, bias=False), \n    BatchNorm1d(n_hidden), \n    Tanh(),\n    Linear(n_hidden, vocab_size)\n]\n```\n\nThis represents the code found in the image.",
    "frame_5609.jpg": "```python\nclass BatchNorm1d:\n    def __init__(self, dim, eps=1e-5, momentum=0.1):\n        self.eps = eps\n        self.momentum = momentum\n        self.training = True\n        self.parameters = (self.weight, self.bias) if self.bias is None else []\n        self.gamma = torch.zeros(dim)\n        self.beta = torch.zeros(dim)\n        self.running_mean = torch.zeros(dim)\n        self.running_var = torch.ones(dim)\n\n    def __call__(self, x):\n        # calculate the forward pass\n        if self.training:\n            xmean = x.mean(0, keepdim=True)  # batch mean\n            xvar = x.var(0, keepdim=True)  # batch variance\n        else:\n            xmean = self.running_mean\n            xvar = self.running_var\n\n        xhat = (x - xmean) / torch.sqrt(xvar + self.eps)  # normalize to unit variance\n        self.out = self.gamma * xhat + self.beta  # update the buffers\n        if self.training:\n            with torch.no_grad():\n                self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * xmean\n                self.running_var = (1 - self.momentum) * self.running_var + self.momentum * xvar\n\n        return self.out\n\n    def parameters(self):\n        return [self.gamma, self.beta]\n\nclass Tanh:\n    def __call__(self, x):\n        self.out = torch.tanh(x)\n        return self.out\n\n    def parameters(self):\n        return []\n\nn_embd = 10  # the dimensionality of the character embedding vectors\nn_hidden = 100  # the number of neurons in the hidden layer of the MLP\ng = torch.Generator().manual_seed(2147483647)  # for reproducibility\n\nc = torch.rand((vocab_size, n_embd), generator=g)\nlayers = [\n    Linear(n_embd, n_hidden, bias=False),\n    Tanh(),\n    Linear(n_hidden, n_hidden, bias=False),\n    BatchNorm1d(n_hidden),\n    Tanh(),\n    Linear(n_hidden, vocab_size),\n]\n```",
    "frame_5616.jpg": "```python\n# buffers (trained with a running 'momentum update')\nself.running_mean = torch.zeros(dim)\nself.running_var = torch.ones(dim)\n\ndef __call__(self, x):\n    # calculate the forward pass\n    if self.training:\n        xmean = x.mean(0, keepdim=True)  # batch mean\n        xvar = x.var(0, keepdim=True)    # batch variance\n    else:\n        xmean = self.running_mean\n        xvar = self.running_var\n        xhat = (x - xmean) / torch.sqrt(xvar + self.eps)  # normalize to unit variance\n    self.out = self.gamma * xhat + self.beta\n\n    # update the buffers\n    if self.training:\n        with torch.no_grad():\n            self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * xmean\n            self.running_var = (1 - self.momentum) * self.running_var + self.momentum * xvar\n    return self.out\n\ndef parameters(self):\n    return [self.gamma, self.beta]\n\ntorch.manual_seed(1337)\nmodule = BatchNorm1d(100)  # batch size 32 of 100-dimensional vectors\nx = torch.randn(32, 100)\nx.shape\n\ntorch.Size([32, 100])\n```",
    "frame_5617.jpg": "```python\nclass BatchNorm1d:\n    def __init__(self, dim, eps=1e-5, momentum=0.1):\n        self.eps = eps\n        self.momentum = momentum\n        self.training = True\n        # parameters (trained with backprop)\n        self.gamma = torch.ones(dim)\n        self.beta = torch.zeros(dim)\n        # buffers (trained with a running 'momentum update')\n        self.running_mean = torch.zeros(dim)\n        self.running_var = torch.ones(dim)\n\n    def __call__(self, x):\n        # calculate the forward pass\n        if self.training:\n            xmean = x.mean(0, keepdim=True)  # batch mean\n            xvar = x.var(0, keepdim=True)  # batch variance\n        else:\n            xmean = self.running_mean\n            xvar = self.running_var\n            xhat = (x - xmean) / torch.sqrt(xvar + self.eps)  # normalize to unit variance\n            out = self.gamma * xhat + self.beta\n\n        # update the buffers\n        if self.training:\n            with torch.no_grad():\n                self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * xmean\n                self.running_var = (1 - self.momentum) * self.running_var + self.momentum * xvar\n        return out\n\n    def parameters(self):\n        return [self.gamma, self.beta]\n\ntorch.manual_seed(1373)\n```",
    "frame_5621.jpg": "```python\nself.beta = torch.zeros(dim)\nself.running_mean = torch.zeros(dim)\nself.running_var = torch.ones(dim)\n\ndef __call__(self, x):\n    if self.training:\n        xmean = x.mean(0, keepdim=True)  # batch mean\n        xvar = x.var(0, keepdim=True)    # batch variance\n    else:\n        xmean = self.running_mean\n        xvar = self.running_var\n    xhat = (x - xmean) / torch.sqrt(xvar + self.eps)  # normalize to unit variance\n    self.out = self.gamma * xhat + self.beta\n\n    if self.training:\n        with torch.no_grad():\n            self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * xmean\n            self.running_var = (1 - self.momentum) * self.running_var + self.momentum * xvar\n\n    return self.out\n\ndef parameters(self):\n    return [self.gamma, self.beta]\n\ntorch.manual_seed(1337)\nmodule = BatchNorm1d(100)\nx = torch.rand(32, 100)  # batch size of 32 of 100-dimensional vectors\nx.shape\n\ntorch.Size([32, 100])\n```",
    "frame_5632.jpg": "```python\nxmean = self.running_mean\nxvar = self.running_var\nxhat = (x - xmean) / torch.sqrt(xvar + self.eps)  # normalize to unit variance\nself.out = self.gamma * xhat + self.beta\n\n# update the buffers\nif self.training:\n    with torch.no_grad():\n        self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * xmean\n        self.running_var = (1 - self.momentum) * self.running_var + self.momentum * xvar\nreturn self.out\n\ndef parameters(self):\n    return [self.gamma, self.beta]\n\ntorch.manual_seed(1337)\nmodule = BatchNorm1d(100)\nx = torch.randn(32, 100)  # batch size 32 of 100-dimensional vectors\nx = module(x)\nx.shape\n\ntorch.Size([32, 100])\nk[i, 0].mean(), x[0, :].std()  # mean,std of one feature across all batch inputs\n(tensor(7.4506e-09), tensor(1.0000))\n\nk[182][0].mean(), x[0, :].std()  # mean,std of a single input from the batch, of its features\n(tensor(0.0411), tensor(1.0431))\n```",
    "frame_5649.jpg": "```python\nxmean = self.running_mean\nxvar = self.running_var\nxhat = (x - xmean) / torch.sqrt(xvar + self.eps)  # normalize to unit variance\nself.out = self.gamma * xhat + self.beta\n# update the buffers\nif self.training:\n    with torch.no_grad():\n        self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * xmean\n        self.running_var = (1 - self.momentum) * self.running_var + self.momentum * xvar\nreturn self.out\n\ndef parameters(self):\n    return [self.gamma, self.beta]\n\ntorch.manual_seed(1337)\nmodule = BatchNorm1d(100)\nx = torch.randn(32, 100)  # batch size 32 of 100-dimensional vectors\nx = module(x)\nx.shape\n\ntorch.Size([32, 100])\nx[0, 0].mean(), x[0, 0].std()  # mean,std of one feature across all batch inputs\n(tensor(7.4506e-09), tensor(1.0000))\n\nx[0, 0].mean(), x[0, 0].std()  # mean,std of a single input from the batch, of its features\n(tensor(0.0411), tensor(1.0431))\n```",
    "frame_5653.jpg": "```python\ndef __call__(self, x):\n    # calculate the forward pass\n    if self.training:\n        xmean = x.mean(0, keepdim=True)  # batch mean\n        xvar = x.var(0, keepdim=True)  # batch variance\n    else:\n        xmean = self.running_mean\n        xvar = self.running_var\n        xhat = (x - xmean) / torch.sqrt(xvar + self.eps)  # normalize to unit variance\n        self.out = self.gamma * xhat + self.beta\n    \n    # update the buffers\n    if self.training:\n        with torch.no_grad():\n            self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * xmean\n            self.running_var = (1 - self.momentum) * self.running_var + self.momentum * xvar\n\n    return self.out\n\ndef parameters(self):\n    return [self.gamma, self.beta]\n\ntorch.manual_seed(1337)\nmodule = BatchNorm1d(100)\nx = torch.randn(32, 100)  # batch size 32 of 100-dimensional vectors\nx.shape\ntorch.Size([32, 100])\n```",
    "frame_5663.jpg": "```python\nself.beta = torch.zeros(dim)\n# buffers (trained with a running 'momentum update')\nself.running_mean = torch.zeros(dim)\nself.running_var = torch.ones(dim)\n\ndef __call__(self, x):\n    # calculate the forward pass\n    if self.training:\n        xmean = x.mean(1, keepdim=True)  # batch mean\n        xvar = x.var(1, keepdim=True)  # batch variance\n    else:\n        xmean = self.running_mean\n        xvar = self.running_var\n        xhat = (x - xmean) / torch.sqrt(xvar + self.eps)  # normalize to unit variance\n        self.out = self.gamma * xhat + self.beta\n        # update the buffers\n    if self.training:\n        with torch.no_grad():\n            self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * xmean\n            self.running_var = (1 - self.momentum) * self.running_var + self.momentum * xvar\n    return self.out\n\ndef parameters(self):\n    return [self.gamma, self.beta]\n\ntorch.manual_seed(1337)\nmodule = BatchNorm1d(100)\nx = torch.randn(32, 100)  # batch size 32 of 100-dimensional vectors\nx.shape\n```",
    "frame_5676.jpg": "Here is the extracted developer code from the image:\n\n```python\n# update the buffers\nif self.training:\n    with torch.no_grad():\n        self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * xmean\n        self.running_var = (1 - self.momentum) * self.running_var + self.momentum * xvar\n\nreturn self.out\n\ndef parameters(self):\n    return [self.gamma, self.beta]\n\ntorch.manual_seed(1337)\nmodule = BatchNorm1d(100)\nx = torch.randn(32, 100)  # batch size 32 of 100-dimensional vectors\nx = module(x)\nx.shape\n\ntorch.Size([32, 100])\n\nx[:, 0].mean(), x[:, 0].std()  # mean,std of one feature across all batch inputs\n(tensor(0.1469), tensor(0.8803))\n\nx[0, :].mean(), x[0, :].std()  # mean,std of a single input from the batch, of its features\n(tensor(-9.5367e-09), tensor(1.0000))\n```",
    "frame_5677.jpg": "```python\ndef __call__(self, x):\n    # calculate the forward pass\n    if self.training:\n        xmean = x.mean(1, keepdim=True)  # batch mean\n        xvar = x.var(1, keepdim=True)    # batch variance\n    else:\n        xmean = self.running_mean\n        xvar = self.running_var\n        xhat = (x - xmean) / torch.sqrt(xvar + self.eps)  # normalize to unit variance\n        self.out = self.gamma * xhat + self.beta  # update the buffers\n        if self.training:\n            with torch.no_grad():\n                self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * xmean\n                self.running_var = (1 - self.momentum) * self.running_var + self.momentum * xvar\n    return self.out\n\ndef parameters(self):\n    return [self.gamma, self.beta]\n\ntorch.manual_seed(1337)\nmodule = BatchNorm1d(100)\nx = torch.randn(32, 100)  # batch size 32 of 100-dimensional vectors\nx = module(x)\nx.shape\n\ntorch.Size([32, 100])\nx[i, 0].mean(), x[:, 0].std()  # mean,std of one feature across all batch inputs\n(tensor(0.1469), tensor(0.8803))\n\nx[0, :].mean(), x[0, :].std()  # mean,std of a single input from the batch, of its features\n```",
    "frame_5680.jpg": "```python\ndef __call__(self, x):\n    # calculate the forward pass\n    if self.training:\n        xmean = x.mean(1, keepdim=True)  # batch mean\n        xvar = x.var(1, keepdim=True)    # batch variance\n    else:\n        xmean = self.running_mean\n        xvar = self.running_var\n        xhat = (x - xmean) / torch.sqrt(xvar + self.eps)  # normalize to unit variance\n        self.out = self.gamma * xhat + self.beta\n\n    # update the buffers\n    if self.training:\n        with torch.no_grad():\n            self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * xmean\n            self.running_var = (1 - self.momentum) * self.running_var + self.momentum * xvar\n\n    return self.out\n\ndef parameters(self):\n    return [self.gamma, self.beta]\n\ntorch.manual_seed(1337)\nmodule = BatchNorm1d(100)  # batch size 32 of 100-dimensional vectors\nx = torch.randn(32, 100)\nx = module(x)\nx.shape\n```",
    "frame_5684.jpg": "```python\nif self.training:\n    with torch.no_grad():\n        self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * xmean\n        self.running_var = (1 - self.momentum) * self.running_var + self.momentum * xvar\n```",
    "frame_5685.jpg": "```python\ndef __call__(self, x):\n    # calculate the forward pass\n    if self.training:\n        xmean = x.mean(1, keepdim=True)  # batch mean\n        xvar = x.var(1, keepdim=True)  # batch variance\n    else:\n        xmean = self.running_mean\n        xvar = self.running_var\n        xhat = (x - xmean) / torch.sqrt(xvar + self.eps)  # normalize to unit variance\n        self.out = self.gamma * xhat + self.beta\n        # update the buffers\n        if self.training:\n            with torch.no_grad():\n                self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * xmean\n                self.running_var = (1 - self.momentum) * self.running_var + self.momentum * xvar\n    return self.out\n\ndef parameters(self):\n    return [self.gamma, self.beta]\n\ntorch.manual_seed(1337)\nmodule = BatchNorm1d(100)\nx = torch.randn(32, 100)  # batch size of 32 of 100-dimensional vectors\nx.shape\n```",
    "frame_5687.jpg": "self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * xmean  \nself.running_var = (1 - self.momentum) * self.running_var + self.momentum * xvar  ",
    "frame_5689.jpg": "```python\ndef __call__(self, x):\n    # calculate the forward pass\n    if self.training:\n        xmean = x.mean(1, keepdim=True)  # batch mean\n        xvar = x.var(1, keepdim=True)  # batch variance\n    else:\n        xmean = self.running_mean\n        xvar = self.running_var\n    \n    xhat = (x - xmean) / torch.sqrt(xvar + self.eps)  # normalize to unit variance\n    # update the buffers\n    if self.training:\n        with torch.no_grad():\n            self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * xmean\n            self.running_var = (1 - self.momentum) * self.running_var + self.momentum * xvar\n\n    return self.out\n\ndef parameters(self):\n    return [self.gamma, self.beta]\n\ntorch.manual_seed(1337)\nmodule = BatchNorm1d(100)\nx = torch.randn(32, 100)  # batch size 32 of 100-dimensional vectors\nx.shape\n```",
    "frame_5699.jpg": "```python\nself.training = True\nself.gamma = torch.zeros(dim)\nself.beta = torch.zeros(dim)\nself.running_mean = torch.zeros(dim)\nself.running_var = torch.ones(dim)\n\ndef __call__(self, x):\n    xmean = x.mean(1, keepdim=True)  # batch mean\n    xvar = x.var(1, keepdim=True)    # batch variance\n    xhat = (x - xmean) / torch.sqrt(xvar + self.eps)  # normalize to unit variance\n    self.out = self.gamma * xhat + self.beta\n    return self.out\n\ndef parameters(self):\n    return [self.gamma, self.beta]\n\ntorch.manual_seed(1337)\nmodule = BatchNorm1d(100)\nx = torch.rand(32, 100)  # batch size 32 of 100-dimensional vectors\nx = module(x)\nx.shape\n```",
    "frame_5703.jpg": "```python\nclass BatchNorm1d:\n    def __init__(self, dim, eps=1e-5, momentum=0.1):\n        self.eps = eps\n        self.momentum = momentum\n        self.training = True\n        # parameters (trained with backprop)\n        self.gamma = torch.ones(dim)\n        self.beta = torch.zeros(dim)\n\n    def __call__(self, x):\n        # calculate the forward pass\n        xmean = x.mean(1, keepdim=True)  # batch mean\n        xvar = x.var(1, keepdim=True)  # batch variance\n        xhat = (x - xmean) / torch.sqrt(xvar + self.eps)  # normalize to unit variance\n        out = self.gamma * xhat + self.beta\n        return out\n\n    def parameters(self):\n        return [self.gamma, self.beta]\n\ntorch.manual_seed(1337)\nmodule = BatchNorm1d(100)\nx = torch.randn(32, 100)  # batch size of 32 of 100-dimensional vectors\nx = module(x)\nx.shape\n```",
    "frame_5707.jpg": "```python\nclass BatchNorm1d:\n    def __init__(self, dim, eps=1e-5, momentum=0.1):\n        self.eps = eps\n        # parameters (trained with backprop)\n        self.gamma = torch.ones(dim)\n        self.beta = torch.zeros(dim)\n\n    def __call__(self, x):\n        # calculate the forward pass\n        xmean = x.mean(1, keepdim=True)  # batch mean\n        xvar = x.var(1, keepdim=True)    # batch variance\n        xhat = (x - xmean) / torch.sqrt(xvar + self.eps)  # normalize to unit variance\n        self.out = self.gamma * xhat + self.beta\n        return self.out\n\n    def parameters(self):\n        return [self.gamma, self.beta]\n\ntorch.manual_seed(1337)\nmodule = BatchNorm1d(100)\nx = torch.randn(32, 100)  # batch size of 32 of 100-dimensional vectors\nx = module(x)\nx.shape\n\nx[:, 0].mean(), x[:, 0].std()  # mean,std of one feature across all batch inputs\n```",
    "frame_5712.jpg": "```python\nclass BatchNorm1d:\n    def __init__(self, dim, eps=1e-5, momentum=0.1):\n        self.eps = eps\n        self.gamma = torch.ones(dim)\n        self.beta = torch.zeros(dim)\n\n    def __call__(self, x):\n        # calculate the forward pass\n        xmean = x.mean(dim=True)  # batch mean\n        xvar = x.var(keepdim=True)  # batch variance\n        xhat = (x - xmean) / torch.sqrt(xvar + self.eps)  # normalize to unit variance\n        self.out = self.gamma * xhat + self.beta\n        return self.out\n\n    def parameters(self):\n        return [self.gamma, self.beta]\n\ntorch.manual_seed(1337)\nmodule = BatchNorm1d(100)  # batch size 32 of 100-dimensional vectors\nx = torch.randn(32, 100)\nx.shape\n\nx[:, 0].mean(), x[:, 0].std()  # mean,std of one feature across all batch inputs\n(tensor(0.1469), tensor(0.8803))\n\nk[0, :].mean(), x[0, :].std()  # mean,std of a single input from the batch, of its features\n```",
    "frame_5713.jpg": "```python\nself.eps = eps\nself.gamma = torch.ones(dim)\nself.beta = torch.zeros(dim)\n\ndef __call__(self, x):\n    # calculate the forward pass\n    xmean = x.mean(1, keepdim=True)  # batch mean\n    xvar = x.var(1, keepdim=True)  # batch variance\n    xhat = (x - xmean) / torch.sqrt(xvar + self.eps)  # normalize to unit variance\n    self.out = self.gamma * xhat + self.beta\n    return self.out\n\ndef parameters(self):\n    return [self.gamma, self.beta]\n\ntorch.manual_seed(1337)\nmodule = BatchNorm1d(100)\nx = torch.randn(32, 100)  # batch size of 32 of 100-dimensional vectors\n_= module(x)\nx.shape\n\nx[0, :].mean(), x[0, :].std()  # mean,std of one feature across all batch inputs\n(x[0, :].mean(), x[0, :].std())\nx[0, :].mean(), x[0, :].std()  # mean,std of a single input from the batch, of its features\n```",
    "frame_5714.jpg": "```python\nclass BatchNorm1d:\n\n    def __init__(self, dim, eps=1e-5, momentum=0.1):\n        self.eps = eps\n        self.gamma = torch.ones(dim)\n        self.beta = torch.zeros(dim)\n\n    def __call__(self, x):\n        # calculate the forward pass\n        xmean = x.mean(1, keepdim=True)  # batch mean\n        xvar = x.var(1, keepdim=True)    # batch variance\n        xhat = (x - xmean) / torch.sqrt(xvar + self.eps)  # normalize to unit variance\n        self.out = self.gamma * xhat + self.beta\n        return self.out\n\n    def parameters(self):\n        return [self.gamma, self.beta]\n\ntorch.manual_seed(1337)\nmodule = BatchNorm1d(100)\nx = torch.randn(32, 100)  # batch size 32 of 100-dimensional vectors\nx.shape\n\ntorch.Size([32, 100])\n```",
    "frame_5718.jpg": "Code NA",
    "frame_5754.jpg": "```python\nclass Block(nn.Module):\n    \"\"\" Transformer block: communication followed by computation \"\"\"\n    \n    def __init__(self, n_embd, n_head):\n        # n_embd: embedding dimension, n_head: the number of heads we'd like\n        super().__init__()\n        head_size = n_embd // n_head\n        self.sa = MultiHeadAttention(n_head, head_size)\n        self.fwd = FeedForward(n_embd)\n\n    def forward(self, x):\n        x = x + self.sa(x)\n        x = x + self.fwd(x)\n        return x\n\n# super simple bigram model\nclass BigramLanguageModel(nn.Module):\n    def __init__(self):\n        # each token directly reads off the logits for the next token from a lookup table\n        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n```",
    "frame_5756.jpg": "```python\nclass Block(nn.Module):\n    \"\"\" Transformer block: communication followed by computation \"\"\"\n\n    def __init__(self, n_embd, n_head):\n        # n_embd: embedding dimension, n_head: the number of heads we'd like\n        super().__init__()\n        head_size = n_embd // n_head\n        self.sa = MultiHeadAttention(n_head, head_size)\n        self.fwd = FeedForward(n_embd)\n\n    def forward(self, x):\n        x = x + self.sa(x)\n        x = x + self.fwd(x)\n        return x\n\n# super simple bigram model\nclass BigramLanguageModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        # each token directly reads off the logits for the next token from a lookup table\n        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n```",
    "frame_5759.jpg": "```python\nclass Block(nn.Module):\n    \"\"\" Transformer block: communication followed by computation \"\"\"\n    \n    def __init__(self, n_embd, n_head):\n        # n_embd: embedding dimension, n_head: the number of heads we'd like\n        head_size = n_embd // n_head\n        self.sa = MultiHeadAttention(n_head, head_size)\n        self.fwd = FeedForward(n_embd)\n        self.ln1 = nn.LayerNorm(n_embd)\n\n    def forward(self, x):\n        x = x + self.sa(x)\n        x = x + self.fwd(x)\n        return x\n\nclass BigramLanguageModel(nn.Module):\n    \n    def __init__(self):\n        super().__init__()\n        # each token directly reads off the logits for the next token from a lookup table\n        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n```",
    "frame_5772.jpg": "```python\nclass Block(nn.Module):\n    def __init__(self, n_embd, n_head):\n        super().__init__()\n        self.sa = MultiHeadAttention(n_head, head_size)\n        self.fwd = FeedForward(n_embd)\n        self.ln1 = nn.LayerNorm(n_embd)\n        self.ln2 = nn.LayerNorm(n_embd)\n\n    def forward(self, x):\n        x = x + self.ln1(x)\n        x = x + self.fwd(x)\n        return x\n```",
    "frame_5775.jpg": "```python\nclass Block(nn.Module):\n    \"\"\"***** Transformer block: communication followed by computation *****\"\"\"\n    \n    def __init__(self, n_embd, n_head):\n        # n_embd: embedding dimension, n_head: the number of heads we'd like\n        super().__init__()\n        head_size = n_embd // n_head\n        self.sa = MultiHeadAttention(n_head, head_size)\n        self.fwd = FeedForward(n_embd)\n        self.ln1 = nn.LayerNorm(n_embd)\n        self.ln2 = nn.LayerNorm(n_embd)\n\n    def forward(self, x):\n        x = x + self.sa(self.ln1(x))\n        x = x + self.fwd(self.ln2(x))\n        return x\n\n# super simple bigram model\nclass BigramLanguageModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        # each token directly reads off the logits for the next token from a lookup table\n        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n```",
    "frame_5788.jpg": "Code NA",
    "frame_5798.jpg": "```python\nclass Block(nn.Module):\n    \"\"\" Transformer block: communication followed by computation \"\"\"\n    \n    def __init__(self, n_embd, n_head):\n        super().__init__()\n        # n_embd: embedding dimension, n_head: the number of heads we'd like\n        head_size = n_embd // n_head\n        self.sa = MultiHeadAttention(n_head, head_size)\n        self.fwd = FeedForward(n_embd)\n        self.ln1 = nn.LayerNorm(n_embd)\n        self.ln2 = nn.LayerNorm(n_embd)\n\n    def forward(self, x):\n        x = x + self.sa(self.ln1(x))\n        x = x + self.fwd(self.ln2(x))\n        return x\n\n# super simple bigram model\nclass BigramLanguageModel(nn.Module):\n    \n    def __init__(self):\n        super().__init__()\n        # each token directly reads off the logits for the next token from a lookup table\n```",
    "frame_5810.jpg": "Code NA",
    "frame_5811.jpg": "```python\nclass BatchNorm1d:\n    def __init__(self, dim, eps=1e-5, momentum=0.1):\n        self.eps = eps\n        self.gamma = torch.ones(dim)\n        self.beta = torch.zeros(dim)\n\n    def __call__(self, x):\n        # calculate the forward pass\n        xmean = x.mean(1, keepdim=True)  # batch mean\n        xvar = x.var(1, keepdim=True)    # batch variance\n        xhat = (x - xmean) / torch.sqrt(xvar + self.eps)  # normalize to unit variance\n        self.out = self.gamma * xhat + self.beta\n        return self.out\n\n    def parameters(self):\n        return [self.gamma, self.beta]\n\ntorch.manual_seed(1337)\nmodule = BatchNorm1d(100)\nx = torch.randn(32, 100)  # batch size 32 of 100-dimensional vectors\nx.shape\n```",
    "frame_5819.jpg": "```python\nclass Block(nn.Module):\n    \"\"\"***** Transformer block: communication followed by computation *****\"\"\"\n\n    def __init__(self, n_embd, n_head):\n        # n_embd: embedding dimension, n_head: the number of heads we'd like\n        super().__init__()\n        head_size = n_embd // n_head\n        self.sa = MultiHeadAttention(n_embd)\n        self.ffwd = FeedForward(n_embd)\n        self.ln1 = nn.LayerNorm(n_embd)\n        self.ln2 = nn.LayerNorm(n_embd)\n\n    def forward(self, x):\n        x = x + self.sa(self.ln1(x))\n        x = x + self.ffwd(self.ln2(x))\n        return x\n\n# super simple bigram model\nclass BigramLanguageModel(nn.Module):\n\n    def __init__(self):\n        super().__init__()\n        # each token directly reads off the logits for the next token from a lookup table\n```",
    "frame_5823.jpg": "```python\nclass Block(nn.Module):\n    \"\"\"***** Transformer block: communication followed by computation *****\"\"\"\n\n    def __init__(self, n_embd, n_head):\n        self.n_embd = n_embd  # n_embd: embedding dimension, n_head: the number of heads we'd like\n        super().__init__()\n        head_size = n_embd // n_head\n        self.sa = MultiHeadAttention(n_embd)\n        self.ffwd = FeedForward(n_embd)\n        self.ln1 = nn.LayerNorm(n_embd)\n        self.ln2 = nn.LayerNorm(n_embd)\n\n    def forward(self, x):\n        x = x + self.sa(self.ln1(x))\n        x = x + self.ffwd(self.ln2(x))\n        return x\n\n# super simple bigram model\nclass BigramLanguageModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        # each token directly reads off the logits for the next token from a lookup table\n```",
    "frame_5833.jpg": "```\nclass BigramLanguageModel(nn.Module):\n\n    def __init__(self):\n        super().__init__()\n        # each token directly reads off the logits for the next token from a lookup table\n        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n        self.blocks = nn.Sequential(\n            Block(n_embd, n_head=4),\n            Block(n_embd, n_head=4),\n            Block(n_embd, n_head=4),\n            nn.LayerNorm(n_embd),\n        )\n\n        self.lm_head = nn.Linear(n_embd, vocab_size)\n\n    def forward(self, idx, targets=None):\n        B, T = idx.shape\n        tok_emb = self.token_embedding_table(idx)  # (B, T, C)\n        pos_emb = self.position_embedding_table(torch.arange(T, device=device))  # (T, C)\n        x = tok_emb + pos_emb\n        x = self.blocks(x)\n        return x\n```",
    "frame_5858.jpg": "Code NA",
    "frame_5863.jpg": "```python\nclass BigramLanguageModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        # each token directly reads off the logits for the next token from a lookup table\n        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n        self.blocks = nn.Sequential(\n            Block(n_embd, n_head=4),\n            Block(n_embd, n_head=4),\n            Block(n_embd, n_head=4),\n        )\n        nn.LayerNorm(n_embd)\n        self.lm_head = nn.Linear(n_embd, vocab_size)\n\n    def forward(self, idx, targets=None):\n        B, T = idx.shape\n        token_emb = self.token_embedding_table(idx)  # (B, T, n_embd)\n        pos_emb = self.position_embedding_table(torch.arange(T, device=device))  # (T, n_embd)\n        x = token_emb + pos_emb\n        x = self.blocks(x)\n        return x\n```",
    "frame_5870.jpg": "```\ndef forward(self, x):\n    x = x + self.fwd(self.ln1(x))\n    return x\n\nclass BigramLanguageModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n        self.blocks = nn.Sequential(*[Block(n_embd, n_head, _ for _ in range(n_layer)])\n        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n        self.lm_head = nn.Linear(n_embd, vocab_size)\n\n    def forward(self, idx, targets=None):\n        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n        x = tok_emb + pos_emb # (B,T,C)\n        x = self.ln_f(x) # (B,T,vocab_size)\n        logits = self.lm_head(x) @ (B,T,vocab_size)\n        \n        if targets is None:\n            loss = None\n        else:\n            B, T, C = logits.shape\n            targets = logits.view(B*T, C)\n            loss = F.cross_entropy(logits, targets)\n```\n",
    "frame_5890.jpg": "```python\ndef __init__(self, num_heads, head_size):\n    super().__init__()\n    self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n    self.proj = nn.Linear(n_embd, n_embd)\n    self.dropout = nn.Dropout(dropout)\n\ndef forward(self, x):\n    out = torch.cat([h(x) for h in self.heads], dim=-1)\n    out = self.dropout(self.proj(out))\n    return out\n\nclass FeedForward(nn.Module):\n    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n    \n    def __init__(self, n_embd):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(n_embd, 4 * n_embd),\n            nn.ReLU(),\n            nn.Linear(4 * n_embd, n_embd),\n            nn.Dropout(dropout),\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\nclass Block(nn.Module):\n    \"\"\" Transformer block: communication followed by computation \"\"\"\n\n    def __init__(self, n_embd, n_head):\n        # n_embd: embedding dimension, n_head: the number of heads we'd like\n        super().__init__()\n        head_size = n_embd // n_head\n        self.sa = MultiHeadAttention(n_head, head_size)\n        self.ffwd = FeedForward(n_embd)\n```",
    "frame_5892.jpg": "```python\nclass MultiHeadAttention(nn.Module):\n    \"\"\" multiple heads of self-attention in parallel \"\"\"\n    \n    def __init__(self, num_heads, head_size):\n        super().__init__()\n        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n        self.proj = nn.Linear(n_embd, n_embd)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        out = torch.cat([h(x) for h in self.heads], dim=-1)\n        out = self.dropout(self.proj(out))\n        return out\n\nclass FeedForward(nn.Module):\n    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n    \n    def __init__(self, n_embd):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(n_embd, 4 * n_embd),\n            nn.ReLU(),\n            nn.Linear(4 * n_embd, n_embd),\n            nn.Dropout(dropout),\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\nclass Block(nn.Module):\n    \"\"\" Transformer block: communication followed by computation \"\"\"\n    \n    def __init__(self, n_embd, n_head):\n        super().__init__()\n        ...\n```\n",
    "frame_5901.jpg": "```python\nclass MultiHeadAttention(nn.Module):\n    \"\"\" multiple heads of self-attention in parallel \"\"\"\n    \n    def __init__(self, num_heads, head_size):\n        super().__init__()\n        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n        self.proj = nn.Linear(n_embd, n_embd)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        out = torch.cat([h(x) for h in self.heads], dim=-1)\n        out = self.dropout(self.proj(out))\n        return out\n\n\nclass FeedForward(nn.Module):\n    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n    \n    def __init__(self, n_embd):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(n_embd, 4 * n_embd),\n            nn.ReLU(),\n            nn.Linear(4 * n_embd, n_embd),\n            nn.Dropout(dropout),\n        )\n\n    def forward(self, x):\n        return self.net(x)\n```",
    "frame_5903.jpg": "```python\nclass FeedForward(nn.Module):\n    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n    \n    def __init__(self, n_embd):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(n_embd, 4 * n_embd),\n            nn.ReLU(),\n            nn.Linear(4 * n_embd, n_embd),\n            nn.Dropout(dropout),\n        )\n\n    def forward(self, x):\n        return self.net(x)\n```",
    "frame_5904.jpg": "```\nclass MultiHeadAttention(nn.Module):\n    \"\"\" multiple heads of self-attention in parallel \"\"\"\n    \n    def __init__(self, num_heads, head_size):\n        super().__init__()\n        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n        self.proj = nn.Linear(n_embd, n_embd)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        out = torch.cat([h(x) for h in self.heads], dim=-1)\n        out = self.dropout(self.proj(out))\n        return out\n\nclass FeedForward(nn.Module):\n    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n    \n    def __init__(self, n_embd):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(n_embd, 4 * n_embd),\n            nn.ReLU(),\n            nn.Linear(4 * n_embd, n_embd),\n            nn.Dropout(dropout),\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\nclass Block(nn.Module):\n    \"\"\" Transformer block: communication followed by computation \"\"\"\n    \n    def __init__(self, n_embd, n_head):\n```\n",
    "frame_5910.jpg": "```python\nclass Head(nn.Module):\n    \"\"\" one head of self-attention \"\"\"\n\n    def __init__(self, head_size):\n        super().__init__()\n        self.key = nn.Linear(n_embd, head_size, bias=False)\n        self.query = nn.Linear(n_embd, head_size, bias=False)\n        self.value = nn.Linear(n_embd, head_size, bias=False)\n        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        B, T, C = x.shape\n        k = self.key(x)  # (B, T, C)\n        q = self.query(x)  # (B, T, C)\n        # compute attention scores \"affinities\"\n        wei = q @ k.transpose(-2, -1) * C**-0.5  # (B, T, C) @ (B, C, T) -> (B, T, T)\n        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n        wei = F.softmax(wei, dim=-1)\n        v = self.value(x)  # (B, T, C)\n        # perform the weighted aggregation of the values\n        out = wei @ v  # (B, T, C)\n        return out\n```\n",
    "frame_5925.jpg": "Code NA",
    "frame_5969.jpg": "```python\nclass Head(nn.Module):\n    \"\"\" one head of self-attention \"\"\"\n\n    def __init__(self, head_size):\n        super().__init__()\n        self.key = nn.Linear(n_embd, head_size, bias=False)\n        self.query = nn.Linear(n_embd, head_size, bias=False)\n        self.value = nn.Linear(n_embd, head_size, bias=False)\n        self.register_buffer('tri', torch.tril(torch.ones(block_size, block_size)))\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        B, T, C = x.shape\n        k = self.key(x)  # (B, T, C)\n        q = self.query(x)  # (B, T, C)\n        wei = q @ k.transpose(-2, -1) * C**-0.5  # (B, T, T)\n        wei = wei.masked_fill(self.tri[:T, :T] == 0, float('-inf'))\n        wei = self.dropout(wei)\n        v = self.value(x)  # (B, T, C)\n        out = v @ wei.transpose(-2, -1)  # (B, T, C)\n        return out\n```",
    "frame_5976.jpg": "```python\nimport torch\nimport torch.nn as functional as F\n\n# hyperparameters\nbatch_size = 64  # how many independent sequences will we process in parallel?\nblock_size = 256  # what is the maximum context length for predictions?\nmax_iters = 5000\neval_interval = 500\nlearning_rate = 3e-4\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\neval_iters = 200\nn_embd = 384\nn_head = 6\nn_layer = 6\ndropout = 0.2\n\ntorch.manual_seed(1337)\n\n# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\nwith open('input.txt', 'r', encoding='utf-8') as f:\n    text = f.read()\n\n# here are all the unique characters that occur in this text\nchars = sorted(list(set(text)))\nvocab_size = len(chars)\n\n# create a mapping from characters to integers\nstoi = {ch: i for i, ch in enumerate(chars)}\nitos = {i: ch for i, ch in enumerate(chars)}\nencode = lambda s: [stoi[c] for c in s]  # encoder: take a string, output a list of integers\ndecode = lambda l: ''.join([itos[i] for i in l])  # decoder: take a list of integers, output a string\n\n# Train and test splits\ndata = torch.tensor(encode(text), dtype=torch.long)\nn = int(0.9 * len(data))  # first 90% will be train, rest val\n```",
    "frame_6000.jpg": "```python\nimport torch\nimport torch.nn.functional as F\n\n# hyperparameters\nbatch_size = 64  # how many independent sequences will we process in parallel?\nblock_size = 256  # what is the maximum context length for predictions?\nmax_iters = 5000\neval_interval = 500\nlearning_rate = 3e-4\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\neval_iters = 200\nn_embd = 384\nn_head = 6\nn_layer = 6\ndropout = 0.2\n\ntorch.manual_seed(1337)\n\n# get https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\nwith open('input.txt', 'r', encoding='utf-8') as f:\n    text = f.read()\n\n# here are all the unique characters that occur in this text\nchars = sorted(list(set(text)))\nvocab_size = len(chars)\n\n# create a mapping from characters to integers\nstoi = {ch: i for i, ch in enumerate(chars)}\nitos = {i: ch for i, ch in enumerate(chars)}\nencode = lambda s: [stoi[c] for c in s]  # encoder: take a string, output a list of integers\ndecode = lambda l: ''.join(itos[i] for i in l)  # decoder: take a list of integers, output a string\n\n# Train and test splits\ndata = torch.tensor(encode(text), dtype=torch.long)\nn = int(0.9 * len(data))  # first 90% will be train, rest val\n```",
    "frame_6020.jpg": "```python\nbatch_size = 64  # how many independent sequences will we process in parallel?\nblock_size = 256  # what is the maximum context length for predictions?\nmax_iters = 5000\neval_interval = 500\nlearning_rate = 3e-4\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\neval_iters = 200\nn_embd = 384\nn_head = 6\nn_layer = 6\ndropout = 0.2\n\ntorch.manual_seed(1337)\n\n# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\nwith open('input.txt', 'r', encoding='utf-8') as f:\n    text = f.read()\n\n# here are all the unique characters that occur in this text\nchars = sorted(list(set(text)))\nvocab_size = len(chars)\n\n# create a mapping from characters to integers\nstoi = { ch: i for i, ch in enumerate(chars) }\nitos = { i: ch for i, ch in enumerate(chars) }\nencode = lambda s: [stoi[c] for c in s]  # encoder: take a string, output a list of integers\ndecode = lambda l: ''.join([itos[i] for i in l])  # decoder: take a list of integers, output a string\n\n# Train and test splits\ndata = torch.tensor(encode(text), dtype=torch.long)\nn = int(0.9 * len(data))  # first 90% will be train, rest val\ntrain_data = data[:n]\nval_data = data[n:]\n\n# data loading\ndef get_batch(split):\n    # generate a small batch of data of inputs x and targets y\n```\n",
    "frame_6029.jpg": "```python\nimport torch\nimport torch.nn.functional as F\n\n# hyperparameters\nbatch_size = 64  # how many independent sequences will we process in parallel?\nblock_size = 256  # what is the maximum context length for predictions?\nmax_iters = 5000\neval_interval = 500\nlearning_rate = 3e-4\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\neval_iters = 200\nn_embd = 384\nn_head = 6\nn_layer = 6\ndropout = 0.2\n\ntorch.manual_seed(1337)\n\n# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\nwith open('input.txt', 'r', encoding='utf-8') as f:\n    text = f.read()\n\n# here are all the unique characters that occur in this text\nchars = sorted(list(set(text)))\nvocab_size = len(chars)\n\n# create a mapping from characters to integers\nstoi = {ch: i for i, ch in enumerate(chars)} \nitos = {i: ch for i, ch in enumerate(chars)} \nencode = lambda s: [stoi[c] for c in s]  # encoder: take a string, output a list of integers\ndecode = lambda l: ''.join(itos[i] for i in l)  # decoder: take a list of integers, output a string\n```",
    "frame_6034.jpg": "```python\nimport torch\nimport torch.nn as functional as F\n\n# hyperparameters\nbatch_size = 64  # how many independent sequences will we process in parallel?\nblock_size = 256  # what is the maximum context length for predictions?\nmax_iters = 5000\neval_interval = 500\nlearning_rate = 3e-4\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\neval_iters = 200\nn_embd = 384\nn_head = 6\nn_layer = 6\ndropout = 0.2\n\ntorch.manual_seed(1337)\n\n# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\nwith open('input.txt', 'r', encoding='utf-8') as f:\n    text = f.read()\n```\n",
    "frame_6073.jpg": "```python\nimport torch\nimport torch.nn.functional as F\n\n# hyperparameters\nbatch_size = 64  # how many independent sequences will we process in parallel?\nblock_size = 256  # what is the maximum context length for predictions?\nmax_iters = 500\neval_interval = 3e-4\nlearning_rate = 3e-4\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\neval_iters = 200\nn_embd = 384\nn_head = 6\nn_layer = 6\ndropout = 0.2\n\ntorch.manual_seed(1337)\n\n# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\nwith open('input.txt', 'r', encoding='utf-8') as f:\n    text = f.read()\n```",
    "frame_6078.jpg": "Code NA",
    "frame_6147.jpg": "```python\nimport torch\nimport torch.nn as functional as F\n\n# hyperparameters\nbatch_size = 64  # how many independent sequences will we process in parallel?\nblock_size = 256  # what is the maximum context length for predictions?\nmax_iters = 5000\neval_interval = 500\nlearning_rate = 3e-4\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\neval_iters = 200\nn_embd = 384\nn_head = 6\nn_layer = 6\ndropout = 0.2\n\ntorch.manual_seed(1337)\n\n# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\nwith open('input.txt', 'r', encoding='utf-8') as f:\n    text = f.read()\n\n# here are all the unique characters that occur in this text\nchars = sorted(list(set(text)))\nvocab_size = len(chars)\n\n# create a mapping from characters to integers\nstoi = { ch:i for i, ch in enumerate(chars) }\nitos = { i:ch for i, ch in enumerate(chars) }\nencode = lambda s: [stoi[c] for c in s]  # encoder: take a string, output a list of integers\ndecode = lambda l: ''.join(itos[i] for i in l)  # decoder: take a list of integers, output a string\n\n# Train and test splits\n```",
    "frame_6150.jpg": "Code NA",
    "frame_6189.jpg": "```python\nx[:, 0].mean(), x[:, 0].std()  # mean,std of one feature across all batch inputs\n(tensor(0.1469), tensor(0.8803))\n\nx[0, :].mean(), x[0, :].std()  # mean,std of a single input from the batch, of its features\n(tensor(-9.5367e-09), tensor(1.0000))\n```\n",
    "frame_6197.jpg": "```python\nx[:,0].mean(), x[:,0].std()  # mean,std of one feature across all batch inputs\n(tensor(0.1469), tensor(0.8803))\n\nx[0,:].mean(), x[0,:].std()  # mean,std of a single input from the batch, of its features\n(tensor(-9.5367e-09), tensor(1.0000))\n```\n",
    "frame_6203.jpg": "```python\nx[:,0].mean(), x[:,0].std()  # mean,std of one feature across all batch inputs\n(tensor(0.1469), tensor(0.8803))\n\nx[0, :].mean(), x[0, :].std()  # mean,std of a single input from the batch, of its features\n(tensor(-9.5367e-09), tensor(1.0000))\n```\n\nCode extraction complete.",
    "frame_6208.jpg": "```python\nx[:, 0].mean(), x[:, 0].std()  # mean, std of one feature across all batch inputs\n(tensor(0.1469), tensor(0.8803))\n\nx[0, :].mean(), x[0, :].std()  # mean, std of a single input from the batch, of its features\n(tensor(-9.5367e-09), tensor(1.0000))\n```\n",
    "frame_6297.jpg": "```python\nx[:,0].mean(), x[:,0].std()  # mean,std of one feature across all batch inputs\n(tensor(0.1469), tensor(0.8803))\n\nx[0, :].mean(), x[0, :].std()  # mean,std of a single input from the batch, of its features\n(tensor(-9.5367e-09), tensor(1.0000))\n```",
    "frame_6301.jpg": "```python\n[x[i, 0].mean(), x[i, 0].std()]  # mean,std of one feature across all batch inputs\n(tensor(0.1469), tensor(0.8803))\n\n[x[0, :].mean(), x[0, :].std()]  # mean,std of a single input from the batch, of its features\n(tensor(-9.5367e-09), tensor(1.0000))\n```",
    "frame_6315.jpg": "```\nx[i,:0].mean(), x[i,:0].std()  # mean,std of one feature across all batch inputs\n(tensor(0.1469), tensor(0.8803))\n\nx[0,:].mean(), x[0,:].std()  # mean,std of a single input from the batch, of its features\n(tensor(-9.5367e-09), tensor(1.0000))\n\n# French to English translation example:\n# <---------- ENCODE ----------><---------- DECODE ----------> \n# les r\u00e9seaux de neurones sont g\u00e9niaux! <START> neural networks are awesome!<END>\n```",
    "frame_6347.jpg": "Here is the extracted developer code from the image:\n\n```python\nx[:,0].mean(), x[:,0].std() # mean,std of one feature across all batch inputs\n(tensor(0.1469), tensor(0.8803))\n\nx[0,:].mean(), x[0,:].std() # mean,std of a single input from the batch, of its features\n(tensor(-9.5367e-09), tensor(1.0000))\n```\n\nLet me know if you need anything else!",
    "frame_6350.jpg": "```python\nx[i,0].mean(), x[i,0].std()  # mean,std of one feature across all batch inputs\n(tensor(0.1469), tensor(0.8803))\n\nx[0,:].mean(), x[0,:].std()  # mean,std of a single input from the batch, of its features\n(tensor(-9.5367e-09), tensor(1.0000))\n```\n",
    "frame_6362.jpg": "```plaintext\nx[:,0].mean(), x[:,0].std()  # mean,std of one feature across all batch inputs\n(tensor(0.1469), tensor(0.8803))\n\nx[0,:].mean(), x[0,:].std()  # mean,std of a single input from the batch, of its features\n(tensor(-9.5367e-09), tensor(1.0000))\n```",
    "frame_6383.jpg": "```python\n[x[:,0].mean(), x[:,0].std()]  # mean,std of one feature across all batch inputs\n(tensor(0.1469), tensor(0.8803))\n\n[x[0,:].mean(), x[0,:].std()]  # mean,std of a single input from the batch, of its features\n(tensor(-9.5367e-09), tensor(1.0000))\n```",
    "frame_6384.jpg": "Code NA",
    "frame_6398.jpg": "```python\nclass MultiHeadAttention(nn.Module):\n    \"\"\" multiple heads of self-attention in parallel \"\"\"\n\n    def __init__(self, num_heads, head_size):\n        super().__init__()\n        self.heads = nn.ModuleList[Head(head_size) for _ in range(num_heads)]\n        self.proj = nn.Linear(n_embd, n_embd)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        out = torch.cat([h(x) for h in self.heads], dim=-1)\n        out = self.dropout(self.proj(out))\n        return out\n```\n",
    "frame_6399.jpg": "```python\nclass BigramLanguageModel(nn.Module):\n\n    def __init__(self):\n        super().__init__()\n        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n        self.blocks = nn.Sequential(*[Block(n_embd, n_head) for _ in range(n_layer)])\n        self.ln_f = nn.LayerNorm(n_embd)  # final layer norm\n        self.lm_head = nn.Linear(n_embd, vocab_size)\n\n    def forward(self, idx, targets=None):\n        B, T = idx.shape\n        tok_emb = self.token_embedding_table(idx)  # (B,T,C)\n        pos_emb = self.position_embedding_table(torch.arange(T, device=device))  # (T,C)\n        x = tok_emb + pos_emb  # (B,T,C)\n        x = self.blocks(x)  # (B,T,C)\n        x = self.ln_f(x)  # (B,T,C)\n        logits = self.lm_head(x)  # (B,T,vocab_size)\n\n        if targets is None:\n            loss = None\n        else:\n            B, T, C = logits.shape\n            logits = logits.view(B*T, C)\n            targets = targets.view(B*T)\n            loss = F.cross_entropy(logits, targets)\n\n        return logits, loss\n\n    def generate(self, idx, max_new_tokens):\n```",
    "frame_6400.jpg": "```python\ndef generate(self, idx, max_new_tokens):\n    # idx is (B, T) array of indices in the current context\n    for _ in range(max_new_tokens):\n        # crop idx to the last block.size tokens\n        idx_cond = idx[:, -block_size:]\n        # get the predictions\n        logits, loss = self(idx_cond)\n        # focuses only on the last time step\n        logits = logits[:, -1, :]  # if becomes (B, C)\n        # apply softmax to get probabilities\n        probs = F.softmax(logits, dim=-1)  # (B, C)\n        # sample from the distribution\n        idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n        # append sampled index to the running sequence\n        idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n    return idx\n\nmodel = BigramLanguageModel()\nm = model.to(device)\n\n# create a PyTorch optimizer\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n\nfor iter in range(max_iters):\n    # every once in a while evaluate the loss on train and val sets\n    if iter % eval_interval == 0 or iter == max_iters - 1:\n        losses = estimate_loss()\n        print(f\"step {iter}: train loss {losses['train']:1.4f}, val loss {losses['val']:1.4f}\")\n\n    # sample a batch of data\n    xb, yb = get_batch('train')\n\n    # evaluate the loss\n```\n",
    "frame_6402.jpg": "Here is the extracted developer code from the image:\n\n```python\nmodel = BigramLanguageModel()\nmodel = model.to(device)\n\n# create a PyTorch optimizer\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n\nfor iter in range(max_iters):\n    # every once in a while evaluate the loss on train and val sets\n    if iter % val_interval == 0 or iter == max_iters - 1:\n        losses = estimate_loss()\n        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n\n    # sample a batch of data\n    xb, yb = get_batch('train')\n\n    # evaluate the loss\n    logits, loss = model(xb, yb)\n    optimizer.zero_grad(set_to_none=True)\n    loss.backward()\n    optimizer.step()\n\n# generate from the model\ncontext = torch.zeros(1, 1, dtype=torch.long, device=device)\nprint(decode(m.generate(context, max_new_tokens=500)[0].tolist()))\n# open('more.txt', 'w').write(decode(m.generate(context, max_new_tokens=10000)[0].tolist()))\n```",
    "frame_6405.jpg": "```python\nmodel = BigramLanguageModel()\nmodel = model.to(device)\noptimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n\nfor iter in range(max_iters):\n    # every once in a while evaluate the loss on train and val sets\n    if iter % eval_interval == 0 or iter == max_iters - 1:\n        losses = estimate_loss()\n        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n\n    # sample a batch of data\n    xb, yb = get_batch('train')\n\n    # evaluate the loss\n    logits, loss = model(xb, yb)\n    optimizer.zero_grad(set_to_none=True)\n    loss.backward()\n    optimizer.step()\n\n    # generate from the model\n    context = torch.zeros((1, 1), dtype=torch.long, device=device)\n    print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))\n```",
    "frame_6412.jpg": "Here's the extracted code from the image:\n\n```python\nmodel = BigramLanguageModel()\nmodel = model.to(device)\n\noptimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n\nfor iter in range(max_iters):\n    # every once in a while evaluate the loss on train and val sets\n    if iter % val_interval == 0 or iter == max_iters - 1:\n        losses = estimate_loss()\n        print(f\"step {iter}: train loss {losses['train']:4f}, val loss {losses['val']:4f}\")\n\n    # sample a batch of data\n    xb, yb = get_batch('train')\n\n    # evaluate the loss\n    logits, loss = model(xb, yb)\n    optimizer.zero_grad(set_to_none=True)\n    loss.backward()\n    optimizer.step()\n\n# generate from the model\ncontext = torch.zeros((1, 1), dtype=torch.long, device=device)\nprint(decode(m.generate(context, max_new_tokens=500)[0].tolist()))\n\n# open('more.txt', 'w').write(decode(m.generate(context, max_new_tokens=10000)[0].tolist()))\n```",
    "frame_6415.jpg": "Code NA",
    "frame_6417.jpg": "```python\nv_head = 3\nn_emb = 768  # for pretraining 0 is good, for finetuning try 0.1+\n# adam optimizer\nlearning_rate = 6e-4  # max learning rate\nmax_iters = 6000000  # total number of training iterations\nweight_decay = 1e-2\nbeta1 = 0.9\nbeta2 = 0.95\n\n# learning rate decay settings\ndecay_lr = True  # whether to decay the learning rate\nwarmup_iters = 2000  # how many steps to warm up for\nlr_decay_iters = 6000000  # should be = max_iters per Chinchilla\nmin_lr = 6e-5  # minimum learning rate, should be = learning_rate/10 per Chinchilla\n\n# DDP settings\nbackend = 'nccl', 'nccl', 'gloo', etc.\n# system\ndevice = 'cuda'  # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1'\ndtype = 'bfloat16'  # 'float32' or 'bfloat16'\ncompile = True  # use PyTorch 2.0 to compile the model to be faster\n\nconfig_keys = [k for k, v in globals().items() if not k.startswith('_') and isinstance(v, (int, float, bool, str))]\nexec(open('configurator.py').read())  # overrides from command line or config file\nconfig = {k: globals()[k] for k in config_keys}  # will be useful for logging\n# ...\n# various init, derived attributes, I/O setup\nddp = int(os.environ.get('RANK', -1))  # -1 if this is a ddp run\nif ddp:\n    init_process_group(backend=backend)\n    ddp_rank = int(os.environ['RANK'])\n    ddp_local_rank = int(os.environ['LOCAL_RANK'])\n    device = f'cuda:ddp_local_rank'  # this process will do logging, checkpointing etc.\n    master_process = ddp_rank == 0  # each process gets a different seed\nelse:\n    # if not ddp, we are running a single gpu, and process\n    master_process = True\n    seed_offset = 0\n\nif master_process:\n    os.makedirs(out_dir, exist_ok=True)\n    torch.manual_seed(1337 + seed_offset)\n    torch.backends.cuda.matmul.allow_tf32 = True  # allow tf32 on matmul\n    torch.backends.cudnn.allow_tf32 = True  # allow tf32 on cudnn\n    dtype = 'cuda' if device else 'cpu' for later use in torch.autocast\n    # note: float32 to, 'bfloat32' to 'bfloat16' etc. for the idea to GradScaler\n```\n",
    "frame_6418.jpg": "```\n# init these up here, can override if init_from='resume' (i.e. from a checkpoint)\niter_num = 0\nbest_val_loss = 1e9\n\n# attempt to derive vocab_size from the dataset\nmeta_path = os.path.join(data_dir, 'meta.pkl')\nif os.path.exists(meta_path):\n    with open(meta_path, 'rb') as f:\n        meta = pickle.load(f)\n        vocab_size = meta['vocab_size']\n        print(f'vocab_size = {vocab_size}')\nelse:\n    print(f'vocab_size not found in {meta_path}, using GPT-2 default of 50257')\n    vocab_size = 50257\n\n# model init\nmodel_args = dict(n_layer=n_layer, n_head=n_head, n_embd=n_embd, block_size=block_size, dropout=dropout, vocab_size=vocab_size)\nif init_from == 'scratch':\n    print(\"Initializing a new model from scratch\")\n    gptconf = GPTConfig(**model_args)\n    model = GPT(gptconf)\nelif init_from == 'resume':\n    print(f\"Resuming training from {out_dir}\")\n    # resume training from a checkpoint.\n    ckpt_path = os.path.join(out_dir, 'ckpt.pt')\n    checkpoint_model_args = checkpoint['model_args']\n    for k, v in model_args.items():\n        assert checkpoint_model_args[k] == v, \"for now\"\n    gptconf = GPTConfig(**model_args)\n    model = GPT(gptconf)\n    state_dict = checkpoint['state_dict']\n    # fix the keys of the state dictionary\n    # honestly no idea how checkpoints sometimes get this prefix, have to debug more\n    unwanted_prefix = '_orig_mod.'\n    for k, v in list(state_dict.items()):\n        if k.startswith(unwanted_prefix):\n            state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n    model.load_state_dict(state_dict)\n\niter_num = checkpoint['iter_num']\nbest_val_loss = checkpoint['best_val_loss']\n```\n",
    "frame_6419.jpg": "```python\n@torch.no_grad()\ndef estimate_loss():\n    out = {}\n    model.eval()\n    for split in ['train', 'val']:\n        losses = torch.zeros(eval_iters)\n        for k in range(eval_iters):\n            X, Y = get_batch(split)\n            with ctx:\n                logits, loss = model(X, Y)\n                losses[k] = loss.item()\n        out[split] = losses.mean()\n    return out\n\n# learning rate decay scheduler (cosine with warmup)\ndef get_lr(iter):\n    # 1) linear warmup for warmup_iters steps\n    if iter < warmup_iters:\n        return learning_rate * iter / warmup_iters\n    # 2) if iter > lr_decay_iters, return min learning rate\n    if iter > lr_decay_iters:\n        return min_lr\n    # 3) In between, use cosine decay down to min learning rate\n    decay_ratio = (iter - warmup_iters) / (lr_decay_iters - warmup_iters)\n    assert 0 <= decay_ratio <= 1\n    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))  # coeff ranges 0..1\n    return min_lr + coeff * (learning_rate - min_lr)\n\n# logging\nimport wandb\nwandb.init(project=wandb_project, name=wandb_run_name, config=config)\n\n# training loop\nt0 = time.time()\nwhile True:\n    # determine the learning rate for this iteration\n    if decay_lr:\n        lr = get_lr(iter_num)\n        for param_group in optimizer.param_groups:\n            param_group['lr'] = lr\n    else:\n        lr = learning_rate\n\n    # evaluate the loss on train/val sets and write checkpoints\n```",
    "frame_6420.jpg": "```\nimport wandb\nwandb.init(project=wandb_project, name=wandb_run_name, config=config)\n\n# training loop\nt = time.time()\nwhile True:\n    # determine the learning rate for this iteration\n    if decay_lr:\n        lr = get_lr(iter_num)\n        for param_group in optimizer.param_groups:\n            param_group['lr'] = lr\n    else:\n        lr = learning_rate\n\n    # evaluate the loss on train/val sets and write checkpoints\n    if iter_num == eval_interval and master_process:\n        losses = estimate_loss()\n        printf(f\"step {iter_num}: train/loss: {losses['train']:.4f}, val loss: {losses['val']:.4f}\")\n        \n        wandb.log({\n            \"iter\": iter_num,\n            \"train/loss\": losses['train'],\n            \"val/loss\": losses['val'],\n            \"lr\": lr,\n        })\n\n    if losses['val'] < best_val_loss or always_save_checkpoint:\n        best_val_loss = losses['val']\n        raw_model = model.module if ddp else model\n        checkpoint = {\n            'model': raw_model.state_dict(),\n            'optimizer': optimizer.state_dict(),\n            'model_args': model_args,\n            'iter_num': iter_num,\n            'best_val_loss': best_val_loss,\n            'config': config,\n        }\n        printf(f\"saving checkpoint to {out_dir}\")\n        torch.save(checkpoint, os.path.join(out_dir, 'ckpt.pt'))\n\n    if iter_num == 0 and eval_only:\n        break\n\n# forward backward update, with optional gradient accumulation to simulate larger batch size\noptimizer.zero_grad(set_to_none=True)\nfor micro_step in range(gradient_accumulation_steps):\n    x, y = get_batch('train')\n```",
    "frame_6423.jpg": "```python\n\"\"\"\nFull definition of a GPT Language Model, all of it in this single file.\nReferences:\n1) the official GPT-2 TensorFlow implementation released by OpenAI:\nhttps://github.com/openai/gpt-2/blob/master/src/model.py\n2) huggingface/transformers PyTorch implementation:\nhttps://github.com/huggingface/transformers/blob/main/src/transformers/models/gpt2/modeling_gpt2.py\n\"\"\"\n\nimport math\nfrom dataclasses import dataclass\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\n\n@torch.jit.script  # good to enable when not using torch.compile, disable when using (our default)\ndef new_gelu(x):\n    \"\"\"\n    Implementation of the GELU activation function currently in Google BERT repo (identical to OpenAI GPT).\n    Reference: Gaussian Error Linear Units (GELU) paper: https://arxiv.org/abs/1606.08415\n    \"\"\"\n    return 0.5 * x * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (x + 0.044715 * torch.pow(x, 3.0))))\n\nclass CausalSelfAttention(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        assert config.n_embd % config.n_head == 0\n        # key, query, value projections for all heads, but in a batch\n        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n        # output projection\n        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n        # regularization\n```",
    "frame_6424.jpg": "```python\nmodel = BigramLanguageModel()\nm = model.to(device)\n\n# create a PyTorch optimizer\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n\nfor iter in range(max_iters):\n    # every once in a while evaluate the loss on train and val sets\n    if iter % eval_interval == 0 or iter == max_iters - 1:\n        losses = estimate_loss()\n        print(f'step {iter}: train loss {losses[\"train\"]:.4f}, val loss {losses[\"val\"]:.4f}')\n\n    # sample a batch of data\n    xb, yb = get_batch('train')\n\n    # evaluate the loss\n    logits, loss = model(xb, yb)\n    optimizer.zero_grad(set_to_none=True)\n    loss.backward()\n    optimizer.step()\n\n    # generate from the model\n    context = torch.zeros((1, 1), dtype=torch.long, device=device)\n    print(decode(m.generate(context, max_new_tokens=500).tolist()))\n    # open('more.txt', 'w').write(decode(m.generate(context, max_new_tokens=10000).tolist()))\n```",
    "frame_6429.jpg": "```python\nimport torch.nn as nn\nfrom torch.nn import functional as F\n\ndef new_gelu(x):\n    \"\"\"\n    Implementation of the GELU activation function currently in Google BERT repo (identical to OpenAI GPT).\n    Reference: Gaussian Error Linear Units (GELU) paper: https://arxiv.org/abs/1606.08415\n    \"\"\"\n    return 0.5 * x * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (x + 0.044715 * torch.pow(x, 3.0))))\n\nclass CausalSelfAttention(nn.Module):\n    def __init__(self, Config):\n        super().__init__()\n        assert config.n_embd % config.n_head == 0\n        # key, query, value projections for all heads, but in a batch\n        self.attn = nn.Linear(config.n_embd, 3 * config.n_embd)  # output projection\n        self.c_proj = nn.Linear(config.n_embd, config.n_embd)  # regularization\n        self.attn_dropout = nn.Dropout(config.dropout)\n        self.resid_dropout = nn.Dropout(config.dropout)\n        # causal mask to ensure that attention is only applied to the left in the input sequence\n        self.register_buffer(\"bias\", torch.tril(torch.ones((1, config.block_size, config.block_size))))\n        self.head = config.n_head\n        self.n_embd = config.n_embd\n\n    def forward(self, x):\n        B, T, C = x.size()  # batch size, sequence length, embedding dimensionality (n_embd)\n        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n        k, v = self.attn(x).split(self.n_embd, dim=2)\n        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)  # (B, nh, T, hs)\n        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)  # (B, nh, T, hs)\n```",
    "frame_6431.jpg": "Here is the extracted code from the image:\n\n```python\ndef new_gelu(x):\n    \"\"\"\n    Implementation of the GELU activation function currently in Google BERT repo (identical to OpenAI GPT).\n    Reference: Gaussian Error Linear Units (GELU) paper: https://arxiv.org/abs/1606.08415\n    \"\"\"\n    return 0.5 * x * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (x + 0.044715 * torch.pow(x, 3.0))))\n\nclass CausalSelfAttention(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        assert config.n_embd % config.n_head == 0\n        # key, query, value projections for all heads, but in a batch\n        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)  # output projection\n        self.c_proj = nn.Linear(config.n_embd, config.n_embd)  # regularization\n        self.attn_dropout = nn.Dropout(config.dropout)\n        self.resid_dropout = nn.Dropout(config.dropout)\n        self.register_buffer(\n            \"bias\", torch.tril(torch.ones(config.block_size, config.block_size)).view(1, config.block_size, config.block_size)\n        )\n\n        self.n_head = config.n_head\n        self.n_embd = config.n_embd\n\n    def forward(self, x):  # batch size, sequence length, embedding dimensionality (n_embd)\n        B, T, C = x.size()\n        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n        q, k, v = self.c_attn(x).split(self.n_embd, dim=2)\n\n        k = k.view(B, T, self.n_head).transpose(1, 2)  # (B, nh, T, hs)\n        q = q.view(B, T, self.n_head).transpose(1, 2)  # (B, nh, T, hs)\n        v = v.view(B, T, self.n_head).transpose(1, 2)  # (B, nh, T, hs)\n\n        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, T) -> (B, nh, T, T)\n        att = att.masked_fill(att < 0, float('-inf'))\n```\n\nIf there's anything else you need, let me know!",
    "frame_6433.jpg": "```python\nclass CausalSelfAttention(nn.Module):\n\n    def __init__(self, config):\n        super().__init__()\n        assert config.n_embd % config.n_head == 0\n        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n        self.attn_dropout = nn.Dropout(config.dropout)\n        self.resid_dropout = nn.Dropout(config.dropout)\n        self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size)).view(1, 1, config.block_size, config.block_size))\n        self.n_head = config.n_head\n        self.n_embd = config.n_embd\n\n    def forward(self, x):\n        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n\n        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n        q, k, v = self.c_attn(x).split(self.n_embd, dim=2)\n        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n\n        # causal self-attention; Self-Att  (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n        att = (q @ k.transpose(-2, -1)) / (math.sqrt(C // self.n_head))\n        att = att.masked_fill(self.bias[:, :, :T, :T] == 0, float('-inf'))\n        att = F.softmax(att, dim=-1)\n        y = att @ self.attn_dropout(v) # (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, hs)\n\n        # output projection\n        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n        return y\n```",
    "frame_6434.jpg": "Here is the extracted developer code from the image:\n\n```python\ndef __init__(self, config):\n    super().__init__()\n    assert config.n_embd % config.n_head == 0\n    # key, query, value projections for all heads, but in a batch\n    self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)  # output projection\n    self.c_proj = nn.Linear(config.n_embd, config.n_embd)  # regularization\n    self.attn_dropout = nn.Dropout(config.dropout)\n    self.resid_dropout = nn.Dropout(config.dropout)\n    # causal mask to ensure that attention is only applied to the left in the input sequence\n    q = self.register_buffer(\"bias\", torch.triu(torch.ones(config.block_size, config.block_size))\n                             .view(1, 1, config.block_size, config.block_size))\n\ndef forward(self, x):\n    B, T, C = x.size()  # batch size, sequence length, embedding dimensionality (n_embd)\n    q, k, v = self.c_attn(x).split(n_embd, dim=2)\n    k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)  # (B, nh, T, hs)\n    q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)  # (B, nh, T, hs)\n    v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)  # (B, nh, T, hs)\n    \n    # causal self-attention; Self-Attend: (B, nh, T, hs) -> (B, nh, T, T)\n    att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n    att = att.masked_fill(self.bias[:, :, T:, :T] == 0, float('-inf'))\n    att = self.softmax(att, dim=-1)\n    att = self.attn_dropout(att)\n    y = att @ v  # (B, nh, T, hs)\n    y = y.transpose(1, 2).contiguous().view(B, T, C)  # re-assemble all head outputs side by side\n    # output projection\n    y = self.resid_dropout(self.c_proj(y))\n    return y\n```\n\nIf you need any further assistance, feel free to ask!",
    "frame_6437.jpg": "Here's the extracted code:\n\n```python\nassert config.n_embd % config.n_head == 0\nself.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)  # output projection\nself.attn_dropout = nn.Dropout(config.dropout)\nself.resid_dropout = nn.Dropout(config.dropout)\n\ndef forward(self, x):\n    B, T, C = x.size()  # batch size, sequence length, embedding dimensionality (n_embd)\n    q, k, v = self.c_attn(x).split(config.n_embd, dim=2)\n    k = k.view(B, self.n_head, C // self.n_head).transpose(1, 2)  # (B, nh, T, hs)\n    q = q.view(B, self.n_head, C // self.n_head).transpose(1, 2)  # (B, nh, T, hs)\n    v = v.view(B, self.n_head, C // self.n_head).transpose(1, 2)  # (B, nh, T, hs)\n\n    att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(q.size(-1)))\n    att = att.masked_fill(self.bias[:, :, :T, :T] == 0, float('-inf'))\n    att = self.attn_dropout(att)\n\n    y = att @ v  # (B, nh, T, hs) -> (B, nh, T, T)\n    y = y.transpose(1, 2).contiguous().view(B, T, -1)  # re-assemble all head outputs side by side\n    return y\n\nclass MLP(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n```\n\nLet me know if you need anything else!",
    "frame_6442.jpg": "```\nsuper().__init__()\nassert config.n_embd % config.n_head == 0\nself.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\nself.c_proj = nn.Linear(config.n_embd, config.n_embd)\nself.attn_dropout = nn.Dropout(config.dropout)\nself.resid_dropout = nn.Dropout(config.dropout)\nself.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size)))\nself.n_head = config.n_head\nself.n_embd = config.n_embd\n\ndef forward(self, x):\n    B, T, C = x.size()  # batch size, sequence length, embedding dimensionality (n_embd)\n    k, v = self.c_attn(x).split(self.n_embd, dim=2)\n    k = k.view(B, T, self.n_head, C).transpose(1, 2)  # (B, nh, T, hs)\n    q = q.view(B, T, self.n_head, C).transpose(1, 2)\n    v = v.view(B, T, self.n_head, C).transpose(1, 2)\n\n    att = (q @ k.transpose(-2, -1)) / math.sqrt(k.size(-1))\n    att = att.masked_fill(self.bias[:,:T,:T] == 0, float('-inf'))\n    att = F.softmax(att, dim=-1)\n    att = self.attn_dropout(att)\n    y = att @ v  # (B, nh, T, hs)\n    y = y.transpose(1, 2).contiguous().view(B, T, C)  # re-assemble all head outputs side by side\n    y = self.resid_dropout(self.c_proj(y))\n    return y\n```",
    "frame_6443.jpg": "```python\nsuper().__init__()\nassert config.n_embd % config.n_head == 0\nself.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)  # output projection\nself.attn_dropout = nn.Dropout(config.dropout)\nself.register_buffer(\"bias\", torch.ones(config.block_size, config.block_size).view(1, 1, config.block_size, config.block_size))\n\ndef forward(self, x):\n    B, T, C = x.size()  # batch size, sequence length, embedding dimensionality (n_embd)\n    \n    k, v = self.c_attn(x).split(self.n_head, dim=2)\n    k = k.view(B, T, self.n_head).transpose(1, 2)  # (B, nh, T, hs)\n    v = v.view(B, T, self.n_head).transpose(1, 2)  # (B, nh, T, hs)\n\n    att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n    att = att.masked_fill(self.bias[:, :, :T, :T] == 0, float('-inf'))\n    att = F.softmax(att, dim=-1)\n    att = self.attn_dropout(att)\n    \n    y = att @ v  # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n    y = y.transpose(1, 2).contiguous().view(B, T, C)  # re-assemble all head outputs side by side\n    return y\n\nclass MLP(nn.Module):\n    def __init__(self, config):\n```\n\nThis code snippet includes various parts of a neural network model, including initialization, a forward pass, and a multi-layer perceptron (MLP).",
    "frame_6446.jpg": "```python\ndef __init__(self, config):\n    super().__init__()\n    assert config.n_embd % config.n_head == 0\n    # ...\n    self.n_head = config.n_head\n    self.n_embd = config.n_embd\n\ndef forward(self, x):\n    B, T, C = x.size()  # batch size, sequence length, embedding dimensionality (n_embd)\n    # calculate query, key, value\n    q, k, v = self.c_attn(x).split(self.n_embd, dim=2)\n    # ...\n    return y\n```",
    "frame_6447.jpg": "```python\nmodel = BigramLanguageModel()\nm = model.to(device)\n\n# create a PyTorch optimizer\noptimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n\nfor iter in range(max_iters):\n    # every once in a while evaluate the loss on train and val sets\n    if iter % val_interval == 0 or iter == max_iters - 1:\n        losses = estimate_loss()\n        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n\n    # sample a batch of data\n    xb, yb = get_batch('train')\n\n    # evaluate the loss\n    logits, loss = model(xb, yb)\n    optimizer.zero_grad(set_to_none=True)\n    loss.backward()\n    optimizer.step()\n\n# generate from the model\ncontext = torch.zeros((1, 1), dtype=torch.long, device=device)\nprint(decode(m.generate(context, max_new_tokens=500).tolist()))\n# open('more.txt', 'w').write(decode(m.generate(context, max_new_tokens=10000)[0].tolist()))\n```",
    "frame_6448.jpg": "```\nclass BigramLanguageModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n        self.blocks = nn.Sequential(*(Block(n_embd, n_head) for _ in range(n_layer)))\n        self.ln_f = nn.LayerNorm(n_embd)  # final layer norm\n        self.lm_head = nn.Linear(n_embd, vocab_size)\n\n    def forward(self, idx, targets=None):\n        tok_emb = self.token_embedding_table(idx)\n        pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n        x = tok_emb + pos_emb\n        x = self.blocks(x)\n        x = self.ln_f(x)  # (B,T,C)\n        logits = self.lm_head(x)  # (B,T,vocab_size)\n\n        if targets is None:\n            return logits\n```",
    "frame_6449.jpg": "```python\nclass FeedForward(nn.Module):\n    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n    \n    def __init__(self, n_embd):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(n_embd, 4 * n_embd),\n            nn.ReLU(),\n            nn.Linear(4 * n_embd, n_embd),\n            nn.Dropout(dropout),\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\nclass Block(nn.Module):\n    \"\"\" Transformer block: communication followed by computation \"\"\"\n    \n    def __init__(self, n_embd, n_head):\n        # n_embd embedding dimension, n_head: the number of heads we'd like\n        super().__init__()\n        head_size = n_embd // n_head\n        self.sa = MultiHeadAttention(n_head, head_size)\n```\n",
    "frame_6450.jpg": "Here is the extracted code from the image:\n\n```python\ndef forward(self, x):\n    B, T, C = x.shape\n    k = self.key(x)  # (B, T, C)\n    q = self.query(x)  # (B, T, C)\n\n    # compute attention scores (\"affinities\")\n    wei = q @ k.transpose(-2, -1) * (C ** -0.5)  # (B, T, C) -> (B, T, T)\n    wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))  # (B, T, T)\n    wei = F.softmax(wei, dim=-1)  # (B, T, T)\n    wei = self.dropout(wei)\n\n    # perform the weighted aggregation of the values\n    v = self.value(x)  # (B, T, C)\n    out = wei @ v @ (B, T, C)  # (B, T, C) -> (B, T, C)\n    return out\n\nclass MultiHeadAttention(nn.Module):\n    \"\"\" multiple heads of self-attention in parallel \"\"\"\n\n    def __init__(self, num_heads, head_size):\n        super().__init__()\n        self.heads = nn.ModuleList(Head(head_size) for _ in range(num_heads))\n        self.proj = nn.Linear(n_embd, n_embd)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        out = torch.cat([h(x) for h in self.heads], dim=-1)\n        out = self.dropout(self.proj(out))\n        return out\n```\n\nThis code implements parts of a multi-head attention mechanism commonly used in transformer models.",
    "frame_6452.jpg": "```python\nclass Head(nn.Module):\n    \"\"\"\"\"\"\" one head of self-attention \"\"\"\n\n    def __init__(self, head_size):\n        super().__init__()\n        self.key = nn.Linear(n_embd, head_size, bias=False)\n        self.query = nn.Linear(n_embd, head_size, bias=False)\n        self.value = nn.Linear(n_embd, head_size, bias=False)\n        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        B, T, C = x.shape\n        k = self.key(x)\n        q = self.query(x)\n        # compute attention scores (\"affinities\")\n        wei = q @ k.transpose(-2, -1) * C**-0.5  # (B, T, C) @ (B, C, T) -> (B, T, T)\n        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n        wei = F.softmax(wei, dim=-1)\n        v = self.value(x)  # (B, T, C) -> (B, T, C)\n        out = wei @ v  # (B, T, T) @ (B, T, C) -> (B, T, C)\n        return out\n\nclass MultiHeadAttention(nn.Module):\n    \"\"\"multiple heads of self-attention in parallel \"\"\"\n\n    def __init__(self, num_heads, head_size):\n        ...\n```",
    "frame_6454.jpg": "```python\nclass FeedForward(nn.Module):\n    \"\"\" A simple linear layer followed by a non-linearity \"\"\"\n\n    def __init__(self, n_embd):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(n_embd, 4 * n_embd),\n            nn.ReLU(),\n            nn.Linear(4 * n_embd, n_embd),\n            nn.Dropout(dropout),\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\nclass Block(nn.Module):\n    \"\"\" Transformer Block: communication followed by computation \"\"\"\n\n    def __init__(self, n_embd, n_head):\n        # n_embd: embedding dimension, n_head: the number of heads we'd like\n        super().__init__()\n        self.sa = MultiHeadAttention(n_head, head_size)\n        self.ffwd = FeedForward(n_embd)\n```",
    "frame_6455.jpg": "```\nclass MultiHeadAttention(nn.Module):\n    \"\"\" multiple heads of self-attention in parallel \"\"\"\n    \n    def __init__(self, num_heads, head_size):\n        super().__init__()\n        self.heads = nn.ModuleList(Head(head_size) for _ in range(num_heads))\n        self.proj = nn.Linear(n_embd, n_embd)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        out = torch.cat([h(x) for h in self.heads], dim=-1)\n        out = self.dropout(self.proj(out))\n        return out\n\nclass FeedForward(nn.Module):\n    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n    \n    def __init__(self, n_embd):\n        super().__init__()\n        self.net = nn.Sequential(\n```\n",
    "frame_6458.jpg": "```python\ndef __init__(self, config):\n    super().__init__()\n    assert config.n_embd % config.n_head == 0\n    self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n    self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n    self.attn_dropout = nn.Dropout(config.dropout)\n    self.resid_dropout = nn.Dropout(config.dropout)\n    self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size)).view(1, 1, config.block_size, config.block_size))\n\n    self.n_head = config.n_head\n    self.n_embd = config.n_embd\n\ndef forward(self, x): \n    B, T, C = x.size()  # batch size, sequence length, embedding dimensionality (n_embd)\n    q, k, v = self.c_attn.split(self.n_embd, dim=2)\n    \n    k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)  # (B, nh, T, hs)\n    v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)  # (B, nh, T, hs)\n    \n    att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(self.size(-1)))\n    att = att.masked_fill(self.bias[:,:T,:T] == 0, float('-inf'))\n    att = F.softmax(att, dim=-1)\n    y = att @ v  # (B, nh, T, hs) -> (B, nh, T, hs)\n    y = y.transpose(1, 2).contiguous().view(B, T, -1)  # re-assemble all head outputs side by side\n    y = self.resid_dropout(self.c_proj(y))\n    return y\n```",
    "frame_6481.jpg": "```python\nclass MultiHeadAttention(nn.Module):\n    \"\"\" multiple heads of self-attention in parallel \"\"\"\n\n    def __init__(self, num_heads, head_size):\n        super().__init__()\n        self.heads = nn.ModuleList(Head(head_size) for _ in range(num_heads))\n        self.proj = nn.Linear(n_embd, n_embd)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        out = torch.cat([h(x) for h in self.heads], dim=-1)\n        out = self.dropout(self.proj(out))\n        return out\n\nclass FeedForward(nn.Module):\n    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n\n    def __init__(self, n_embd):\n        super().__init__()\n        self.net = nn.Sequential(\n```",
    "frame_6482.jpg": "```python\ndef __init__(self, config):\n    super().__init__()\n    assert config.n_embd % config.n_head == 0\n    self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n    self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n    self.attn_dropout = nn.Dropout(config.dropout)\n    self.resid_dropout = nn.Dropout(config.dropout)\n    self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size)))\n    self.n_head = config.n_head\n    self.n_embd = config.n_embd\n\ndef forward(self, x):\n    B, T, C = x.size()\n    q, k, v = self.c_attn(x).split(self.n_embd, dim=2)\n    k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)  # (B, nh, T, hs)\n    v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)  # (B, nh, T, hs)\n\n    att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n    att = att.masked_fill(self.bias[:T, :T] == 0, float('-inf'))\n    att = F.softmax(att, dim=-1)\n    y = att @ v  # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n    y = y.transpose(1, 2).contiguous().view(B, T, C)  # re-assemble all head outputs side by side\n    y = self.resid_dropout(self.c_proj(y))\n    return y\n```",
    "frame_6487.jpg": "```python\ndef forward(self, x):\n    B, T, C = x.size()  # batch size, sequence length, embedding dimensionality (n_emb)\n\n    # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n    k = k.view(B, T, self.n_head, C // self.n_head).transpose(2, 1)  # (B, nh, T, hs)\n    q = q.view(B, T, self.n_head, C // self.n_head).transpose(2, 1)  # (B, nh, T, hs)\n    v = v.view(B, T, self.n_head, C // self.n_head).transpose(2, 1)  # (B, nh, T, hs)\n\n    # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n    att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n    att = att.masked_fill(self.bias[:, :, :T, :T] == 0, float('-inf'))\n    att = F.softmax(att, dim=-1)\n    y = att @ v  # (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, hs)\n    y = y.transpose(1, 2).contiguous().view(B, T, C)  # re-assemble all head outputs side by side\n    # output projection\n    y = self.resid_dropout(self.c_proj(y))\n    return y\n\nclass MLP(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.c_fc = nn.Linear(config.n_embd, 4 * config.n_embd)\n        self.c_proj = nn.Linear(4 * config.n_embd, config.n_embd)\n        self.dropout = nn.Dropout(config.dropout)\n\n    def forward(self, x):\n        x = self.c_fc(x)\n        x = self.new_gelu(x)\n        x = self.c_proj(x)\n        x = self.dropout(x)\n        return x\n```",
    "frame_6488.jpg": "Here's the extracted developer code from the image:\n\n```python\n# causal self-attention; Self-attend: (B, nh, T) x (B, nh, T) -> (B, nh, T, T)\natt = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\natt = att.masked_fill(self.bias[:, :, :T, :T] == float('-inf'))\natt = F.softmax(att, dim=1)\natt = self.attn_dropout(att)\n\n# output projection\ny = self.resid_dropout(self.c_proj(y))\nreturn y\n\nclass MLP(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.c_fc = nn.Linear(config.n_embd, 4 * config.n_embd)\n        self.c_proj = nn.Linear(4 * config.n_embd, config.n_embd)\n        self.dropout = nn.Dropout(config.dropout)\n\n    def forward(self, x):\n        x = self.c_fc(x)\n        x = new_gelu(x)\n        x = self.c_proj(x)\n        x = self.dropout(x)\n        return x\n\nclass Block(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.ln_1 = nn.LayerNorm(config.n_embd)\n        self.attn = CausalSelfAttention(config)\n        self.ln_2 = nn.LayerNorm(config.n_embd)\n        self.mlp = MLP(config)\n```\n\nIf you need further assistance or modifications, let me know!",
    "frame_6492.jpg": "```python\nimport math\nfrom dataclasses import dataclass\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\n\n@torch.jit.script  # good to enable when not using torch.compile, disable when using our default\ndef new_gelu(x):\n    \"\"\"\n    Implementation of the GELU activation function currently in Google BERT repo (identical to OpenAI GPT).\n    Reference: Gaussian Error Linear Units (GELU) paper: https://arxiv.org/abs/1606.08415\n    \"\"\"\n    return 0.5 * x * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (x + 0.044715 * torch.pow(x, 3.0))))\n\nclass CausalSelfAttention(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        assert config.n_embd % config.n_head == 0\n        # key, query, value projections for all heads, but in a batch\n        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)  # output projection\n        self.c_proj = nn.Linear(config.n_embd, config.n_embd)  # regularization\n        self.attn_dropout = nn.Dropout(config.dropout)\n        self.resid_dropout = nn.Dropout(config.dropout)\n        # causal mask to ensure that attention is only applied to the left in the input sequence\n        self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size)))\n```",
    "frame_6496.jpg": "```python\ndef forward(self, x):\n    B, T, C = x.size()  # batch size, sequence length, embedding dimensionality (n_embd)\n\n    # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n    k, v = self.c_attn(x).split(self.n_embd, dim=2)\n    q = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)  # (B, nh, T, hs)\n    v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)  # (B, nh, T, hs)\n\n    # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, T, hs) -> (B, nh, T, T)\n    att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n    att = att.masked_fill(self.bias[:, :, :T, :T] == 0, float('-inf'))\n    att = self.attn_dropout(att)\n    y = att @ v  # (B, nh, T, hs)\n    y = y.transpose(1, 2).contiguous().view(B, T, C)  # re-assemble all head outputs side by side\n\n    # output projection\n    y = self.resid_dropout(self.c_proj(y))\n    return y\n\nclass MLP(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.c_fc = nn.Linear(config.n_embd, 4 * config.n_embd)\n        self.c_proj = nn.Linear(4 * config.n_embd, config.n_embd)\n        self.drop = nn.Dropout(config.dropout)\n\n    def forward(self, x):\n        x = self.c_fc(x)\n        x = new_geLU(x)\n        x = self.c_proj(x)\n        x = self.drop(x)\n        return x\n\nclass Block(nn.Module):\n```",
    "frame_6497.jpg": "```python\nclass MLP(nn.Module):\n\n    def __init__(self, config):\n        super().__init__()\n        self.c_fc = nn.Linear(config.n_embd, 4 * config.n_embd)\n        self.c_proj = nn.Linear(4 * config.n_embd, config.n_embd)\n        self.dropout = nn.Dropout(config.dropout)\n\n    def forward(self, x):\n        x = self.c_fc(x)\n        x = new_gelu(x)\n        x = self.c_proj(x)\n        x = self.dropout(x)\n        return x\n\n\nclass Block(nn.Module):\n\n    def __init__(self, config):\n        super().__init__()\n        self.ln_1 = nn.LayerNorm(config.n_embd)\n        self.attn = CausalSelfAttention(config)\n        self.ln_2 = nn.LayerNorm(config.n_embd)\n        self.mlp = MLP(config)\n\n    def forward(self, x):\n        x = x + self.attn(self.ln_1(x))\n        x = x + self.mlp(self.ln_2(x))\n        return x\n\n\n@dataclass\nclass GPTConfig:\n    block_size: int = 1024\n    vocab_size: int = 50257\n    n_layer: int = 12\n    n_head: int = 12\n    n_embd: int = 768\n    dropout: float = 0.1\n```",
    "frame_6503.jpg": "```python\nclass Block(nn.Module):\n\n    def __init__(self, config):\n        super().__init__()\n        self.ln_1 = nn.LayerNorm(config.n_embd)\n        self.attn = CausalSelfAttention(config)\n        self.ln_2 = nn.LayerNorm(config.n_embd)\n        self.mlp = MLP(config)\n\n    def forward(self, x):\n        x = x + self.attn(self.ln_1(x))\n        x = x + self.mlp(self.ln_2(x))\n        return x\n\n@dataclass\nclass GPTConfig:\n    block_size: int = 1024\n    vocab_size: int = 50257\n    n_layer: int = 12\n    n_head: int = 12\n    n_embd: int = 768\n    dropout: float = 0.1\n\nclass GPT(nn.Module):\n    \n    def __init__(self, config):\n        super().__init__()\n        assert config.vocab_size is not None\n        assert config.block_size is not None\n        self.config = config\n\n        self.transformer = nn.ModuleDict(dict(\n            wte=nn.Embedding(config.vocab_size, config.n_embd),\n            wpe=nn.Embedding(config.block_size, config.n_embd),\n            drop=nn.Dropout(config.dropout),\n            h=nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n            ln_f=nn.LayerNorm(config.n_embd),\n        ))\n\n        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n```",
    "frame_6504.jpg": "```\n@dataclass\nclass GPTConfig:\n    block_size: int = 1024\n    vocab_size: int = 50257\n    n_layer: int = 12\n    n_head: int = 12\n    n_embd: int = 768\n    dropout: float = 0.1\n\nclass GPT(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        assert config.vocab_size is not None\n        assert config.block_size is not None\n        self.config = config\n        self.transformer = nn.ModuleDict(dict(\n            wte = nn.Embedding(config.vocab_size, config.n_embd),\n            wpe = nn.Embedding(config.block_size, config.n_embd),\n            drop = nn.Dropout(config.dropout),\n            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n            ln_f = nn.LayerNorm(config.n_embd),\n        ))\n\n        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n        # UserWarning: functional_call was passed multiple values for tied weights.\n        # This behavior is deprecated and will be an error in future versions.\n        self.wte.weight = self.lm_head.weight  # https://paperswithcode.com/method/weight-tying\n\n    def forward(self, idx, targets=None):\n        device = idx.device\n        # report number of parameters\n        n_params = sum(p.numel() for p in self.parameters())\n        print(\"number of parameters: %.2fM\" % (n_params / 1e6))\n```\n",
    "frame_6508.jpg": "```python\nclass GPT(nn.Module):\n\n    def __init__(self, config):\n        super().__init__()\n        assert config.vocab_size is not None\n        assert config.block_size is not None\n        self.config = config\n\n        self.transformer = nn.ModuleDict({\n            'wte': nn.Embedding(config.vocab_size, config.n_embd),\n            'wpe': nn.Embedding(config.block_size, config.n_embd),\n            'drop': nn.Dropout(config.dropout),\n            'h': nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n            'ln_f': nn.LayerNorm(config.n_embd),\n        })\n\n        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n        # \"UserWarning: functional_call was passed multiple values for tied weights.\"\n        # This behavior is deprecated and will be an error in future versions\"\n        \n        self.transformer.wte.weight = self.lm_head.weight # https://paperswithcode.com/method/weight-tying\n\n        # report number of parameters\n        n_params = sum(p.numel() for p in self.parameters())\n        print(\"number of parameters: %.2fm\" % (n_params / 1e6))\n\n    def forward(self, idx, targets=None):\n        device = idx.device\n        b, t = idx.size()\n        assert t <= self.config.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.config.block_size}\"\n        \n        pos = torch.arange(0, t, dtype=torch.long, device=device).unsqueeze(0)  # shape (1, t)\n        tok_emb = self.transformer.wte(idx)  # token embeddings of shape (b, t, n_embd)\n        pos_emb = self.transformer.wpe(pos)  # position embeddings of shape (1, t, n_embd)\n        x = self.transformer.drop(tok_emb + pos_emb)\n        for block in self.transformer.h:\n            x = block(x)\n```\n",
    "frame_6512.jpg": "```python\nself.transformer = nn.ModuleDict(dict(\n    wte = nn.Embedding(config.vocab_size, config.n_embd),\n    wpe = nn.Embedding(config.block_size, config.n_embd),\n    drop = nn.Dropout(config.dropout),\n    h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n    ln_f = nn.LayerNorm(config.n_embd)\n))\n\n# report number of parameters\nn_params = sum(p.numel() for p in self.parameters())\nprint(\"number of parameters: %.2fM\" % (n_params / 1e6))\n\ndef forward(self, idx, targets=None):\n    device = idx.device\n    t = idx.size(1)\n    assert t <= self.config.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.config.block_size}\"\n    pos = torch.arange(0, t, dtype=torch.long, device=device).unsqueeze(0) # shape (1, t)\n\n    # forward the GPT model itself\n    tok_emb = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n    pos_emb = self.transformer.wpe(pos) # position embeddings of shape (1, t, n_embd)\n    x = self.transformer.drop(tok_emb + pos_emb)\n    for block in self.transformer.h:\n        x = block(x)\n    x = self.transformer.ln_f(x)\n\n    if targets is not None:\n        # if we are given some desired targets also calculate the loss\n        logits = self.lm_head(x)\n        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n    else:\n        # inference-time mini-optimization: only forward the lm_head on the very last position\n        logits = self.lm_head(x[:, -1, :])\n```\n",
    "frame_6513.jpg": "```python\ndef forward(self, idx, targets=None):\n    device = idx.device\n    b, t = idx.size()\n    assert t <= self.config.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.config.block_size}\"\n    \n    tok_emb = self.transformer.wte(idx)  # token embeddings of shape (b, t, n_emb)\n    pos_emb = self.transformer.wpe(pos)  # position embeddings of shape (1, t, n_emb)\n    x = self.transformer.drop(tok_emb + pos_emb)\n    \n    for block in self.transformer.h:\n        x = block(x)\n\n    x = self.transformer.ln_f(x)\n\n    if targets is not None:\n        # if we are given some desired targets also calculate the loss\n        logits = self.lm_head(x)\n        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n    else:\n        # inference-time mini-optimization: only forward the lm_head on the very last position\n        logits = self.lm_head(x[:, -1, :])  # note: using list [-1] to preserve the time dim\n        loss = None\n\n    return logits, loss\n\ndef crop_block_size(self, block_size):\n    # function implementation here\n```\n",
    "frame_6515.jpg": "```python\npos_emb = self.transformer.wpe(pos) # position embeddings of shape (1, t, n_embd)\nx = self.transformer.drop(tok_emb + pos_emb)\nfor block in self.transformer.h:\n    x = block(x)\n\nif targets is not None:\n    logits = self.lm_head(x.view(-1, logits.size(-1), targets.view(-1, ignore_index=-1))\n    loss = F.cross_entropy(logits.view(-1), logits.size(-1), targets.view(-1), ignore_index=-1)\nelse:\n    logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim\n    loss = None\n    \nreturn logits, loss\n\ndef crop_block_size(self, block_size):\n    assert block_size == self.config.block_size\n    self.config.block_size = block_size\n    self.transformer.wpe.weight = nn.Parameter(self.transformer.wpe.weight[:block_size])\n    for block in self.transformer.h:\n        block.attn.bias = block.attn.bias[:, :, :block_size, :block_size]\n\n@classmethod\ndef from_pretrained(cls, model_type, override_args=None):\n    assert model_type in {'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}\n    override_args = override_args or {} # default to empty dict\n    assert all(k == 'dropout' for k in override_args)\n    print(\"loading weights from pretrained gpt2: %s\" % model_type)\n\n# n_layer, n_head and n_embd are determined from model_type\nconfig.args = {\n    'gpt2': dict(n_layer=12, n_head=12, n_embd=768), # 124M params\n}\n```",
    "frame_6516.jpg": "Here is the extracted code from the image:\n\n```python\ndef from_pretrained(cls, model_type, override_args=None):\n    assert model_type in {'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}\n    override_args = override_args or {}  # default to empty dict\n    assert all(k == 'dropout' for k in override_args)\n    from transformers import GPT2LMHeadModel\n\n    # n_layer, n_head and n_embd are determined from model_type\n    config_args = {\n        'gpt2': dict(n_layer=12, n_head=12, n_embd=768),\n        'gpt2-medium': dict(n_layer=24, n_head=16, n_embd=1024),\n        'gpt2-large': dict(n_layer=36, n_head=20, n_embd=1280),\n        'gpt2-xl': dict(n_layer=48, n_head=25, n_embd=1600),\n    }[model_type]\n\n    # we can override the dropout rate\n    if 'dropout' in override_args:\n        config_args['dropout'] = override_args['dropout']\n\n    # block_size is always 1024 for GPT model checkpoints\n    # ...\n\n    # create a from-scratch initialized minGPT model\n    config = GPTConfig(block_size=1024, **config_args)\n    model = GPT(config)\n    sd = model.state_dict()\n\n    # init a huggingface/transformers model\n    model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n    sd_hf = model_hf.state_dict()\n\n    # copy while ensuring all of the parameters are aligned and match in names and shapes\n    keys = [k for k in sd_hf if not k.endswith('attn.masked_bias')]\n\n    # ...\n```\n\nThis code snippet includes definitions and initialization for a GPT model and configurations related to its parameters.",
    "frame_6517.jpg": "```python\nfrom transformers import GPT2LMHeadModel\nprint(\"loading weights from pretrained gpt: %s\" % model_type)\n\nconfig_args = {\n    'gpt2': dict(n_layer=12, n_head=12, n_embd=768),    # 124M params\n    'gpt2-medium': dict(n_layer=24, n_head=16, n_embd=1024),  # 355M params\n    'gpt2-large': dict(n_layer=36, n_head=20, n_embd=1280),  # 774M params\n    'gpt2-xl': dict(n_layer=48, n_head=25, n_embd=1600),  # 1558M params\n}[model_type]\n\nconfig = GPTConfig(block_size=1024, **config_args)\nmodel = GPT(config)\nsd = model.state_dict()\n\nmodel_hf = GPT2LMHeadModel.from_pretrained(model_type)\nsd_hf = model_hf.state_dict()\n\nkeys = [k for k in sd_hf if not k.endswith('attn.masked_bias')]  # ignore these\ntransposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\nassert len(keys) == len(sd)\n\nfor k in keys:\n    if any(k.endswith(w) for w in transposed):\n        assert sd_hf[k].shape[:-1] == sd[k].shape\n        with torch.no_grad():\n            sd_k.copy_(sd_hf[k])\n```\n",
    "frame_6518.jpg": "```python\ndef configure_optimizers(self, weight_decay, learning_rate, betas):\n    \"\"\"\n    This long function is unfortunately doing something very simple and is being very defensive:\n    we are separating out all parameters of the model into two buckets: those that will experience\n    weight decay for regularization and those that won't (biases, and layernorm/embedding weights).\n    We are then returning the PyTorch optimizer object.\n    \"\"\"\n\n    # separate out all parameters to those that will and won't experience regularizing weight decay\n    decay = set()\n    no_decay = set()\n    whitelist_weight_modules = (torch.nn.Linear, )\n    blacklist_weight_modules = (torch.nn.LayerNorm, torch.nn.Embedding)\n\n    for mn, m in self.named_modules():\n        for pn, p in m.named_parameters():\n            fpn = '%s.%s' % (mn, pn) if mn else pn  # full param name\n            # random note: because named_modules and named_parameters are recursive\n            # we will see the same tensors p many many times, but doing it this way\n            # allows us to know which parent module any tensor p belongs to...\n            if pn.endswith('bias'):\n                no_decay.add(fpn)\n            elif pn.endswith('weight') and isinstance(m, whitelist_weight_modules):\n                decay.add(fpn)\n            elif pn.endswith('weight') and isinstance(m, blacklist_weight_modules):\n                no_decay.add(fpn)\n\n    # Additional code may follow\n```\n",
    "frame_6519.jpg": "```python\ndecay = set()\nno_decay = set()\nwhitelist_weight_modules = []\nblacklist_weight_modules = []\nfor pn, m in self.named_parameters():\n    fpn = '%s.%s' % (nm, pn) if m else pn  # full param name\n    if pn.endswith('bias'):\n        no_decay.add(fpn)\n    elif pn.endswith('weight') and isinstance(m, whitelist_weight_modules):\n        decay.add(fpn)\n    elif pn.endswith('weight') and isinstance(m, blacklist_weight_modules):\n        no_decay.add(fpn)\n\n# validate that we considered every parameter\nparam_dict = {pn: p for pn, p in self.named_parameters()}\ninter_params = decay & no_decay\nunion_params = decay | no_decay\nassert len(inter_params) == 0, \"parameters %s are in both decay/no_decay sets!\" % (str(inter_params),)\nassert len(param_dict.keys()) == len(union_params), \n```",
    "frame_6520.jpg": "```python\n# separate out all parameters to those that will and won't experience regularizing weight decay\ndecay = set()\nno_decay = set()\n\nwhitelist_weight_modules = (torch.nn.Linear,\n                             torch.nn.LayerNorm, torch.nn.Embedding)\nblacklist_weight_modules = set()\n\nfor mm, m in self.named_modules():\n    for pn, p in m.named_parameters():\n        # random note: because named_modules and named_parameters are recursive\n        # we will see the same tensors p many times. but doing it this way\n        # allows us to know which parent module any tensor p belongs to.\n        if pn.endswith('bias'):\n            # all biases will not be decayed\n            no_decay.add(fpn)\n        elif pn.endswith('weight') and isinstance(m, whitelist_weight_modules):\n            # weights of whitelist modules will be weight decayed\n            decay.add(fpn)\n        elif pn.endswith('weight') and isinstance(m, blacklist_weight_modules):\n            # weights of blacklist modules will NOT be weight decayed\n            no_decay.add(fpn)\n\n# subtle: 'transformer.wte.weight' and 'lm_head.weight' are tied, so they\n# will appear in the no_decay and decay sets respectively after the above.\n# in addition, because named_parameters() doesn't return duplicates, it\n# will only return the first occurrence, key'd by 'transformer.wte.weight', below.\n# so let's manually remove 'lm_head.weight' from decay set. This will include\n# this tensor into optimization via transformer.wte.weight only, and not decay.\ndecay.remove('lm_head.weight')\n\n# validate that we considered every parameter\nparam_dict = {pn: p for pn, p in self.named_parameters()}\ninter_params = decay & no_decay\nunion_params = decay | no_decay\nassert len(inter_params) == 0, \"parameters %s made it into both decay/no_decay set!\" % (str(inter_params),)\nassert len(param_dict.keys() - union_params) == 0, \"parameters %s were not separated into either decay/no_decay!\" % (str(param_dict.keys() - union_params),)\n\n# create the pytorch optimizer object\n```",
    "frame_6521.jpg": "Here is the extracted developer code from the image:\n\n```python\nno_decay = set()\nwhitelist_weight_modules = (torch.nn.Linear, torch.nn.Embedding)\nblacklist_weight_modules = (torch.nn.LayerNorm, torch.nn.Embedding)\n\nfor pn, m in self.named_modules():\n    for pn, p in m.named_parameters():\n        fpn = '%s.%s' % (mn, pn) if mn else pn  # full param name\n        # random note: because named_modules and named_parameters are recursive\n        # we will see the same tensors many times many times, but doing it this way \n        # allows us to know which parent module any tensor p belongs to...\n\n        if pn.endswith('bias'):\n            no_decay.add(fpn)\n        elif pn.endswith('weight') and isinstance(m, whitelist_weight_modules):\n            decay.add(fpn)\n        elif pn.endswith('weight') and isinstance(m, blacklist_weight_modules):\n            no_decay.add(fpn)\n\n# subtle: 'transformer.wte.weight' and 'lm_head.weight' are tied, so they \n# will appear in the no_decay and decay sets respectively after the loop.\n# In addition, because named_parameters() doesn't return duplicates, it \n# will only return the first occurrence, keyed by 'transformer.wte.weight', below. \n# so let's manually remove 'lm_head.weight' from decay set. This will include \n# this tensor into optimization via transformer.wte.weight only, and not decayed.\ndecay.remove('lm_head.weight')\n\n# validate that we considered every parameter\nparam_dict = {pn: p for pn, p in self.named_parameters()}\ninter_params = decay & no_decay\nunion_params = decay | no_decay\nassert len(inter_params) == 0, \"parameters %s made it into both decay/no_decay sets!\" % (str(inter_params),)\nassert len(param_dict.keys() - union_params) == 0, \"parameters %s were not separated into either decay/no_decay sets.\" % (str(param_dict.keys() - union_params),)\n\n# create the pytorch optimizer object\noptim_groups = [\n    {\"params\": [param_dict[pn] for pn in sorted(list(decay))], \"weight_decay\": weight_decay},\n    {\"params\": [param_dict[pn] for pn in sorted(list(no_decay))], \"weight_decay\": 0.0},\n]\n```\n\nIf you need anything else, let me know!",
    "frame_6523.jpg": "Code NA",
    "frame_6524.jpg": "```python\n# validate that we considered every parameter\nparam_dict = {pn: p for pn, p in self.named_parameters()}\ninter_params = decay & no_decay\nunion_params = decay | no_decay\nassert len(inter_params) == 0, \"parameters %s made it into both decay/no decay sets!\" % (str(inter_params), )\nassert len(param_dict.keys() & union_params) == 0, \"parameters %s were not separated into either decay/no_decay set!\" % (str(param_dict.keys() & union_params), )\n\n# create the pytorch optimizer object\noptim_groups = [\n    {\"params\": [param_dict[pn] for pn in sorted(list(decay))], \"weight_decay\": weight_decay},\n    {\"params\": [param_dict[pn] for pn in sorted(list(no_decay))], \"weight_decay\": 0.0},\n]\noptimizer = torch.optim.Adam(optim_groups, lr=learning_rate, betas=betas)\n\n@torch.no_grad()\ndef generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n    \"\"\"\n    Take a conditioning sequence of indices idx (LongTensor of shape (b,t)) and complete\n    the sequence max_new_tokens times, feeding the predictions back into the model each time.\n    Most likely you'll want to make sure to be in model.eval() mode of operation for this.\n    \"\"\"\n    for _ in range(max_new_tokens):\n        # if the sequence context is growing too long we must crop it at block_size\n        idx_cond = idx if idx.size(0) < self.config.block_size else idx[-self.config.block_size:]\n        # forward the model to get the logits for the index in the sequence\n        logits, _ = self(idx_cond)\n```\n",
    "frame_6528.jpg": "```python\ndecay = set()\nno_decay = set()\nwhitelist_weight_modules = (torch.nn.Linear, torch.nn.Embedding)\nblacklist_weight_modules = (torch.nn.LayerNorm, torch.nn.Embedding)\n\nfor mn, pn in self.named_parameters():\n    fpn = '%s.%s' % (mn, pn)  # full param name\n    if pn.endswith('bias'):\n        # all biases will not be decayed\n        no_decay.add(fpn)\n    elif pn.endswith('weight') and isinstance(mn, whitelist_weight_modules):\n        # weights of whitelist modules will be weight decayed\n        decay.add(fpn)\n    elif pn.endswith('weight') and isinstance(mn, blacklist_weight_modules):\n        # weights of blacklist modules will NOT be weight decayed\n        no_decay.add(fpn)\n\nparam_dict = {pn: p for pn, p in self.named_parameters()}\ninter_params = decay & no_decay\nunion_params = decay | no_decay\nassert len(inter_params) == 0, \"parameters %s made it into both decay/no_decay sets!\" % (str(inter_params),)\nassert len(param_dict.keys() - union_params) == 0, \"parameters %s were not separated into either decay/no_decay.\" % (str(param_dict.keys() - union_params),)\n\noptim_groups = [\n    {\"params\": [param_dict[ks] for ks in sorted(list(decay))], \"weight_decay\": weight_decay},\n]\n```",
    "frame_6529.jpg": "```python\n@classmethod\ndef from_pretrained(cls, model_type, override_args=None):\n    assert model_type in ('gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl')\n    override_args = override_args or {}  # default to empty dict\n    assert all(k == 'dropout' for k in override_args)\n    print(\"loading weights from pretrained gpt: %s\" % model_type)\n\n    config_args = {\n        'gpt2': dict(n_layer=12, n_head=12, n_embd=768),  # 124M params\n        'gpt2-medium': dict(n_layer=24, n_head=16, n_embd=1024),  # 350M params\n        'gpt2-large': dict(n_layer=36, n_head=20, n_embd=1280),  # 774M params\n        'gpt2-xl': dict(n_layer=48, n_head=25, n_embd=1600),  # 1558M params\n    }[model_type]\n\n    if 'dropout' in override_args:\n        config_args['dropout'] = override_args['dropout']\n    \n    # block_size is always 1024 for GPT model checkpoints\n    # if one wants a lower block_size it has to be done through model surgery\n    # later, by calling crop_block_size()\n    \n    config = GPTConfig(block_size=1024, **config_args)\n    model = GPT(config)\n    sd = model.state_dict()\n\n    model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n    sd_hf = model_hf.state_dict()\n\n    keys = [k for k in sd_hf if not k.endswith('.masked_bias')]  # ignore these\n    transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n    # basically the openai checkpoint uses a \"Conv1D\" module, but we only want to use a vanilla Linear\n```\n",
    "frame_6530.jpg": "```python\nclass MLP(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.c_fc = nn.Linear(config.n_embd, 4 * config.n_embd)\n        self.c_proj = nn.Linear(4 * config.n_embd, config.n_embd)\n        self.dropout = nn.Dropout(config.dropout)\n\n    def forward(self, x):\n        x = self.c_fc(x)\n        x = new_geul(x)\n        x = self.c_proj(x)\n        x = self.dropout(x)\n        return x\n\nclass Block(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.ln_1 = nn.LayerNorm(config.n_embd)\n        self.attn = CausalSelfAttention(config)\n        self.ln_2 = nn.LayerNorm(config.n_embd)\n        self.mlp = MLP(config)\n\n    def forward(self, x):\n        x = x + self.attn(self.ln_1(x))\n        x = x + self.mlp(self.ln_2(x))\n        return x\n```",
    "frame_6531.jpg": "```python\nimport math\nfrom dataclasses import dataclass\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\n\n@torch.jit.script  # good to enable when not using torch.compile, disable when using (our default)\ndef new_gelu(x):\n    \"\"\"\n    Implementation of the GELU activation function currently in Google BERT repo (identical to OpenAI GPT).\n    Reference: Gaussian Error Linear Units (GELU) paper: https://arxiv.org/abs/1606.08415\n    \"\"\"\n    return 0.5 * x * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (x + 0.044715 * torch.pow(x, 3.0))))\n\nclass CausalSelfAttention(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        assert config.n_embd % config.n_head == 0\n        # key, query, value projections for all heads, but in a batch\n        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)  # output projection\n        self.c_proj = nn.Linear(config.n_embd, config.n_embd)  # regularization\n        self.attn_dropout = nn.Dropout(config.dropout)\n        self.resid_dropout = nn.Dropout(config.dropout)\n        self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size)))\n        self.bias = self.bias.view(1, 1, config.block_size, config.block_size)\n\n    def forward(self, x):\n        B, T, C = x.size()  # batch size, sequence length, embedding dimensionality (n_embd)\n```\n",
    "frame_6532.jpg": "```python\nimport math\nfrom dataclasses import dataclass\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\n\n# @torch.jit.script # good to enable when not using torch.compile, disable when using (our default)\ndef new_gelu(x):\n    \"\"\"\n    Implementation of the GELU activation function currently in Google BERT repo (identical to OpenAI GPT).\n    Reference: Gaussian Error Linear Units (GELU) paper: https://arxiv.org/abs/1606.08415\n    \"\"\"\n    return 0.5 * x * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (x + 0.044715 * torch.pow(x, 3.0))))\n```",
    "frame_6535.jpg": "Code NA",
    "frame_6563.jpg": "```python\nmodel = BigramLanguageModel()\nm = model.to(device)\n# print the number of parameters in the model\nprint(sum(p.numel() for p in m.parameters()) / 1e6, 'M parameters')\n\n# create a PyTorch optimizer\noptimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n\nfor iter in range(max_iters):\n    # every once in a while evaluate the loss on train and val sets\n    if iter % eval_interval == 0 or iter == max_iters - 1:\n        losses = estimate_loss()\n        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n\n    # sample a batch of data\n    xb, yb = get_batch('train')\n    \n    # evaluate the loss\n```",
    "frame_6564.jpg": "```python\nidx = idx[:, -block_size:]\n# get the predictions\nlogits, loss = self(idx_cond)\n# Focus only on the last time step\nprobs = F.softmax(logits, dim=-1)  # becomes (B, M)\n# sample from the distribution\nidx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n# append sampled index to the running sequence\nidx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\nreturn idx\n\nmodel = BigramLanguageModel()\nm = model.to(device)\n# print the number of parameters in the model\nprint(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n\n# create a PyTorch optimizer\noptimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n\nfor iter in range(max_iters):\n    # every once in a while evaluate the loss on train and val sets\n    if iter % eval_interval == 0 or iter == max_iters - 1:\n        losses = estimate_loss()\n        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n\n    # sample a batch of data\n    xb, yb = get_batch('train')\n\n    # evaluate the loss\n```\n",
    "frame_6575.jpg": "```python\n# create a mapping from characters to integers\nstoi = { char: i for i, char in enumerate(chars) }\nitos = { i: char for i, char in enumerate(chars) }\nencode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\ndecode = lambda l: ''.join(itos[i] for i in l) # decoder: take a list of integers, output a string\n\n# Train and test splits\ndata = torch.tensor(encode(text), dtype=torch.long)\nn = int(0.9 * len(data))  # first 90% will be train, rest val\ntrain_data = data[:n]\nval_data = data[n:]\n\n# data loading\ndef get_batch(split):\n    # generate a small batch of data of inputs x and targets y\n    data = train_data if split == 'train' else val_data\n    ix = torch.randint(len(data), (block_size, batch_size,))\n    x = torch.stack([data[i:i+block_size] for i in ix])\n    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n    x, y = x.to(device), y.to(device)\n    return x, y\n\n@torch.no_grad()\ndef estimate_loss():\n    out = {}\n    model.eval()\n    for split in ['train', 'val']:\n        losses = torch.zeros(eval_iters)\n        for k in range(eval_iters):\n            X, Y = get_batch(split)\n            logits, loss = model(X, Y)\n```\n",
    "frame_6576.jpg": "```python\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\n\n# hyperparameters\nbatch_size = 64  # how many independent sequences will we process in parallel?\nblock_size = 256  # what is the maximum context length for predictions?\nmax_iters = 5000\neval_interval = 500\nlearning_rate = 3e-4\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\neval_iters = 200\nn_embd = 384\nn_head = 6\nn_layer = 6\ndropout = 0.2\n\ntorch.manual_seed(1337)\n\n# wget https://raw.githubusercontent.com/karpathy char-rnn/master/data/tinyshakespeare/input.txt\nwith open('input.txt', 'r', encoding='utf-8') as f:\n    text = f.read()\n\n# here are all the unique characters that occur in this text\nchars = sorted(list(set(text)))\n\nvocab_size = len(chars)\n# create a mapping from characters to integers\nstoi = {ch: i for i, ch in enumerate(chars)}\nitos = {i: ch for i, ch in enumerate(chars)}\nencode = lambda s: [stoi[c] for c in s]  # encoder: take a string, output a list of integers\n```",
    "frame_6578.jpg": "Here is the extracted developer code from the image:\n\n```python\nclass BigramLanguageModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        # each token directly reads off the logits for the next token from a lookup table\n        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n        self.blocks = nn.Sequential(*(Block(n_embd, n_head) for _ in range(n_layer)))\n        self.ln_f = nn.LayerNorm(n_embd)  # final layer norm\n        self.ln_head = nn.Linear(n_embd, vocab_size)\n\n    def forward(self, idx, targets=None):\n        B, T = idx.shape\n\n        # idx and targets are both (B, T) tensor of integers\n        tok_emb = self.token_embedding_table(idx)  # (B,T,C)\n        pos_emb = self.position_embedding_table(torch.arange(T, device=device))  # (T,C)\n        x = tok_emb + pos_emb  # (B,T,C)\n        x = self.blocks(x)  # (B,T,C)\n        x = self.ln_f(x)  # (B,T,C)\n        logits = self.ln_head(x)  # (B,T,vocab_size)\n\n        if targets is None:\n            loss = None\n        else:\n            B, T, C = logits.shape\n            logits = logits.view(B*T, C)\n            targets = targets.view(B*T)\n            loss = F.cross_entropy(logits, targets)\n```\n\nIf you need any further assistance, let me know!",
    "frame_6579.jpg": "Here is the extracted developer code from the image you provided:\n\n```python\nmodel = BigramLanguageModel()\n# print the number of parameters in the model\nprint(sum(p.numel() for p in m.parameters()) / 1e6, 'M parameters')\n\n# create a PyTorch optimizer\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n\nfor iter in range(max_iters):\n    # every once in a while evaluate the loss on train and val sets\n    if iter % eval_interval == 0 or iter == max_iters - 1:\n        losses = estimate_loss()\n        print(f\"step {iter}: train loss {losses['train'][:4]}, val loss {losses['val'][:4]}\")\n    \n    # sample a batch of data\n    xb, yb = get_batch('train')\n    \n    # evaluate the loss\n    logits, loss = model(xb, yb)\n    optimizer.zero_grad(set_to_none=True)\n    loss.backward()\n    optimizer.step()\n    \n    # generate from the model\n```\n\nIf you need any more help or further details, just let me know!",
    "frame_6585.jpg": "Here's the extracted code from the image:\n\n```python\n# get the predictions\nlogits, loss = self(idx_cond)\n# focus only on the last step\nlogits = logits[-1, :, :] if B becomes (B, C)\n# apply softmax to get probabilities\nprobs = F.softmax(logits, dim=-1)  # (B, C)\n# sample from the distribution\nidx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n# append sampled index to the running sequence\nidx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\nreturn idx\n\nmodel = BigramLanguageModel()\n# print the number of parameters in the model\nprint(sum(p.numel() for p in model.parameters())/1e6, 'M parameters')\n\n# create a PyTorch optimizer\noptimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n\nfor iter in range(max_iters):\n    # every once in a while evaluate the loss on train and val sets\n    if iter % eval_interval == 0 or iter == max_iters - 1:\n        losses = estimate_loss()\n        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n\n    # sample a batch of data\n    xb, yb = get_batch('train')\n\n    # evaluate the loss\n    ...\n```\n\nThis appears to be a snippet from a Python program related to training a bigram language model using PyTorch.",
    "frame_6595.jpg": "```python\n# get the predictions\nlogits, loss = self(idx_cond)\n# focus only on the last time step\nlogits = logits[:, -1, :]  # i becomes (B, C)\n# apply softmax to get probabilities\nprobs = F.softmax(logits, dim=-1)  # (B, T+1)\n# sample from the distribution\nidx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n# append sampled index to the running sequence\nidx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\nreturn idx\n\nmodel = BigramLanguageModel()\nmodel = model.to(device)\n# print the number of parameters in the model\nprint(sum(p.numel() for p in model.parameters()) / 1e6, 'M parameters')\n\n# create a PyTorch optimizer\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n\nfor iter in range(max_iters):\n    # every once in a while evaluate the loss on train and val sets\n    if iter % eval_interval == 0 or iter == max_iters - 1:\n        losses = estimate_loss()\n        print(f'step {iter}: train loss {losses[\"train\"]:.4f}, val loss {losses[\"val\"]:.4f}')\n\n    # sample a batch of data\n    xb, yb = get_batch('train')\n    \n    # evaluate the loss\n```",
    "frame_6615.jpg": "Code NA",
    "frame_6678.jpg": "```python\nmodel = BigramLanguageModel()\nmodel = model.to(device)\nprint(sum(p.numel() for p in model.parameters()) / 1e6, 'M parameters')\n\noptimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n\nfor iter in range(max_iters):\n    if iter % eval_interval == 0 or iter == max_iters - 1:\n        losses = estimate_loss()\n        print(f'step {iter}: train loss {losses[\"train\"]:1.4f}, val loss {losses[\"val\"]:1.4f}')\n    \n    xb, yb = get_batch('train')\n```",
    "frame_6680.jpg": "Code NA",
    "frame_6752.jpg": "```go\nresultWorkerErr := make(chan error)\ndefer close(resultWorkerErr)\ngo func() {\n    defer cancel()\n    resultWorkerErr <- b.resultWorker(ctx)\n}()\n\nerr := b.worker(ctx)\ncancel()\nif err != nil {\n    return <-resultWorkerErr\n}\nreturn multierror.Append(err, <-resultWorkerErr)\n```",
    "frame_6754.jpg": "Code NA",
    "frame_6889.jpg": "```python\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\n\n# hyperparameters\nbatch_size = 64  # how many independent sequences will we process in parallel?\nblock_size = 256  # what is the maximum context length for predictions?\nmax_iters = 5000\neval_interval = 500\nlearning_rate = 3e-4\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\neval_iters = 200\nn_embd = 384\nn_head = 6\nn_layer = 6\ndropout = 0.2\n\ntorch.manual_seed(1337)\n\n# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\nwith open('input.txt', 'r', encoding='utf-8') as f:\n    text = f.read()\n```",
    "frame_6890.jpg": "Code NA",
    "frame_6894.jpg": "```python\nimport torch\nimport torch.nn functional as F\n\n# hyperparameters\nbatch_size = 64  # how many independent sequences will we process in parallel?\nblock_size = 256  # what is the maximum context length for predictions?\nmax_iters = 5000\neval_interval = 500\nlearning_rate = 3e-4\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\neval_iters = 200\nn_embd = 384\nn_head = 6\nn_layer = 6\ndropout = 0.2\n\ntorch.manual_seed(1337)\n\n# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\nwith open('input.txt', 'r', encoding='utf-8') as f:\n    text = f.read()\n```",
    "frame_6895.jpg": "```python\ndef generate(self, idx, max_new_tokens):\n    # idx is (B, T) array of indices in the current context\n    for _ in range(max_new_tokens):\n        # crop idx to the last block_size tokens\n        idx_cond = idx[:, -block_size:]  # get the predictions\n        logits, loss = self(idx_cond)\n        # focus only on the last time step\n        logits = logits[:, -1, :]  # becomes (B, C)\n        # apply softmax to get probabilities\n        probs = F.softmax(logits, dim=-1)  # (B, C)\n        # sample from the distribution\n        idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n        # append sampled index to the running sequence\n        idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n    return idx\n```\n",
    "frame_6896.jpg": "```python\ncontext = torch.zeros(1, 1, dtype=torch.long, device=device)\nprint(decode(m.generate(context, max_new_tokens=500)[0].tolist()))\n# open('more.txt', 'w').write(decode(m.generate(context, max_new_tokens=10000)[0].tolist()))\n```",
    "frame_6903.jpg": "```python\n# evaluate the loss\nlogits, loss = model(xb, yb)\noptimizer.zero_grad(set_to_none=True)\nloss.backward()\noptimizer.step()\n\n# generate from the model\ncontext = torch.zeros(1, 1, dtype=torch.long, device=device)\nprint(decode(m.generate(context, max_new_tokens=500)[0].tolist()))\n# open('more.txt', 'w').write(decode(m.generate(context, max_new_tokens=10000)[0].tolist()))\n```",
    "frame_6906.jpg": "```python\ncontext = torch.zeros(1, 1, dtype=torch.long, device=device)\nprint(decode(m.generate(context, max_new_tokens=500)[0].tolist()))\n# open('more.txt', 'w').write(decode(m.generate(context, max_new_tokens=10000)[0].tolist()))\n```",
    "frame_6909.jpg": "Code NA",
    "frame_6910.jpg": "```python\n# version 4: self-attention!\ntorch.manual_seed(1337)\nB, T, C = 4, 8, 32  # batch, time, channels\nx = torch.randn(B, T, C)\n\n# let's see a single Head perform self-attention\nhead_size = 16\nkey = nn.Linear(C, head_size, bias=False)\nquery = nn.Linear(C, head_size, bias=False)\nvalue = nn.Linear(C, head_size, bias=False)\nk = key(x)\nq = query(x)\nwei = q @ k.transpose(-2, -1)  # (B, T, 16) @ (B, 16, T) ---> (B, T, T)\nwei = torch.zeros((T, T))\nwei = wei.masked_fill(wei == 0, float('-inf'))\nwei = F.softmax(wei, dim=-1)\n\nv = value(x)\nout = wei @ v\nout.shape\n\ntorch.Size([4, 8, 16])\n```",
    "frame_6914.jpg": "Code NA"
}