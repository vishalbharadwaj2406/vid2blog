["```bash\npython sample_streaming.py --out_dir=out--shakespeare-char\n```\n\n```python\n# read it in to inspect it\nwith open('input.txt', 'r', encoding='utf-8') as f:\n    text = f.read()\n\nprint(\"length of dataset in characters: \", len(text))\n\n# let's look at the first 1000 characters\nprint(text[:1000])\n\n# here are all the unique characters that occur in this text\nchars = sorted(list(set(text)))\nvocab_size = len(chars)\nprint(''.join(chars))\nprint(vocab_size)\n\n# create a mapping from characters to integers\nstoi = {ch: i for i, ch in enumerate(chars)}\nitos = {i: ch for i, ch in enumerate(chars)}\nencode = lambda s: [stoi[ch] for ch in s]  # encoder: take a string, output a list of integers\ndecode = lambda l: ''.join([itos[i] for i in l])  # decoder: take a list of integers, output a string\n\nprint(encode(\"hii there\"))\nprint(decode(encode(\"hii there\")))\n```", "```python\n# read it in to inspect it\nwith open('input.txt', 'r', encoding='utf-8') as f:\n    text = f.read()\n\nprint(\"length of dataset in characters: \", len(text))\n\n# let's look at the first 1000 characters\nprint(text[:1000])\n\n# here are all the unique characters that occur in this text\nchars = sorted(list(set(text)))\nvocab_size = len(chars)\nprint(''.join(chars))\nprint(vocab_size)\n\n# create a mapping from characters to integers\nstoi = {ch: i for i, ch in enumerate(chars)}\nitos = {i: ch for i, ch in enumerate(chars)}\nencode = lambda s: [stoi[c] for c in s]  # encoder: take a string, output a list of integers\ndecode = lambda l: ''.join([itos[i] for i in l])  # decoder: take a list of integers, output a string\n\nprint(encode(\"hii there\"))\nprint(decode(encode(\"hii there\")))\n\n# let's now encode the entire text dataset and store it into a torch.Tensor\nimport torch  # we use PyTorch: https://pytorch.org\ndata = torch.tensor(encode(text), dtype=torch.long)\nprint(data.shape, data.dtype)\nprint(data[:1000])  # the 1000 characters we looked at earlier will to the GPT look like this\n\n# Let's now split up the data into train and validation sets\nn = int(0.9 * len(data))  # first 90% will be train, rest val\ntrain_data = data[:n]\nval_data = data[n:]\n\nblock_size = 8\ntrain_data[:block_size + 1]\n\nx = train_data[:block_size]\ny = train_data[1:block_size + 1]\nfor t in range(block_size):\n    context = x[t:t + 1]\n    target = y[t]\n    print(f\"when input is {context} the target: {target}\")\n\nimport tiktoken\nenc = tiktoken.get_encoding('gpt2')\nassert enc.decode(enc.encode('hello world')) == \"hello world\"\nenc.n_vocab\nenc.encode(\"hii there\")\nenc.decode([71, 4178, 612])\n\nprint('woot')\n```", "```python\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\ntorch.manual_seed(1337)\n\n# Let's now split up the data into train and validation sets\nn = int(0.9 * len(data))  # first 90% will be train, rest val\ntrain_data = data[:n]\nval_data = data[n:]\n\nblock_size = 8\ntrain_data[:block_size + 1]\n\nx = train_data[:block_size]\ny = train_data[1:block_size + 1]\nfor t in range(block_size):\n    context = x[t:t + 1]\n    target = y[t]\n    print(f\"when input is {context} the target: {target}\")\n\nbatch_size = 4  # how many independent sequences will we process in parallel?\n\ndef get_batch(split):\n    # generate a small batch of data of inputs x and targets y\n    data = train_data if split == 'train' else val_data\n    ix = torch.randint(len(data) - block_size, (batch_size,))\n    x = torch.stack([data[i:i + block_size] for i in ix])\n    y = torch.stack([data[i + 1:i + block_size + 1] for i in ix])\n    return x, y\n\nxb, yb = get_batch('train')\nprint('inputs:')\nprint(xb.shape)\nprint(xb)\nprint('targets:')\nprint(yb.shape)\nprint(yb)\n\nprint('-----')\n\nfor b in range(batch_size):  # batch dimension\n    for t in range(block_size):  # time dimension\n        context = xb[b, :t + 1]\n        target = yb[b, t]\n        print(f\"when input is {context.tolist()} the target: {target}\")\n\nclass BigramLanguageModel(nn.Module):\n    def __init__(self, vocab_size):\n        super().__init__()\n        # each token directly reads off the logits for the next token from a lookup table\n        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n\n    def forward(self, idx, targets):\n        # idx and targets are both (B,T) tensor of integers\n        logits = self.token_embedding_table(idx)  # (B,T,C)\n        loss = F.cross_entropy(logits, targets)\n        return logits, loss\n\nm = BigramLanguageModel(vocab_size)\nlogits, loss = m(xb, yb)\nprint(logits.shape)\n```", "```python\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\ntorch.manual_seed(1337)\n\nclass BigramLanguageModel(nn.Module):\n    def __init__(self, vocab_size):\n        super().__init__()\n        # each token directly reads off the logits for the next token from a lookup table\n        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n\n    def forward(self, idx, targets=None):\n        # idx and targets are both (B,T) tensor of integers\n        logits = self.token_embedding_table(idx)  # (B,T,C)\n\n        if targets is None:\n            loss = None\n        else:\n            B, T, C = logits.shape\n            logits = logits.view(B*T, C)\n            targets = targets.view(B*T)\n            loss = F.cross_entropy(logits, targets)\n\n        return logits, loss\n\n    def generate(self, idx, max_new_tokens):\n        # idx is (B, T) array of indices in the current context\n        for _ in range(max_new_tokens):\n            # get the predictions\n            logits, _ = self(idx)\n            # focus only on the last time step\n            logits = logits[:, -1, :]  # becomes (B, C)\n            # apply softmax to get probabilities\n            probs = F.softmax(logits, dim=-1)  # (B, C)\n            # sample from the distribution\n            idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n            # append sampled index to the running sequence\n            idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n        return idx\n\nm = BigramLanguageModel(vocab_size)\nlogits, loss = m(xb, yb)\nprint(logits.shape)\nprint(loss)\n\nidx = torch.zeros((1, 1), dtype=torch.long)\nprint(decode(m.generate(idx, max_new_tokens=100)[0].tolist()))\n```", "```python\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\n\n# hyperparameters\nbatch_size = 32  # how many independent sequences will we process in parallel?\nblock_size = 8  # what is the maximum context length for predictions?\nmax_iters = 10000\neval_interval = 100\nlearning_rate = 1e-2\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\neval_iters = 200\n\ntorch.manual_seed(1337)\n\n# load data\nwith open('input.txt', 'r', encoding='utf-8') as f:\n    text = f.read()\n\n# here are all the unique characters that occur in this text\nchars = sorted(list(set(text)))\nvocab_size = len(chars)\n\n# create a mapping from characters to integers\nstoi = {ch: i for i, ch in enumerate(chars)}\nitos = {i: ch for i, ch in enumerate(chars)}\nencode = lambda s: [stoi[c] for c in s]  # encoder: take a string, output a list of integers\ndecode = lambda l: ''.join([itos[i] for i in l])  # decoder: take a list of integers, output a string\n\n# Train and test splits\ndata = torch.tensor(encode(text), dtype=torch.long)\nn = int(0.9 * len(data))  # first 90% will be train, rest val\ntrain_data = data[:n]\nval_data = data[n:]\n\n# data loading\ndef get_batch(split):\n    # generate a small batch of data of inputs x and targets y\n    data = train_data if split == 'train' else val_data\n    ix = torch.randint(len(data) - block_size, (batch_size,))\n    x = torch.stack([data[i:i + block_size] for i in ix])\n    y = torch.stack([data[i + 1:i + block_size + 1] for i in ix])\n    x, y = x.to(device), y.to(device)\n    return x, y\n\n@torch.no_grad()\ndef estimate_loss():\n    out = {}\n    model.eval()\n    for split in ['train', 'val']:\n        losses = torch.zeros(eval_iters)\n        for k in range(eval_iters):\n            X, Y = get_batch(split)\n            logits, loss = model(X, Y)\n            losses[k] = loss.item()\n        out[split] = losses.mean()\n    model.train()\n    return out\n\n# super simple bigram model\nclass BigramLanguageModel(nn.Module):\n\n    def __init__(self, vocab_size):\n        super().__init__()\n        # each token directly reads off the logits for the next token from a lookup table\n        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n\n    def forward(self, idx, targets=None):\n        # idx and targets are both (B,T) tensor of integers\n        logits = self.token_embedding_table(idx)  # (B,T,C)\n        if targets is None:\n            loss = None\n        else:\n            B, T, C = logits.shape\n            logits = logits.view(B*T, C)\n            targets = targets.view(B*T)\n            loss = F.cross_entropy(logits, targets)\n\n        return logits, loss\n\n    def generate(self, idx, max_new_tokens):\n        # idx is (B, T) array of indices in the current context\n        for _ in range(max_new_tokens):\n            # get the predictions\n            logits, loss = self(idx)\n            # focus only on the last time step\n            logits = logits[:, -1, :]  # becomes (B, C)\n            # apply softmax to get probabilities\n            probs = F.softmax(logits, dim=-1)  # (B, C)\n            # sample from the distribution\n            idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n            # append sampled index to the running sequence\n            idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n        return idx\n\nmodel = BigramLanguageModel(vocab_size)\nmodel = model.to(device)\n\n# create a PyTorch optimizer\noptimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n\nfor iter in range(max_iters):\n    # every once in a while evaluate the loss on train and val sets\n    if iter % eval_interval == 0:\n        losses = estimate_loss()\n        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n\n    # sample a batch of data\n    xb, yb = get_batch('train')\n\n    # evaluate the loss\n    logits, loss = model(xb, yb)\n    optimizer.zero_grad(set_to_none=True)\n    loss.backward()\n    optimizer.step()\n\n# generate from the model\ncontext = torch.zeros((1, 1), dtype=torch.long, device=device)\nprint(decode(model.generate(context, max_new_tokens=500)[0].tolist()))\n```", "```python\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\n\n# hyperparameters\nbatch_size = 32  # how many independent sequences will we process in parallel?\nblock_size = 8  # what is the maximum context length for predictions?\nmax_iters = 3000\neval_interval = 300\nlearning_rate = 1e-2\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\neval_iters = 200\n\ntorch.manual_seed(1337)\n\n# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\nwith open('input.txt', 'r', encoding='utf-8') as f:\n    text = f.read()\n\n# create a mapping from characters to integers\nchars = sorted(list(set(text)))\nvocab_size = len(chars)\nstoi = {ch: i for i, ch in enumerate(chars)}  # encoder: take a string, output a list of integers\nitos = {i: ch for i, ch in enumerate(chars)}  # decoder: take a list of integers, output a string\n\n# Train and test splits\ndata = torch.tensor([stoi[c] for c in text], dtype=torch.long)\nn = int(0.9 * len(data))  # first 90% will be train, rest val\ntrain_data = data[:n]\nval_data = data[n:]\n\n# data loading\ndef get_batch(split):\n    # generate a small batch of data of inputs x and targets y\n    data = train_data if split == 'train' else val_data\n    ix = torch.randint(len(data) - block_size, (batch_size,))\n    x = torch.stack([data[i:i + block_size] for i in ix])\n    y = torch.stack([data[i + 1:i + block_size + 1] for i in ix])\n    x, y = x.to(device), y.to(device)\n    return x, y\n\n@torch.no_grad()\ndef estimate_loss():\n    out = {}\n    model.eval()\n    for split in ['train', 'val']:\n        losses = torch.zeros(eval_iters)\n        for k in range(eval_iters):\n            x, y = get_batch(split)\n            logits, loss = model(x, y)\n            losses[k] = loss.item()\n        out[split] = losses.mean()\n    model.train()\n    return out\n\n# super simple bigram model\nclass BigramLanguageModel(nn.Module):\n    def __init__(self, vocab_size):\n        super().__init__()\n        # each token directly reads off the logits for the next token from a lookup table\n        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n\n    def forward(self, idx, targets=None):\n        # idx and targets are both (B,T) tensor of integers\n        logits = self.token_embedding_table(idx)  # (B,T,C)\n\n        if targets is None:\n            loss = None\n        else:\n            B, T, C = logits.shape\n            logits = logits.view(B*T, C)\n            targets = targets.view(B*T)\n            loss = F.cross_entropy(logits, targets)\n\n        return logits, loss\n\n    def generate(self, idx, max_new_tokens):\n        # idx is (B, T) array of indices in the current context\n        for _ in range(max_new_tokens):\n            # get the predictions\n            logits, _ = self(idx)\n            # focus only on the last time step\n            logits = logits[:, -1, :]  # becomes (B, C)\n            # apply softmax to get probabilities\n            probs = F.softmax(logits, dim=-1)  # (B, C)\n            # sample from the distribution\n            idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n            # append sampled index to the running sequence\n            idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n        return idx\n\nmodel = BigramLanguageModel(vocab_size)\nmodel = model.to(device)\n\n# create a PyTorch optimizer\noptimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n\nfor iter in range(max_iters):\n    # every once in a while evaluate the loss on train and val sets\n    if iter % eval_interval == 0:\n        losses = estimate_loss()\n        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n\n    # sample a batch of data\n    xb, yb = get_batch('train')\n\n    # evaluate the loss\n    logits, loss = model(xb, yb)\n    optimizer.zero_grad(set_to_none=True)\n    loss.backward()\n    optimizer.step()\n\n# generate from the model\ncontext = torch.zeros((1, 1), dtype=torch.long, device=device)\nprint(decode(model.generate(context, max_new_tokens=500)[0].tolist()))\n```", "```python\nimport torch\nimport torch.nn.functional as F\n\n# Set random seed\ntorch.manual_seed(1337)\n\n# Define dimensions\nB, T, C = 4, 8, 2  # batch, time, channels\n\n# Generate random tensor\nx = torch.randn(B, T, C)\n\n# Initialize xbow\nxbow = torch.zeros((B, T, C))\n\n# Compute xbow\nfor b in range(B):\n    for t in range(T):\n        xprev = x[b, :t+1]  # (t, C)\n        xbow[b, t] = torch.mean(xprev, 0)\n\n# Compute weighted xbow using lower triangular matrix\nwei = torch.tril(torch.ones(T, T))\nwei = wei / wei.sum(1, keepdim=True)\nxbow2 = wei @ x  # (B, T, C) @ (B, T, C) ----> (B, T, C)\n\n# Check if xbow and xbow2 are close\ntorch.allclose(xbow, xbow2)\n\n# Version 3: use Softmax\ntril = torch.tril(torch.ones(T, T))\nwei = torch.zeros((T, T))\nwei = wei.masked_fill(tril == 0, float('-inf'))\nwei = F.softmax(wei, dim=-1)\nxbow3 = wei @ x\ntorch.allclose(xbow, xbow3)\n\n# Additional code\ntorch.manual_seed(42)\na = torch.tril(torch.ones(3, 3))\na = a / torch.sum(a, 1, keepdim=True)\nb = torch.randint(0, 10, (3, 2)).float()\nc = a @ b\nprint('a=')\nprint(a)\nprint('b=')\nprint(b)\nprint('c=')\nprint(c)\n```", "```python\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\n# hyperparameters\nbatch_size = 32  # how many independent sequences will we process in parallel?\nblock_size = 8   # what is the maximum context length for predictions?\nmax_iters = 3000\neval_interval = 300\nlearning_rate = 1e-2\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\neval_iters = 200\n\ntorch.manual_seed(1337)\n\n# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\nwith open('input.txt', 'r', encoding='utf-8') as f:\n    text = f.read()\n\n# here are all the unique characters that occur in this text\nchars = sorted(list(set(text)))\nvocab_size = len(chars)\n# create a mapping from characters to integers\nstoi = {ch: i for i, ch in enumerate(chars)}\nitos = {i: ch for i, ch in enumerate(chars)}\nencode = lambda s: [stoi[c] for c in s]  # encoder: take a string, output a list of integers\ndecode = lambda l: ''.join([itos[i] for i in l])  # decoder: take a list of integers, output a string\n\n# super simple bigram model\nclass BigramLanguageModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        # each token directly reads off the logits for the next token from a lookup table\n        self.token_embedding_table = nn.Embedding(vocab_size, block_size)\n        self.position_embedding_table = nn.Embedding(block_size, block_size)\n        self.lm_head = nn.Linear(block_size, vocab_size)\n\n    def forward(self, idx, targets=None):\n        B, T = idx.shape\n        # idx and targets are both (B,T) tensor of integers\n        tok_emb = self.token_embedding_table(idx)  # (B,T,C)\n        pos_emb = self.position_embedding_table(torch.arange(T, device=device))  # (T,C)\n        x = tok_emb + pos_emb  # (B,T,C)\n        logits = self.lm_head(x)  # (B,T,vocab_size)\n\n        if targets is None:\n            loss = None\n        else:\n            B, T, C = logits.shape\n            logits = logits.view(B*T, C)\n            targets = targets.view(B*T)\n            loss = F.cross_entropy(logits, targets)\n\n        return logits, loss\n\n    def generate(self, idx, max_new_tokens):\n        # idx is (B,T) array of indices in the current context\n        for _ in range(max_new_tokens):\n            # get the predictions\n            logits, loss = self(idx)\n            logits = logits[:, -1, :]  # becomes (B, C)\n            # apply softmax to get probabilities\n            probs = F.softmax(logits, dim=-1)  # (B, C)\n            # sample from the distribution\n            idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n            # append sampled index to the running sequence\n            idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n        return idx\n\n# version 4: self-attention!\ntorch.manual_seed(1337)\nB, T, C = 4, 8, 32  # batch, time, channels\nx = torch.randn(B, T, C)\n\n# let's see a single Head perform self-attention\nhead_size = 16\nkey = nn.Linear(C, head_size, bias=False)\nquery = nn.Linear(C, head_size, bias=False)\nk = key(x)  # (B, T, 16)\nq = query(x)  # (B, T, 16)\n\ntril = torch.tril(torch.ones(T, T))\nwei = torch.zeros((T, T))\nwei = wei.masked_fill(tril == 0, float('-inf'))\nwei = F.softmax(wei, dim=-1)\nout = wei @ x\n\nout.shape\n```", "```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n# version 4: self-attention!\ntorch.manual_seed(1337)\nB, T, C = 4, 8, 32  # batch, time, channels\nx = torch.randn(B, T, C)\n\n# let's see a single Head perform self-attention\nhead_size = 16\nkey = nn.Linear(C, head_size, bias=False)\nquery = nn.Linear(C, head_size, bias=False)\nvalue = nn.Linear(C, head_size, bias=False)\n\nk = key(x)  # (B, T, 16)\nq = query(x)  # (B, T, 16)\nwei = q @ k.transpose(-2, -1)  # (B, T, 16) @ (B, 16, T) ---> (B, T, T)\n\ntril = torch.tril(torch.ones(T, T))\nwei = wei.masked_fill(tril == 0, float('-inf'))\nwei = F.softmax(wei, dim=-1)\n\nv = value(x)\nout = wei @ v\n\nout.shape\n```", "```python\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\n\n# hyperparameters\nbatch_size = 32  # how many independent sequences will we process in parallel?\nblock_size = 8  # what is the maximum context length for predictions?\nmax_iters = 5000\neval_interval = 500\nlearning_rate = 1e-3\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\neval_iters = 200\nn_embd = 32\n\ntorch.manual_seed(1337)\n\n# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\nwith open('input.txt', 'r', encoding='utf-8') as f:\n    text = f.read()\n\n# here are all the unique characters that occur in this text\nchars = sorted(list(set(text)))\nvocab_size = len(chars)\n\n# create a mapping from characters to integers\nstoi = {c: i for i, c in enumerate(chars)}\nitos = {i: c for i, c in enumerate(chars)}\nencode = lambda s: [stoi[c] for c in s]  # encoder: take a string, output a list of integers\ndecode = lambda l: ''.join([itos[i] for i in l])  # decoder: take a list of integers, output a string\n\n# Train and test splits\ndata = torch.tensor(encode(text), dtype=torch.long)\nn = int(0.9 * len(data))  # first 90% will be train, rest val\ntrain_data = data[:n]\nval_data = data[n:]\n\n# data loading\ndef get_batch(split):\n    # generate a small batch of data of inputs x and targets y\n    data = train_data if split == 'train' else val_data\n    ix = torch.randint(len(data) - block_size, (batch_size,))\n    x = torch.stack([data[i:i + block_size] for i in ix])\n    y = torch.stack([data[i + 1:i + block_size + 1] for i in ix])\n    return x, y\n\n@torch.no_grad()\ndef estimate_loss():\n    out = {}\n    model.eval()\n    for split in ['train', 'val']:\n        losses = torch.zeros(eval_iters)\n        for k in range(eval_iters):\n            X, Y = get_batch(split)\n            logits, loss = model(X, Y)\n            losses[k] = loss.item()\n    return out\n\nclass Head(nn.Module):\n    \"\"\" one head of self-attention \"\"\"\n\n    def __init__(self, head_size):\n        super().__init__()\n        self.key = nn.Linear(n_embd, head_size, bias=False)\n        self.query = nn.Linear(n_embd, head_size, bias=False)\n        self.value = nn.Linear(n_embd, head_size, bias=False)\n        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n\n    def forward(self, x):\n        B, T, C = x.shape\n        k = self.key(x)  # (B, T, C)\n        q = self.query(x)  # (B, T, C)\n        # compute attention scores (\"affinities\")\n        wei = q @ k.transpose(-2, -1) * C**-0.5  # (B, T, T)\n        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n        wei = F.softmax(wei, dim=-1)\n        # perform the weighted aggregation of the values\n        v = self.value(x)  # (B, T, C)\n        out = wei @ v  # (B, T, C)\n        return out\n\nclass BigramLanguageModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        # each token directly reads off the logits for the next token from a lookup table\n        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n        self.sa_head = Head(n_embd)\n        self.lm_head = nn.Linear(n_embd, vocab_size)\n\n    def forward(self, idx, targets=None):\n        B, T = idx.shape\n        tok_emb = self.token_embedding_table(idx)  # (B,T,C)\n        pos_emb = self.position_embedding_table(torch.arange(T, device=device))  # (T,C)\n        x = tok_emb + pos_emb  # (B,T,C)\n        x = self.sa_head(x)  # apply one head of self-attention. (B,T,C)\n        logits = self.lm_head(x)\n\n        if targets is None:\n            loss = None\n        else:\n            B, T, C = logits.shape\n            logits = logits.view(B*T, C)\n            targets = targets.view(B*T)\n            loss = F.cross_entropy(logits, targets)\n\n        return logits, loss\n\n    def generate(self, idx, max_new_tokens):\n        # idx is (B, T) array of indices in the current context\n        for _ in range(max_new_tokens):\n            # crop idx to the last block_size tokens\n            idx_cond = idx[:, -block_size:]\n            # get the predictions\n            logits, loss = self(idx_cond)\n            # focus only on the last time step\n            logits = logits[:, -1, :]  # becomes (B, C)\n            # apply softmax to get probabilities\n            probs = F.softmax(logits, dim=-1)  # (B, C)\n            # sample from the distribution\n            idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n            # append sampled index to the running sequence\n            idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n        return idx\n```", "```python\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\n\n# hyperparameters\nbatch_size = 32  # how many independent sequences will we process in parallel?\nblock_size = 8  # what is the maximum context length for predictions?\nmax_iters = 5000\neval_interval = 500\nlearning_rate = 1e-3\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\neval_iters = 200\nn_embd = 32\n\ntorch.manual_seed(1337)\n\n# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\nwith open('input.txt', 'r', encoding='utf-8') as f:\n    text = f.read()\n\n# here are all the unique characters that occur in this text\nchars = sorted(list(set(text)))\nvocab_size = len(chars)\n\n# create a mapping from characters to integers\nstoi = {c: i for i, c in enumerate(chars)}\nitos = {i: c for i, c in enumerate(chars)}\nencode = lambda s: [stoi[c] for c in s]  # encoder: take a string, output a list of integers\ndecode = lambda l: ''.join([itos[i] for i in l])  # decoder: take a list of integers, output a string\n\n# Train and test splits\ndata = torch.tensor(encode(text), dtype=torch.long)\nn = int(0.9 * len(data))  # first 90% will be train, rest val\ntrain_data = data[:n]\nval_data = data[n:]\n\n# data loading\ndef get_batch(split):\n    # generate a small batch of data of inputs x and targets y\n    data = train_data if split == 'train' else val_data\n    ix = torch.randint(len(data) - block_size, (batch_size,))\n    x = torch.stack([data[i:i + block_size] for i in ix])\n    y = torch.stack([data[i + 1:i + block_size + 1] for i in ix])\n    return x, y\n\n@torch.no_grad()\ndef estimate_loss():\n    out = {}\n    model.eval()\n    for split in ['train', 'val']:\n        losses = torch.zeros(eval_iters)\n        for k in range(eval_iters):\n            X, Y = get_batch(split)\n            logits, loss = model(X, Y)\n            losses[k] = loss.item()\n    return out\n\nclass Head(nn.Module):\n    def __init__(self, head_size):\n        super().__init__()\n        self.key = nn.Linear(n_embd, head_size, bias=False)\n        self.query = nn.Linear(n_embd, head_size, bias=False)\n        self.value = nn.Linear(n_embd, head_size, bias=False)\n        self.register_buffer('tril', torch.tril(torch.ones(block_size)))\n\n    def forward(self, x):\n        B, T, C = x.shape\n        k = self.key(x)\n        q = self.query(x)\n        # compute attention scores (\"affinities\")\n        wei = q @ k.transpose(-2, -1) * C**-0.5  # (B, T, T)\n        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))  # (B, T, T)\n        wei = F.softmax(wei, dim=-1)  # perform the weighted aggregation of the values\n        v = self.value(x)  # (B, T, C)\n        out = wei @ v  # (B, T, C)\n        return out\n\nclass MultiHeadAttention(nn.Module):\n    \"\"\" multiple heads of self-attention in parallel \"\"\"\n    \n    def __init__(self, num_heads, head_size):\n        super().__init__()\n        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n    \n    def forward(self, x):\n        return torch.cat([h(x) for h in self.heads], dim=-1)\n\nclass FeedForward(nn.Module):\n    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n    \n    def __init__(self, n_embd):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(n_embd, n_embd),\n            nn.ReLU()\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\nclass Block(nn.Module):\n    \"\"\" Transformer block: communication followed by computation \"\"\"\n    \n    def __init__(self, n_embd, n_head):\n        super().__init__()\n        head_size = n_embd // n_head\n        self.sa = MultiHeadAttention(n_head, head_size)\n        self.ffwd = FeedForward(n_embd)\n\n    def forward(self, x):\n        x = self.sa(x)\n        x = self.ffwd(x)\n        return x\n\nclass BigramLanguageModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n        self.blocks = nn.Sequential(\n            Block(n_embd, n_head=4),\n            Block(n_embd, n_head=4),\n            Block(n_embd, n_head=4),\n            Block(n_embd, n_head=4),\n        )\n        self.lm_head = nn.Linear(n_embd, vocab_size)\n\n    def forward(self, idx, targets=None):\n        B, T = idx.shape\n        tok_emb = self.token_embedding_table(idx)  # (B, T, C)\n        pos_emb = self.position_embedding_table(torch.arange(T, device=device))  # (T, C)\n        x = tok_emb + pos_emb  # (B, T, C)\n        x = self.blocks(x)  # (B, T, C)\n        logits = self.lm_head(x)  # (B, T, vocab_size)\n\n        if targets is None:\n            loss = None\n        else:\n            B, T, C = logits.shape\n            logits = logits.view(B*T, C)\n            targets = targets.view(B*T)\n            loss = F.cross_entropy(logits, targets)\n\n        return logits, loss\n\n    def generate(self, idx, max_new_tokens):\n        # idx is (B, T) array of indices in the current context\n        for _ in range(max_new_tokens):\n            # crop idx to the last block_size tokens\n            idx_cond = idx[:, -block_size:]\n            # get the predictions\n            logits, _ = self(idx_cond)\n            # focus only on the last time step\n            logits = logits[:, -1, :]  # (B, C)\n            # apply softmax to get probabilities\n            probs = F.softmax(logits, dim=-1)  # (B, C)\n            # sample from the distribution\n            idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n            # append sampled index to the running sequence\n            idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n        return idx\n```", "```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt\n\n# MultiHeadAttention class\nclass MultiHeadAttention(nn.Module):\n    \"\"\" multiple heads of self-attention in parallel \"\"\"\n    \n    def __init__(self, num_heads, head_size):\n        super().__init__()\n        self.heads = nn.ModuleList(\n            [Head(head_size) for _ in range(num_heads)]\n        )\n        self.proj = nn.Linear(num_heads * head_size, num_heads * head_size)\n\n    def forward(self, x):\n        out = torch.cat([h(x) for h in self.heads], dim=-1)\n        out = self.proj(out)\n        return out\n\n# FeedForward class\nclass FeedForward(nn.Module):\n    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n    \n    def __init__(self, n_embd):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(n_embd, 4 * n_embd),\n            nn.ReLU(),\n            nn.Linear(4 * n_embd, n_embd),\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n# Block class\nclass Block(nn.Module):\n    \"\"\" Transformer block: communication followed by computation \"\"\"\n    \n    def __init__(self, n_embd, n_head):\n        super().__init__()\n        head_size = n_embd // n_head\n        self.sa = MultiHeadAttention(n_head, head_size)\n        self.ffwd = FeedForward(n_embd)\n        self.ln1 = nn.LayerNorm(n_embd)\n        self.ln2 = nn.LayerNorm(n_embd)\n\n    def forward(self, x):\n        x = x + self.sa(self.ln1(x))\n        x = x + self.ffwd(self.ln2(x))\n        return x\n\n# BigramLanguageModel class\nclass BigramLanguageModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n        self.blocks = nn.Sequential(\n            Block(n_embd, n_head=4),\n            Block(n_embd, n_head=4),\n            Block(n_embd, n_head=4),\n            nn.LayerNorm(n_embd),\n        )\n        self.lm_head = nn.Linear(n_embd, vocab_size)\n\n    def forward(self, idx, targets=None):\n        B, T = idx.shape\n        tok_emb = self.token_embedding_table(idx)  # (B, T, C)\n        pos_emb = self.position_embedding_table(torch.arange(T, device=idx.device))  # (T, C)\n        x = tok_emb + pos_emb\n        x = self.blocks(x)\n        x = self.lm_head(x)\n        return x\n\n# Read in all the words\nwords = open('names.txt', 'r').read().splitlines()\n\n# Build the vocabulary of characters and mappings to/from integers\nchars = sorted(list(set(''.join(words))))\nstoi = {s: i for i, s in enumerate(chars)}\nitos = {i: s for i, s in enumerate(chars)}\nvocab_size = len(stoi)\n\n# Build the dataset\nblock_size = 3  # context length; how many characters do we take to predict the next one?\ndef build_dataset(words):\n    X, Y = [], []\n    for w in words:\n        context = [0] * block_size\n        for ch in w + ' ':\n            ix = stoi[ch]\n            X.append(context)\n            Y.append(ix)\n            context = context[1:] + [ix]  # crop and append\n    X = torch.tensor(X)\n    Y = torch.tensor(Y)\n    return X, Y\n\nimport random\nrandom.seed(42)\nrandom.shuffle(words)\nn1 = int(0.8 * len(words))\nn2 = int(0.9 * len(words))\nXtr, Ytr = build_dataset(words[:n1])  # 80%\nXdev, Ydev = build_dataset(words[n1:n2])  # 10%\nXte, Yte = build_dataset(words[n2:])  # 10%\n\n# MLP revisited\nn_embd = 10  # the dimensionality of the character embedding vectors\nn_hidden = 200  # the number of neurons in the hidden layer of the MLP\n\ng = torch.Generator().manual_seed(2147483647)  # for reproducibility\nc = torch.randn((n_embd, vocab_size), generator=g)\nw1 = torch.randn((n_hidden, n_embd), generator=g) * (5/3) / (n_embd * block_size)**0.5  # 0.2\nb1 = torch.randn(n_hidden, generator=g) * 0.01\n```", "```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n# hyperparameters\nbatch_size = 64  # how many independent sequences will we process in parallel?\nblock_size = 256  # what is the maximum context length for predictions?\nmax_iters = 5000\neval_interval = 500\nlearning_rate = 3e-4\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\neval_iters = 200\nn_embd = 384\nn_head = 6\nn_layer = 6\ndropout = 0.2\n\ntorch.manual_seed(1337)\n\n# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\nwith open('input.txt', 'r', encoding='utf-8') as f:\n    text = f.read()\n\n# here are all the unique characters that occur in this text\nchars = sorted(list(set(text)))\nvocab_size = len(chars)\n\n# create a mapping from characters to integers\nstoi = {ch: i for i, ch in enumerate(chars)}\nitos = {i: ch for i, ch in enumerate(chars)}\nencode = lambda s: [stoi[c] for c in s]  # encoder: take a string, output a list of integers\ndecode = lambda l: ''.join(itos[i] for i in l)  # decoder: take a list of integers, output a string\n\n# Train and test splits\ndata = torch.tensor(encode(text), dtype=torch.long)\nn = int(0.9 * len(data))  # first 90% will be train, rest val\ntrain_data = data[:n]\nval_data = data[n:]\n\nclass Head(nn.Module):\n    \"\"\" one head of self-attention \"\"\"\n\n    def __init__(self, head_size):\n        super().__init__()\n        self.key = nn.Linear(n_embd, head_size, bias=False)\n        self.query = nn.Linear(n_embd, head_size, bias=False)\n        self.value = nn.Linear(n_embd, head_size, bias=False)\n        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        B, T, C = x.shape\n        k = self.key(x)  # (B, T, C)\n        q = self.query(x)  # (B, T, C)\n        wei = q @ k.transpose(-2, -1) * C**-0.5  # (B, T, T)\n        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n        wei = F.softmax(wei, dim=-1)\n        v = self.value(x)  # (B, T, C)\n        out = wei @ v  # (B, T, C)\n        return out\n\nclass MultiHeadAttention(nn.Module):\n    \"\"\" multiple heads of self-attention in parallel \"\"\"\n    \n    def __init__(self, num_heads, head_size):\n        super().__init__()\n        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n        self.proj = nn.Linear(n_embd, n_embd)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        out = torch.cat([h(x) for h in self.heads], dim=-1)\n        out = self.dropout(self.proj(out))\n        return out\n\nclass FeedForward(nn.Module):\n    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n    \n    def __init__(self, n_embd):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(n_embd, 4 * n_embd),\n            nn.ReLU(),\n            nn.Linear(4 * n_embd, n_embd),\n            nn.Dropout(dropout),\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\nclass Block(nn.Module):\n    \"\"\" Transformer block: communication followed by computation \"\"\"\n    \n    def __init__(self, n_embd, n_head):\n        super().__init__()\n        head_size = n_embd // n_head\n        self.sa = MultiHeadAttention(n_head, head_size)\n        self.ffwd = FeedForward(n_embd)\n        self.ln1 = nn.LayerNorm(n_embd)\n        self.ln2 = nn.LayerNorm(n_embd)\n\n    def forward(self, x):\n        x = x + self.sa(self.ln1(x))\n        x = x + self.ffwd(self.ln2(x))\n        return x\n\nclass BigramLanguageModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n        self.blocks = nn.Sequential(\n            *[Block(n_embd, n_head) for _ in range(n_layer)],\n            nn.LayerNorm(n_embd),\n        )\n        self.lm_head = nn.Linear(n_embd, vocab_size)\n\n    def forward(self, idx, targets=None):\n        B, T = idx.shape\n        tok_emb = self.token_embedding_table(idx)  # (B, T, C)\n        pos_emb = self.position_embedding_table(torch.arange(T, device=device))  # (T, C)\n        x = tok_emb + pos_emb\n        x = self.blocks(x)\n        x = self.lm_head(x)\n        \n        if targets is None:\n            loss = None\n        else:\n            B, T, C = x.shape\n            x = x.view(B*T, C)\n            targets = targets.view(B*T)\n            loss = F.cross_entropy(x, targets)\n        \n        return x, loss\n```", "```python\nimport math\nfrom dataclasses import dataclass\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\n\n@torch.jit.script\ndef new_gelu(x):\n    return 0.5 * x * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (x + 0.044715 * torch.pow(x, 3.0))))\n\nclass CausalSelfAttention(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        assert config.n_embd % config.n_head == 0\n        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n        self.attn_dropout = nn.Dropout(config.dropout)\n        self.resid_dropout = nn.Dropout(config.dropout)\n        self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size)).view(1, 1, config.block_size, config.block_size))\n        self.n_head = config.n_head\n        self.n_embd = config.n_embd\n\n    def forward(self, x):\n        B, T, C = x.size()\n        q, k, v = self.c_attn(x).split(self.n_embd, dim=2)\n        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n        att = att.masked_fill(self.bias[:, :, :T, :T] == 0, float('-inf'))\n        att = F.softmax(att, dim=-1)\n        y = att @ v\n        y = y.transpose(1, 2).contiguous().view(B, T, C)\n        y = self.resid_dropout(self.c_proj(y))\n        return y\n\nclass MLP(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.c_fc = nn.Linear(config.n_embd, 4 * config.n_embd)\n        self.c_proj = nn.Linear(4 * config.n_embd, config.n_embd)\n        self.dropout = nn.Dropout(config.dropout)\n\n    def forward(self, x):\n        x = self.c_fc(x)\n        x = new_gelu(x)\n        x = self.c_proj(x)\n        x = self.dropout(x)\n        return x\n\nclass Block(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.ln_1 = nn.LayerNorm(config.n_embd)\n        self.attn = CausalSelfAttention(config)\n        self.ln_2 = nn.LayerNorm(config.n_embd)\n        self.mlp = MLP(config)\n\n    def forward(self, x):\n        x = x + self.attn(self.ln_1(x))\n        x = x + self.mlp(self.ln_2(x))\n        return x\n\n@dataclass\nclass GPTConfig:\n    block_size: int = 1024\n    vocab_size: int = 50257\n    n_layer: int = 12\n    n_head: int = 12\n    n_embd: int = 768\n    dropout: float = 0.1\n\nclass GPT(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        assert config.vocab_size is not None\n        assert config.block_size is not None\n        self.config = config\n        self.transformer = nn.ModuleDict(dict(\n            wte=nn.Embedding(config.vocab_size, config.n_embd),\n            wpe=nn.Embedding(config.block_size, config.n_embd),\n            drop=nn.Dropout(config.dropout),\n            h=nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n            ln_f=nn.LayerNorm(config.n_embd),\n        ))\n        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n        self.transformer.wte.weight = self.lm_head.weight\n\n    def forward(self, idx, targets=None):\n        device = idx.device\n        b, t = idx.size()\n        assert t <= self.config.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.config.block_size}\"\n        pos = torch.arange(0, t, dtype=torch.long, device=device).unsqueeze(0)\n        tok_emb = self.transformer.wte(idx)\n        pos_emb = self.transformer.wpe(pos)\n        x = self.transformer.drop(tok_emb + pos_emb)\n        for block in self.transformer.h:\n            x = block(x)\n        x = self.transformer.ln_f(x)\n        if targets is not None:\n            logits = self.lm_head(x)\n            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n        else:\n            logits = self.lm_head(x[:, -1, :])\n            loss = None\n        return logits, loss\n\n    @classmethod\n    def from_pretrained(cls, model_type, override_args=None):\n        assert model_type in {'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}\n        override_args = override_args or {}\n        assert all(k == 'dropout' for k in override_args)\n        from transformers import GPT2LMHeadModel\n        config_args = {\n            'gpt2': dict(n_layer=12, n_head=12, n_embd=768),\n            'gpt2-medium': dict(n_layer=24, n_head=16, n_embd=1024),\n            'gpt2-large': dict(n_layer=36, n_head=20, n_embd=1280),\n            'gpt2-xl': dict(n_layer=48, n_head=25, n_embd=1600),\n        }[model_type]\n        if 'dropout' in override_args:\n            config_args['dropout'] = override_args['dropout']\n        config = GPTConfig(block_size=1024, **config_args)\n        model = GPT(config)\n        sd = model.state_dict()\n        model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n        sd_hf = model_hf.state_dict()\n        keys = [k for k in sd_hf if not k.endswith('attn.masked_bias')]\n        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n        assert len(keys) == len(sd)\n        for k in keys:\n            if any(k.endswith(w) for w in transposed):\n                assert sd_hf[k].shape[:-1] == sd[k].shape\n                with torch.no_grad():\n                    sd[k].copy_(sd_hf[k])\n\ndef configure_optimizers(self, weight_decay, learning_rate, betas):\n    decay = set()\n    no_decay = set()\n    whitelist_weight_modules = (torch.nn.Linear,)\n    blacklist_weight_modules = (torch.nn.LayerNorm, torch.nn.Embedding)\n    for mn, m in self.named_modules():\n        for pn, p in m.named_parameters():\n            fpn = '%s.%s' % (mn, pn) if mn else pn\n            if pn.endswith('bias'):\n                no_decay.add(fpn)\n            elif pn.endswith('weight') and isinstance(m, whitelist_weight_modules):\n                decay.add(fpn)\n            elif pn.endswith('weight') and isinstance(m, blacklist_weight_modules):\n                no_decay.add(fpn)\n    decay.remove('lm_head.weight')\n    param_dict = {pn: p for pn, p in self.named_parameters()}\n    inter_params = decay & no_decay\n    union_params = decay | no_decay\n    assert len(inter_params) == 0, \"parameters %s made it into both decay/no_decay set!\" % (str(inter_params),)\n    assert len(param_dict.keys() - union_params) == 0, \"parameters %s were not separated into either decay/no_decay!\" % (str(param_dict.keys() - union_params),)\n    optim_groups = [\n        {\"params\": [param_dict[pn] for pn in sorted(list(decay))], \"weight_decay\": weight_decay},\n        {\"params\": [param_dict[pn] for pn in sorted(list(no_decay))], \"weight_decay\": 0.0},\n    ]\n    optimizer = torch.optim.Adam(optim_groups, lr=learning_rate, betas=betas)\n    return optimizer\n\n@torch.no_grad()\ndef generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n    for _ in range(max_new_tokens):\n        idx_cond = idx if idx.size(0) < self.config.block_size else idx[-self.config.block_size:]\n        logits, _ = self(idx_cond)\n        logits = logits[:, -1, :] / temperature\n        if top_k is not None:\n            v, ix = torch.topk(logits, top_k)\n            logits[logits < v[:, [-1]]] = -float('Inf')\n        probs = F.softmax(logits, dim=-1)\n        idx_next = torch.multinomial(probs, num_samples=1)\n        idx = torch.cat((idx, idx_next), dim=1)\n    return idx\n\n# hyperparameters\nbatch_size = 64\nblock_size = 256\nmax_iters = 5000\neval_interval = 500\nlearning_rate = 3e-4\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\neval_iters = 200\nn_embd = 384\nn_head = 6\nn_layer = 6\ndropout = 0.2\n\ntorch.manual_seed(1337)\n\nwith open('input.txt', 'r', encoding='utf-8') as f:\n    text = f.read()\n\nchars = sorted(list(set(text)))\nvocab_size = len(chars)\nstoi = {ch: i for i, ch in enumerate(chars)}\nitos = {i: ch for i, ch in enumerate(chars)}\nencode = lambda s: [stoi[c] for c in s]\ndecode = lambda l: ''.join(itos[i] for i in l)\n\ndata = torch.tensor(encode(text), dtype=torch.long)\nn = int(0.9 * len(data))\ntrain_data = data[:n]\nval_data = data[n:]\n\ndef get_batch(split):\n    data = train_data if split == 'train' else val_data\n    ix = torch.randint(len(data), (block_size, batch_size,))\n    x = torch.stack([data[i:i+block_size] for i in ix])\n    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n    x, y = x.to(device), y.to(device)\n    return x, y\n\n@torch.no_grad()\ndef estimate_loss():\n    out = {}\n    model.eval()\n    for split in ['train', 'val']:\n        losses = torch.zeros(eval_iters)\n        for k in range(eval_iters):\n            X, Y = get_batch(split)\n            logits, loss = model(X, Y)\n            losses[k] = loss.item()\n        out[split] = losses.mean()\n    return out\n\nmodel = BigramLanguageModel()\nm = model.to(device)\nprint(sum(p.numel() for p in m.parameters()) / 1e6, 'M parameters')\n\noptimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n\nfor iter in range(max_iters):\n    if iter % eval_interval == 0 or iter == max_iters - 1:\n        losses = estimate_loss()\n        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n\n    xb, yb = get_batch('train')\n    logits, loss = model(xb, yb)\n    optimizer.zero_grad(set_to_none=True)\n    loss.backward()\n    optimizer.step()\n```", "```go\nresultWorkerErr := make(chan error)\ndefer close(resultWorkerErr)\ngo func() {\n    defer cancel()\n    resultWorkerErr <- b.resultWorker(ctx)\n}()\n\nerr := b.worker(ctx)\ncancel()\nif err != nil {\n    return <-resultWorkerErr\n}\nreturn multierror.Append(err, <-resultWorkerErr)\n```\n\n```python\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\n\n# hyperparameters\nbatch_size = 64  # how many independent sequences will we process in parallel?\nblock_size = 256  # what is the maximum context length for predictions?\nmax_iters = 5000\neval_interval = 500\nlearning_rate = 3e-4\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\neval_iters = 200\nn_embd = 384\nn_head = 6\nn_layer = 6\ndropout = 0.2\n\ntorch.manual_seed(1337)\n\n# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\nwith open('input.txt', 'r', encoding='utf-8') as f:\n    text = f.read()\n\ndef generate(self, idx, max_new_tokens):\n    # idx is (B, T) array of indices in the current context\n    for _ in range(max_new_tokens):\n        # crop idx to the last block_size tokens\n        idx_cond = idx[:, -block_size:]  # get the predictions\n        logits, loss = self(idx_cond)\n        # focus only on the last time step\n        logits = logits[:, -1, :]  # becomes (B, C)\n        # apply softmax to get probabilities\n        probs = F.softmax(logits, dim=-1)  # (B, C)\n        # sample from the distribution\n        idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n        # append sampled index to the running sequence\n        idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n    return idx\n\n# evaluate the loss\nlogits, loss = model(xb, yb)\noptimizer.zero_grad(set_to_none=True)\nloss.backward()\noptimizer.step()\n\n# generate from the model\ncontext = torch.zeros(1, 1, dtype=torch.long, device=device)\nprint(decode(m.generate(context, max_new_tokens=500)[0].tolist()))\n# open('more.txt', 'w').write(decode(m.generate(context, max_new_tokens=10000)[0].tolist()))\n\n# version 4: self-attention!\ntorch.manual_seed(1337)\nB, T, C = 4, 8, 32  # batch, time, channels\nx = torch.randn(B, T, C)\n\n# let's see a single Head perform self-attention\nhead_size = 16\nkey = nn.Linear(C, head_size, bias=False)\nquery = nn.Linear(C, head_size, bias=False)\nvalue = nn.Linear(C, head_size, bias=False)\nk = key(x)\nq = query(x)\nwei = q @ k.transpose(-2, -1)  # (B, T, 16) @ (B, 16, T) ---> (B, T, T)\nwei = torch.zeros((T, T))\nwei = wei.masked_fill(wei == 0, float('-inf'))\nwei = F.softmax(wei, dim=-1)\n\nv = value(x)\nout = wei @ v\nout.shape\n\ntorch.Size([4, 8, 16])\n```"]